---
title: "残差连接"
date: 2024-08-11
aliases: ["/NLP"]
tags: ["NLP"]
categories: ["NLP"]
author: "Kurong"
showToc: true
TocOpen: true
draft: false
hidemeta: false
comments: false
description: ""
disableHLJS: false # to disable highlightjs
disableShare: true
disableHLJS: false
hideSummary: false
searchHidden: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
---

## 原理

残差连接（Residual Connection）最早由何凯明等人在2015年提出的 ResNet 中引入。ResNet 通过引入残差块，使得网络可以扩展到更深的层数，并在 ImageNet 比赛中取得了显著的成功。

残差连接的核心思想是引入跳跃连接，将输入信号直接传递到网络的后续层，从而构建了一条捷径路径。这种结构允许网络学习输入和输出之间的残差，而不是直接学习输出。

残差连接可以表示为：
$$
y=F(x)+x
$$
其中，$x$ 表示输入，$F(x)$ 表示经过非线性变换后的输出。



## 作用

- 解决梯度消失和梯度爆炸问题
- 提高训练效率
- 增强模型的泛化性能



## 例子

下图是 Transformer 论文中的模型结构图。

![](/img/NLP/img14.png)

可以看到在每一个 Attention Layer 中都有一个 `Add` ，原输入和 Multi-head 变换后的输出做了一个简单的相加操作，而这就是所谓的残差连接。
