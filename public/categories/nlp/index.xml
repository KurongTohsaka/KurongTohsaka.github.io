<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on KurongBlog</title>
    <link>http://localhost:1313/categories/nlp/</link>
    <description>Recent content in NLP on KurongBlog</description>
    <image>
      <title>KurongBlog</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.135.0</generator>
    <language>en</language>
    <lastBuildDate>Sat, 17 Aug 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BPE算法</title>
      <link>http://localhost:1313/posts/nlp/bpe/</link>
      <pubDate>Sat, 17 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/bpe/</guid>
      <description>&lt;h2 id=&#34;基本概念&#34;&gt;基本概念&lt;/h2&gt;
&lt;p&gt;Byte-Pair Encoding (BPE) 是一种常用于自然语言处理（NLP）的分词算法。BPE最初是一种数据压缩算法，由Philip Gage在1994年提出。在NLP中，BPE被用来将文本分割成子词（subword）单元，这样可以在处理未见过的单词时更有效。&lt;/p&gt;
&lt;h2 id=&#34;工作原理&#34;&gt;工作原理&lt;/h2&gt;
&lt;p&gt;BPE的核心思想是通过多次迭代，将最常见的字符对（或子词对）合并成一个新的符号，直到词汇表达到预定的大小。具体步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化词汇表&lt;/strong&gt;：将所有单个字符作为初始词汇表。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统计频率&lt;/strong&gt;：统计所有相邻字符对的出现频率。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合并字符对&lt;/strong&gt;：找到出现频率最高的字符对，并将其合并成一个新的符号。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新词汇表&lt;/strong&gt;：将新的符号加入词汇表，并更新文本中的所有相应字符对。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重复步骤2-4&lt;/strong&gt;：直到词汇表达到预定大小。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;例子&#34;&gt;例子&lt;/h2&gt;
&lt;p&gt;假设我们有一个简单的文本：“banana banana”. 初始词汇表为：{b, a, n, }. 具体步骤如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;初始化&lt;/strong&gt;：词汇表为 {b, a, n, }。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统计频率&lt;/strong&gt;：统计相邻字符对的频率，如 “ba” 出现2次，“an” 出现2次，“na” 出现2次。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合并字符对&lt;/strong&gt;：选择频率最高的字符对 “an”，将其合并成一个新符号 “an”。更新后的文本为 “b an an a b an an a”。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更新词汇表&lt;/strong&gt;：词汇表更新为 {b, a, n, an}。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;重复步骤2-4&lt;/strong&gt;：继续统计频率并合并，直到词汇表达到预定大小。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;byte-level-bpe&#34;&gt;Byte-Level BPE&lt;/h2&gt;
&lt;p&gt;在处理多语言文本时，BPE的一个问题是字符集可能会非常大。为了解决这个问题，可以使用Byte-Level BPE。Byte-Level BPE将每个字节视为一个字符，这样基础字符集的大小固定为256（即所有可能的字节值）。&lt;/p&gt;
&lt;h2 id=&#34;优缺点&#34;&gt;优缺点&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;处理未见过的单词&lt;/strong&gt;：通过将单词分割成子词，可以更好地处理未见过的单词&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;词汇表大小可控&lt;/strong&gt;：可以通过设置预定大小来控制词汇表的大小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义信息丢失&lt;/strong&gt;：在某些情况下，分割后的子词可能会丢失部分语义信息&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算复杂度&lt;/strong&gt;：多次迭代合并字符对的过程可能会增加计算复杂度&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>😺 Is All You Need——Transformer补充</title>
      <link>http://localhost:1313/posts/nlp/attention-is-all-you-need/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/attention-is-all-you-need/</guid>
      <description>&lt;h2 id=&#34;关于本文动机&#34;&gt;关于本文动机&lt;/h2&gt;
&lt;p&gt;Transformer主要内容请见 &lt;a href=&#34;https://kurongtohsaka.github.io/posts/cs224n/lesson_9/&#34;&gt;Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)&lt;/a&gt;，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。&lt;/p&gt;
&lt;h2 id=&#34;qkv-的理解&#34;&gt;Q，K，V 的理解&lt;/h2&gt;
&lt;p&gt;假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。&lt;/p&gt;
&lt;p&gt;通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思：
&lt;/p&gt;
$$
softmax(QK)=W \\
WV=Y
$$&lt;p&gt;
一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。&lt;/p&gt;
&lt;h2 id=&#34;attention-机制&#34;&gt;Attention 机制&lt;/h2&gt;
&lt;p&gt;在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。&lt;/p&gt;
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$&lt;ul&gt;
&lt;li&gt;注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。&lt;/li&gt;
&lt;li&gt;返回每个键值的权重代表这种相似性。&lt;/li&gt;
&lt;li&gt;最后，返回所有值的加权组合作为输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mask-掩码&#34;&gt;Mask 掩码&lt;/h2&gt;
&lt;p&gt;在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。&lt;/p&gt;
&lt;p&gt;我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。&lt;/p&gt;
&lt;h2 id=&#34;multi-head-attention-多头注意力机制&#34;&gt;Multi-head Attention 多头注意力机制&lt;/h2&gt;
&lt;p&gt;“小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。&lt;/p&gt;
&lt;h2 id=&#34;位置编码&#34;&gt;位置编码&lt;/h2&gt;
&lt;p&gt;在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Encoder-Decoder 架构</title>
      <link>http://localhost:1313/posts/nlp/encoder-decoder/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/encoder-decoder/</guid>
      <description>&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;
&lt;p&gt;Encoder-Decoder架构通常由两个主要部分组成：编码器（Encoder）和解码器（Decoder）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;编码器（Encoder）&lt;/strong&gt;：编码器的任务是将输入序列（如一句话）转换为一个固定长度的上下文向量（context vector）。这个过程通常通过递归神经网络（RNN）、长短期记忆网络（LSTM）或门控循环单元（GRU）来实现。编码器逐步读取输入序列的每个元素，并将其信息压缩到上下文向量中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;解码器（Decoder）&lt;/strong&gt;：解码器接收编码器生成的上下文向量，并将其转换为输出序列。解码器同样可以使用RNN、LSTM或GRU。解码器在生成每个输出元素时，会考虑上下文向量以及之前生成的输出元素。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;p&gt;Encoder-Decoder架构主要用于处理需要将一个序列转换为另一个序列的任务，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;机器翻译&lt;/strong&gt;：将一种语言的句子翻译成另一种语言。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;文本摘要&lt;/strong&gt;：将长文本压缩成简短摘要。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对话系统&lt;/strong&gt;：生成对用户输入的响应。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;近年来，基于Transformer的Encoder-Decoder架构（如BERT、GPT、T5等）因其更好的性能和并行计算能力，逐渐取代了传统的RNN架构。&lt;/p&gt;
&lt;h2 id=&#34;优缺点&#34;&gt;优缺点&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;优点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;灵活性&lt;/strong&gt;：可以处理不同长度的输入和输出序列。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;强大的表示能力&lt;/strong&gt;：能够捕捉输入序列中的复杂模式和关系。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;长距离依赖问题&lt;/strong&gt;：传统RNN在处理长序列时可能会遗忘早期的信息。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算复杂度高&lt;/strong&gt;：训练和推理过程需要大量计算资源。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>残差连接</title>
      <link>http://localhost:1313/posts/nlp/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/</link>
      <pubDate>Sun, 11 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/</guid>
      <description>&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;
&lt;p&gt;残差连接（Residual Connection）最早由何凯明等人在2015年提出的 ResNet 中引入。ResNet 通过引入残差块，使得网络可以扩展到更深的层数，并在 ImageNet 比赛中取得了显著的成功。&lt;/p&gt;
&lt;p&gt;残差连接的核心思想是引入跳跃连接，将输入信号直接传递到网络的后续层，从而构建了一条捷径路径。这种结构允许网络学习输入和输出之间的残差，而不是直接学习输出。&lt;/p&gt;
&lt;p&gt;残差连接可以表示为：
&lt;/p&gt;
$$
y=F(x)+x
$$&lt;p&gt;
其中，$x$ 表示输入，$F(x)$ 表示经过非线性变换后的输出。&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;解决梯度消失和梯度爆炸问题&lt;/li&gt;
&lt;li&gt;提高训练效率&lt;/li&gt;
&lt;li&gt;增强模型的泛化性能&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;例子&#34;&gt;例子&lt;/h2&gt;
&lt;p&gt;下图是 Transformer 论文中的模型结构图。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/NLP/img14.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;可以看到在每一个 Attention Layer 中都有一个 &lt;code&gt;Add&lt;/code&gt; ，原输入和 Multi-head 变换后的输出做了一个简单的相加操作，而这就是所谓的残差连接。&lt;/p&gt;</description>
    </item>
    <item>
      <title>RNN速成（二）</title>
      <link>http://localhost:1313/posts/nlp/rnn%E9%80%9F%E6%88%90%E4%BA%8C/</link>
      <pubDate>Fri, 12 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/rnn%E9%80%9F%E6%88%90%E4%BA%8C/</guid>
      <description>了解RNN的两种变体</description>
    </item>
    <item>
      <title>RNN速成（一）</title>
      <link>http://localhost:1313/posts/nlp/rnn/</link>
      <pubDate>Sat, 06 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/rnn/</guid>
      <description>了解RNN本体</description>
    </item>
    <item>
      <title>Named Entity Recognition 相关概念与技术</title>
      <link>http://localhost:1313/posts/nlp/ner/</link>
      <pubDate>Fri, 05 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/ner/</guid>
      <description>命名实体识别快速入门</description>
    </item>
    <item>
      <title>Continuous Bag of Words</title>
      <link>http://localhost:1313/posts/nlp/word2vec-variants-continuous_bag_of_words/</link>
      <pubDate>Sat, 29 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/word2vec-variants-continuous_bag_of_words/</guid>
      <description>CBOW的简单认知</description>
    </item>
    <item>
      <title>Skip-gram Model</title>
      <link>http://localhost:1313/posts/nlp/word2vec-variants-skip-gram/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/word2vec-variants-skip-gram/</guid>
      <description>skip-gram的简单见解</description>
    </item>
  </channel>
</rss>
