<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å…… | KurongBlog</title>
<meta name=keywords content="NLP"><meta name=description content="å…³äºæœ¬æ–‡åŠ¨æœº
Transformerä¸»è¦å†…å®¹è¯·è§ Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)ï¼Œå¯¹ Transformer å·²ç»è¿›è¡Œæ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»å’Œè®²è§£äº†ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€äº›ç»†èŠ‚é—®é¢˜ä¸å¥½åœ¨è¯¥ç¯‡æ–‡ç« æåŠï¼Œæ‰€ä»¥å•å¼€ä¸€ç¯‡è®¨è®ºã€‚
Qï¼ŒKï¼ŒV çš„ç†è§£
å‡è®¾æˆ‘ä»¬æƒ³è®©æ‰€æœ‰çš„è¯éƒ½ä¸ç¬¬ä¸€ä¸ªè¯ $v_1$ ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥è®© $v_1$ ä½œä¸ºæŸ¥è¯¢ã€‚ ç„¶åï¼Œå°†è¯¥æŸ¥è¯¢ä¸å¥å­ä¸­æ‰€æœ‰è¯è¿›è¡Œç‚¹ç§¯ï¼Œè¿™é‡Œçš„è¯å°±æ˜¯é”®ã€‚ æ‰€ä»¥æŸ¥è¯¢å’Œé”®çš„ç»„åˆç»™äº†æˆ‘ä»¬æƒé‡ï¼Œæ¥ç€å†å°†è¿™äº›æƒé‡ä¸ä½œä¸ºå€¼çš„æ‰€æœ‰å•è¯ç›¸ä¹˜ã€‚
é€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç†è§£æŸ¥è¯¢ã€é”®ã€å€¼åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼š

$$
softmax(QK)=W \\
WV=Y
$$
ä¸€ç§æ¯”è¾ƒæ„Ÿæ€§çš„ç†è§£ï¼šæƒ³è¦å¾—åˆ°æŸä¸ª $V$ å¯¹åº”çš„æŸä¸ªå¯èƒ½çš„ç›¸ä¼¼ä¿¡æ¯éœ€è¦å…ˆ $Q$ è¿™ä¸ª $V$ çš„ $K$ ï¼Œ$QK$ å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¹‹åç»è¿‡ softmax å¹³æ»‘åå¾—åˆ°æ¦‚ç‡ $W $ï¼Œç„¶å $WV$ åå¾—åˆ°æœ€ç»ˆçš„ç›¸ä¼¼ä¿¡æ¯ $Y$ ã€‚
Attention æœºåˆ¶
åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æŸ¥è¯¢ $q$ å’Œé”® $k_i$ æ£€ç´¢æŸä¸ªå€¼ $v_i$ ã€‚æ³¨æ„åŠ›ä¸è¿™ç§æ•°æ®åº“å–å€¼æŠ€æœ¯ç±»ä¼¼ï¼Œä½†æ˜¯ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œçš„ã€‚
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$
æ³¨æ„åŠ›æœºåˆ¶æµ‹é‡æŸ¥è¯¢ $q$ å’Œæ¯ä¸ªé”®å€¼ $k_i$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
è¿”å›æ¯ä¸ªé”®å€¼çš„æƒé‡ä»£è¡¨è¿™ç§ç›¸ä¼¼æ€§ã€‚
æœ€åï¼Œè¿”å›æ‰€æœ‰å€¼çš„åŠ æƒç»„åˆä½œä¸ºè¾“å‡ºã€‚

Mask æ©ç 
åœ¨æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼Œè¿™ç±»ä»»åŠ¡æˆ‘ä»¬ä¸€æ¬¡åªèƒ½çœ‹åˆ°ä¸€ä¸ªå•è¯ã€‚æ­¤æ—¶æ³¨æ„åŠ›åªèƒ½æ”¾åœ¨ä¸‹ä¸€ä¸ªè¯ä¸Šï¼Œä¸èƒ½æ”¾åœ¨ç¬¬äºŒä¸ªè¯æˆ–åé¢çš„è¯ä¸Šã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ³¨æ„åŠ›ä¸èƒ½æœ‰éå¹³å‡¡çš„è¶…å¯¹è§’çº¿åˆ†é‡ã€‚
æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æ©ç çŸ©é˜µæ¥ä¿®æ­£æ³¨æ„åŠ›ï¼Œä»¥æ¶ˆé™¤ç¥ç»ç½‘ç»œå¯¹æœªæ¥çš„äº†è§£ã€‚
Multi-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â€œå°ç¾é•¿å¾—å¾ˆæ¼‚äº®è€Œä¸”äººè¿˜å¾ˆå¥½â€ ã€‚è¿™é‡Œâ€œäººâ€è¿™ä¸ªè¯ï¼Œåœ¨è¯­æ³•ä¸Šä¸â€œå°ç¾â€å’Œâ€œå¥½â€è¿™äº›è¯å­˜åœ¨æŸç§æ„ä¹‰æˆ–å…³è”ã€‚è¿™å¥è¯ä¸­â€œäººâ€è¿™ä¸ªè¯éœ€è¦ç†è§£ä¸ºâ€œäººå“â€ï¼Œè¯´çš„æ˜¯å°ç¾çš„äººå“å¾ˆå¥½ã€‚ä»…ä»…ä½¿ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½æ— æ³•æ­£ç¡®è¯†åˆ«è¿™ä¸‰ä¸ªè¯ä¹‹é—´çš„å…³è”ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸â€œäººâ€ç›¸å…³çš„è¯ã€‚è¿™å‡å°‘äº†æ³¨æ„åŠ›å¯»æ‰¾æ‰€æœ‰é‡è¦è¯çš„è´Ÿæ‹…ï¼Œå¢åŠ æ‰¾åˆ°æ›´å¤šç›¸å…³è¯çš„æœºä¼šã€‚
ä½ç½®ç¼–ç 
åœ¨ä»»ä½•å¥å­ä¸­ï¼Œå•è¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‡ºç°éƒ½è•´å«ç€é‡è¦æ„ä¹‰ã€‚å¦‚æœå¥å­ä¸­çš„å•è¯ä¹±ä¸ƒå…«ç³Ÿï¼Œé‚£ä¹ˆè¿™å¥è¯å¾ˆå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ä½†æ˜¯å½“ Transformer åŠ è½½å¥å­æ—¶ï¼Œå®ƒä¸ä¼šæŒ‰é¡ºåºåŠ è½½ï¼Œè€Œæ˜¯å¹¶è¡ŒåŠ è½½ã€‚ç”±äº Transformer æ¶æ„åœ¨å¹¶è¡ŒåŠ è½½æ—¶ä¸åŒ…æ‹¬å•è¯çš„é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ˜ç¡®å®šä¹‰å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚è¿™æœ‰åŠ©äº Transformer ç†è§£å¥å­è¯ä¸è¯ä¹‹é—´çš„ä½ç½®ã€‚è¿™å°±æ˜¯ä½ç½®åµŒå…¥æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚ä½ç½®åµŒå…¥æ˜¯ä¸€ç§å®šä¹‰å•è¯ä½ç½®çš„å‘é‡ç¼–ç ã€‚åœ¨è¿›å…¥æ³¨æ„åŠ›ç½‘ç»œä¹‹å‰ï¼Œå°†æ­¤ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/nlp/attention-is-all-you-need/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nlp/attention-is-all-you-need/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å……"><meta property="og:description" content="å…³äºæœ¬æ–‡åŠ¨æœº
Transformerä¸»è¦å†…å®¹è¯·è§ Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)ï¼Œå¯¹ Transformer å·²ç»è¿›è¡Œæ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»å’Œè®²è§£äº†ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€äº›ç»†èŠ‚é—®é¢˜ä¸å¥½åœ¨è¯¥ç¯‡æ–‡ç« æåŠï¼Œæ‰€ä»¥å•å¼€ä¸€ç¯‡è®¨è®ºã€‚
Qï¼ŒKï¼ŒV çš„ç†è§£
å‡è®¾æˆ‘ä»¬æƒ³è®©æ‰€æœ‰çš„è¯éƒ½ä¸ç¬¬ä¸€ä¸ªè¯ $v_1$ ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥è®© $v_1$ ä½œä¸ºæŸ¥è¯¢ã€‚ ç„¶åï¼Œå°†è¯¥æŸ¥è¯¢ä¸å¥å­ä¸­æ‰€æœ‰è¯è¿›è¡Œç‚¹ç§¯ï¼Œè¿™é‡Œçš„è¯å°±æ˜¯é”®ã€‚ æ‰€ä»¥æŸ¥è¯¢å’Œé”®çš„ç»„åˆç»™äº†æˆ‘ä»¬æƒé‡ï¼Œæ¥ç€å†å°†è¿™äº›æƒé‡ä¸ä½œä¸ºå€¼çš„æ‰€æœ‰å•è¯ç›¸ä¹˜ã€‚
é€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç†è§£æŸ¥è¯¢ã€é”®ã€å€¼åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼š

$$
softmax(QK)=W \\
WV=Y
$$
ä¸€ç§æ¯”è¾ƒæ„Ÿæ€§çš„ç†è§£ï¼šæƒ³è¦å¾—åˆ°æŸä¸ª $V$ å¯¹åº”çš„æŸä¸ªå¯èƒ½çš„ç›¸ä¼¼ä¿¡æ¯éœ€è¦å…ˆ $Q$ è¿™ä¸ª $V$ çš„ $K$ ï¼Œ$QK$ å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¹‹åç»è¿‡ softmax å¹³æ»‘åå¾—åˆ°æ¦‚ç‡ $W $ï¼Œç„¶å $WV$ åå¾—åˆ°æœ€ç»ˆçš„ç›¸ä¼¼ä¿¡æ¯ $Y$ ã€‚
Attention æœºåˆ¶
åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æŸ¥è¯¢ $q$ å’Œé”® $k_i$ æ£€ç´¢æŸä¸ªå€¼ $v_i$ ã€‚æ³¨æ„åŠ›ä¸è¿™ç§æ•°æ®åº“å–å€¼æŠ€æœ¯ç±»ä¼¼ï¼Œä½†æ˜¯ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œçš„ã€‚
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$
æ³¨æ„åŠ›æœºåˆ¶æµ‹é‡æŸ¥è¯¢ $q$ å’Œæ¯ä¸ªé”®å€¼ $k_i$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
è¿”å›æ¯ä¸ªé”®å€¼çš„æƒé‡ä»£è¡¨è¿™ç§ç›¸ä¼¼æ€§ã€‚
æœ€åï¼Œè¿”å›æ‰€æœ‰å€¼çš„åŠ æƒç»„åˆä½œä¸ºè¾“å‡ºã€‚

Mask æ©ç 
åœ¨æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼Œè¿™ç±»ä»»åŠ¡æˆ‘ä»¬ä¸€æ¬¡åªèƒ½çœ‹åˆ°ä¸€ä¸ªå•è¯ã€‚æ­¤æ—¶æ³¨æ„åŠ›åªèƒ½æ”¾åœ¨ä¸‹ä¸€ä¸ªè¯ä¸Šï¼Œä¸èƒ½æ”¾åœ¨ç¬¬äºŒä¸ªè¯æˆ–åé¢çš„è¯ä¸Šã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ³¨æ„åŠ›ä¸èƒ½æœ‰éå¹³å‡¡çš„è¶…å¯¹è§’çº¿åˆ†é‡ã€‚
æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æ©ç çŸ©é˜µæ¥ä¿®æ­£æ³¨æ„åŠ›ï¼Œä»¥æ¶ˆé™¤ç¥ç»ç½‘ç»œå¯¹æœªæ¥çš„äº†è§£ã€‚
Multi-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â€œå°ç¾é•¿å¾—å¾ˆæ¼‚äº®è€Œä¸”äººè¿˜å¾ˆå¥½â€ ã€‚è¿™é‡Œâ€œäººâ€è¿™ä¸ªè¯ï¼Œåœ¨è¯­æ³•ä¸Šä¸â€œå°ç¾â€å’Œâ€œå¥½â€è¿™äº›è¯å­˜åœ¨æŸç§æ„ä¹‰æˆ–å…³è”ã€‚è¿™å¥è¯ä¸­â€œäººâ€è¿™ä¸ªè¯éœ€è¦ç†è§£ä¸ºâ€œäººå“â€ï¼Œè¯´çš„æ˜¯å°ç¾çš„äººå“å¾ˆå¥½ã€‚ä»…ä»…ä½¿ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½æ— æ³•æ­£ç¡®è¯†åˆ«è¿™ä¸‰ä¸ªè¯ä¹‹é—´çš„å…³è”ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸â€œäººâ€ç›¸å…³çš„è¯ã€‚è¿™å‡å°‘äº†æ³¨æ„åŠ›å¯»æ‰¾æ‰€æœ‰é‡è¦è¯çš„è´Ÿæ‹…ï¼Œå¢åŠ æ‰¾åˆ°æ›´å¤šç›¸å…³è¯çš„æœºä¼šã€‚
ä½ç½®ç¼–ç 
åœ¨ä»»ä½•å¥å­ä¸­ï¼Œå•è¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‡ºç°éƒ½è•´å«ç€é‡è¦æ„ä¹‰ã€‚å¦‚æœå¥å­ä¸­çš„å•è¯ä¹±ä¸ƒå…«ç³Ÿï¼Œé‚£ä¹ˆè¿™å¥è¯å¾ˆå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ä½†æ˜¯å½“ Transformer åŠ è½½å¥å­æ—¶ï¼Œå®ƒä¸ä¼šæŒ‰é¡ºåºåŠ è½½ï¼Œè€Œæ˜¯å¹¶è¡ŒåŠ è½½ã€‚ç”±äº Transformer æ¶æ„åœ¨å¹¶è¡ŒåŠ è½½æ—¶ä¸åŒ…æ‹¬å•è¯çš„é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ˜ç¡®å®šä¹‰å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚è¿™æœ‰åŠ©äº Transformer ç†è§£å¥å­è¯ä¸è¯ä¹‹é—´çš„ä½ç½®ã€‚è¿™å°±æ˜¯ä½ç½®åµŒå…¥æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚ä½ç½®åµŒå…¥æ˜¯ä¸€ç§å®šä¹‰å•è¯ä½ç½®çš„å‘é‡ç¼–ç ã€‚åœ¨è¿›å…¥æ³¨æ„åŠ›ç½‘ç»œä¹‹å‰ï¼Œå°†æ­¤ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/nlp/attention-is-all-you-need/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-14T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å……"><meta name=twitter:description content="å…³äºæœ¬æ–‡åŠ¨æœº
Transformerä¸»è¦å†…å®¹è¯·è§ Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)ï¼Œå¯¹ Transformer å·²ç»è¿›è¡Œæ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»å’Œè®²è§£äº†ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€äº›ç»†èŠ‚é—®é¢˜ä¸å¥½åœ¨è¯¥ç¯‡æ–‡ç« æåŠï¼Œæ‰€ä»¥å•å¼€ä¸€ç¯‡è®¨è®ºã€‚
Qï¼ŒKï¼ŒV çš„ç†è§£
å‡è®¾æˆ‘ä»¬æƒ³è®©æ‰€æœ‰çš„è¯éƒ½ä¸ç¬¬ä¸€ä¸ªè¯ $v_1$ ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥è®© $v_1$ ä½œä¸ºæŸ¥è¯¢ã€‚ ç„¶åï¼Œå°†è¯¥æŸ¥è¯¢ä¸å¥å­ä¸­æ‰€æœ‰è¯è¿›è¡Œç‚¹ç§¯ï¼Œè¿™é‡Œçš„è¯å°±æ˜¯é”®ã€‚ æ‰€ä»¥æŸ¥è¯¢å’Œé”®çš„ç»„åˆç»™äº†æˆ‘ä»¬æƒé‡ï¼Œæ¥ç€å†å°†è¿™äº›æƒé‡ä¸ä½œä¸ºå€¼çš„æ‰€æœ‰å•è¯ç›¸ä¹˜ã€‚
é€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç†è§£æŸ¥è¯¢ã€é”®ã€å€¼åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼š

$$
softmax(QK)=W \\
WV=Y
$$
ä¸€ç§æ¯”è¾ƒæ„Ÿæ€§çš„ç†è§£ï¼šæƒ³è¦å¾—åˆ°æŸä¸ª $V$ å¯¹åº”çš„æŸä¸ªå¯èƒ½çš„ç›¸ä¼¼ä¿¡æ¯éœ€è¦å…ˆ $Q$ è¿™ä¸ª $V$ çš„ $K$ ï¼Œ$QK$ å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¹‹åç»è¿‡ softmax å¹³æ»‘åå¾—åˆ°æ¦‚ç‡ $W $ï¼Œç„¶å $WV$ åå¾—åˆ°æœ€ç»ˆçš„ç›¸ä¼¼ä¿¡æ¯ $Y$ ã€‚
Attention æœºåˆ¶
åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æŸ¥è¯¢ $q$ å’Œé”® $k_i$ æ£€ç´¢æŸä¸ªå€¼ $v_i$ ã€‚æ³¨æ„åŠ›ä¸è¿™ç§æ•°æ®åº“å–å€¼æŠ€æœ¯ç±»ä¼¼ï¼Œä½†æ˜¯ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œçš„ã€‚
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$
æ³¨æ„åŠ›æœºåˆ¶æµ‹é‡æŸ¥è¯¢ $q$ å’Œæ¯ä¸ªé”®å€¼ $k_i$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚
è¿”å›æ¯ä¸ªé”®å€¼çš„æƒé‡ä»£è¡¨è¿™ç§ç›¸ä¼¼æ€§ã€‚
æœ€åï¼Œè¿”å›æ‰€æœ‰å€¼çš„åŠ æƒç»„åˆä½œä¸ºè¾“å‡ºã€‚

Mask æ©ç 
åœ¨æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼Œè¿™ç±»ä»»åŠ¡æˆ‘ä»¬ä¸€æ¬¡åªèƒ½çœ‹åˆ°ä¸€ä¸ªå•è¯ã€‚æ­¤æ—¶æ³¨æ„åŠ›åªèƒ½æ”¾åœ¨ä¸‹ä¸€ä¸ªè¯ä¸Šï¼Œä¸èƒ½æ”¾åœ¨ç¬¬äºŒä¸ªè¯æˆ–åé¢çš„è¯ä¸Šã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ³¨æ„åŠ›ä¸èƒ½æœ‰éå¹³å‡¡çš„è¶…å¯¹è§’çº¿åˆ†é‡ã€‚
æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æ©ç çŸ©é˜µæ¥ä¿®æ­£æ³¨æ„åŠ›ï¼Œä»¥æ¶ˆé™¤ç¥ç»ç½‘ç»œå¯¹æœªæ¥çš„äº†è§£ã€‚
Multi-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â€œå°ç¾é•¿å¾—å¾ˆæ¼‚äº®è€Œä¸”äººè¿˜å¾ˆå¥½â€ ã€‚è¿™é‡Œâ€œäººâ€è¿™ä¸ªè¯ï¼Œåœ¨è¯­æ³•ä¸Šä¸â€œå°ç¾â€å’Œâ€œå¥½â€è¿™äº›è¯å­˜åœ¨æŸç§æ„ä¹‰æˆ–å…³è”ã€‚è¿™å¥è¯ä¸­â€œäººâ€è¿™ä¸ªè¯éœ€è¦ç†è§£ä¸ºâ€œäººå“â€ï¼Œè¯´çš„æ˜¯å°ç¾çš„äººå“å¾ˆå¥½ã€‚ä»…ä»…ä½¿ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½æ— æ³•æ­£ç¡®è¯†åˆ«è¿™ä¸‰ä¸ªè¯ä¹‹é—´çš„å…³è”ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸â€œäººâ€ç›¸å…³çš„è¯ã€‚è¿™å‡å°‘äº†æ³¨æ„åŠ›å¯»æ‰¾æ‰€æœ‰é‡è¦è¯çš„è´Ÿæ‹…ï¼Œå¢åŠ æ‰¾åˆ°æ›´å¤šç›¸å…³è¯çš„æœºä¼šã€‚
ä½ç½®ç¼–ç 
åœ¨ä»»ä½•å¥å­ä¸­ï¼Œå•è¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‡ºç°éƒ½è•´å«ç€é‡è¦æ„ä¹‰ã€‚å¦‚æœå¥å­ä¸­çš„å•è¯ä¹±ä¸ƒå…«ç³Ÿï¼Œé‚£ä¹ˆè¿™å¥è¯å¾ˆå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ä½†æ˜¯å½“ Transformer åŠ è½½å¥å­æ—¶ï¼Œå®ƒä¸ä¼šæŒ‰é¡ºåºåŠ è½½ï¼Œè€Œæ˜¯å¹¶è¡ŒåŠ è½½ã€‚ç”±äº Transformer æ¶æ„åœ¨å¹¶è¡ŒåŠ è½½æ—¶ä¸åŒ…æ‹¬å•è¯çš„é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ˜ç¡®å®šä¹‰å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚è¿™æœ‰åŠ©äº Transformer ç†è§£å¥å­è¯ä¸è¯ä¹‹é—´çš„ä½ç½®ã€‚è¿™å°±æ˜¯ä½ç½®åµŒå…¥æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚ä½ç½®åµŒå…¥æ˜¯ä¸€ç§å®šä¹‰å•è¯ä½ç½®çš„å‘é‡ç¼–ç ã€‚åœ¨è¿›å…¥æ³¨æ„åŠ›ç½‘ç»œä¹‹å‰ï¼Œå°†æ­¤ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å……","item":"http://localhost:1313/posts/nlp/attention-is-all-you-need/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å……","name":"ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å……","description":"å…³äºæœ¬æ–‡åŠ¨æœº Transformerä¸»è¦å†…å®¹è¯·è§ Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)ï¼Œå¯¹ Transformer å·²ç»è¿›è¡Œæ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»å’Œè®²è§£äº†ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€äº›ç»†èŠ‚é—®é¢˜ä¸å¥½åœ¨è¯¥ç¯‡æ–‡ç« æåŠï¼Œæ‰€ä»¥å•å¼€ä¸€ç¯‡è®¨è®ºã€‚\nQï¼ŒKï¼ŒV çš„ç†è§£ å‡è®¾æˆ‘ä»¬æƒ³è®©æ‰€æœ‰çš„è¯éƒ½ä¸ç¬¬ä¸€ä¸ªè¯ $v_1$ ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥è®© $v_1$ ä½œä¸ºæŸ¥è¯¢ã€‚ ç„¶åï¼Œå°†è¯¥æŸ¥è¯¢ä¸å¥å­ä¸­æ‰€æœ‰è¯è¿›è¡Œç‚¹ç§¯ï¼Œè¿™é‡Œçš„è¯å°±æ˜¯é”®ã€‚ æ‰€ä»¥æŸ¥è¯¢å’Œé”®çš„ç»„åˆç»™äº†æˆ‘ä»¬æƒé‡ï¼Œæ¥ç€å†å°†è¿™äº›æƒé‡ä¸ä½œä¸ºå€¼çš„æ‰€æœ‰å•è¯ç›¸ä¹˜ã€‚\né€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç†è§£æŸ¥è¯¢ã€é”®ã€å€¼åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼š $$ softmax(QK)=W \\\\ WV=Y $$ ä¸€ç§æ¯”è¾ƒæ„Ÿæ€§çš„ç†è§£ï¼šæƒ³è¦å¾—åˆ°æŸä¸ª $V$ å¯¹åº”çš„æŸä¸ªå¯èƒ½çš„ç›¸ä¼¼ä¿¡æ¯éœ€è¦å…ˆ $Q$ è¿™ä¸ª $V$ çš„ $K$ ï¼Œ$QK$ å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¹‹åç»è¿‡ softmax å¹³æ»‘åå¾—åˆ°æ¦‚ç‡ $W $ï¼Œç„¶å $WV$ åå¾—åˆ°æœ€ç»ˆçš„ç›¸ä¼¼ä¿¡æ¯ $Y$ ã€‚\nAttention æœºåˆ¶ åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æŸ¥è¯¢ $q$ å’Œé”® $k_i$ æ£€ç´¢æŸä¸ªå€¼ $v_i$ ã€‚æ³¨æ„åŠ›ä¸è¿™ç§æ•°æ®åº“å–å€¼æŠ€æœ¯ç±»ä¼¼ï¼Œä½†æ˜¯ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œçš„ã€‚\n$$ attention(q,k,v)=\\sum_isimilarity(q,k_i)v_i $$ æ³¨æ„åŠ›æœºåˆ¶æµ‹é‡æŸ¥è¯¢ $q$ å’Œæ¯ä¸ªé”®å€¼ $k_i$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ è¿”å›æ¯ä¸ªé”®å€¼çš„æƒé‡ä»£è¡¨è¿™ç§ç›¸ä¼¼æ€§ã€‚ æœ€åï¼Œè¿”å›æ‰€æœ‰å€¼çš„åŠ æƒç»„åˆä½œä¸ºè¾“å‡ºã€‚ Mask æ©ç  åœ¨æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼Œè¿™ç±»ä»»åŠ¡æˆ‘ä»¬ä¸€æ¬¡åªèƒ½çœ‹åˆ°ä¸€ä¸ªå•è¯ã€‚æ­¤æ—¶æ³¨æ„åŠ›åªèƒ½æ”¾åœ¨ä¸‹ä¸€ä¸ªè¯ä¸Šï¼Œä¸èƒ½æ”¾åœ¨ç¬¬äºŒä¸ªè¯æˆ–åé¢çš„è¯ä¸Šã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ³¨æ„åŠ›ä¸èƒ½æœ‰éå¹³å‡¡çš„è¶…å¯¹è§’çº¿åˆ†é‡ã€‚\næˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æ©ç çŸ©é˜µæ¥ä¿®æ­£æ³¨æ„åŠ›ï¼Œä»¥æ¶ˆé™¤ç¥ç»ç½‘ç»œå¯¹æœªæ¥çš„äº†è§£ã€‚\nMulti-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ â€œå°ç¾é•¿å¾—å¾ˆæ¼‚äº®è€Œä¸”äººè¿˜å¾ˆå¥½â€ ã€‚è¿™é‡Œâ€œäººâ€è¿™ä¸ªè¯ï¼Œåœ¨è¯­æ³•ä¸Šä¸â€œå°ç¾â€å’Œâ€œå¥½â€è¿™äº›è¯å­˜åœ¨æŸç§æ„ä¹‰æˆ–å…³è”ã€‚è¿™å¥è¯ä¸­â€œäººâ€è¿™ä¸ªè¯éœ€è¦ç†è§£ä¸ºâ€œäººå“â€ï¼Œè¯´çš„æ˜¯å°ç¾çš„äººå“å¾ˆå¥½ã€‚ä»…ä»…ä½¿ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½æ— æ³•æ­£ç¡®è¯†åˆ«è¿™ä¸‰ä¸ªè¯ä¹‹é—´çš„å…³è”ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸â€œäººâ€ç›¸å…³çš„è¯ã€‚è¿™å‡å°‘äº†æ³¨æ„åŠ›å¯»æ‰¾æ‰€æœ‰é‡è¦è¯çš„è´Ÿæ‹…ï¼Œå¢åŠ æ‰¾åˆ°æ›´å¤šç›¸å…³è¯çš„æœºä¼šã€‚\nä½ç½®ç¼–ç  åœ¨ä»»ä½•å¥å­ä¸­ï¼Œå•è¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‡ºç°éƒ½è•´å«ç€é‡è¦æ„ä¹‰ã€‚å¦‚æœå¥å­ä¸­çš„å•è¯ä¹±ä¸ƒå…«ç³Ÿï¼Œé‚£ä¹ˆè¿™å¥è¯å¾ˆå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ä½†æ˜¯å½“ Transformer åŠ è½½å¥å­æ—¶ï¼Œå®ƒä¸ä¼šæŒ‰é¡ºåºåŠ è½½ï¼Œè€Œæ˜¯å¹¶è¡ŒåŠ è½½ã€‚ç”±äº Transformer æ¶æ„åœ¨å¹¶è¡ŒåŠ è½½æ—¶ä¸åŒ…æ‹¬å•è¯çš„é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ˜ç¡®å®šä¹‰å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚è¿™æœ‰åŠ©äº Transformer ç†è§£å¥å­è¯ä¸è¯ä¹‹é—´çš„ä½ç½®ã€‚è¿™å°±æ˜¯ä½ç½®åµŒå…¥æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚ä½ç½®åµŒå…¥æ˜¯ä¸€ç§å®šä¹‰å•è¯ä½ç½®çš„å‘é‡ç¼–ç ã€‚åœ¨è¿›å…¥æ³¨æ„åŠ›ç½‘ç»œä¹‹å‰ï¼Œå°†æ­¤ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚\n","keywords":["NLP"],"articleBody":"å…³äºæœ¬æ–‡åŠ¨æœº Transformerä¸»è¦å†…å®¹è¯·è§ Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)ï¼Œå¯¹ Transformer å·²ç»è¿›è¡Œæ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»å’Œè®²è§£äº†ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€äº›ç»†èŠ‚é—®é¢˜ä¸å¥½åœ¨è¯¥ç¯‡æ–‡ç« æåŠï¼Œæ‰€ä»¥å•å¼€ä¸€ç¯‡è®¨è®ºã€‚\nQï¼ŒKï¼ŒV çš„ç†è§£ å‡è®¾æˆ‘ä»¬æƒ³è®©æ‰€æœ‰çš„è¯éƒ½ä¸ç¬¬ä¸€ä¸ªè¯ $v_1$ ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥è®© $v_1$ ä½œä¸ºæŸ¥è¯¢ã€‚ ç„¶åï¼Œå°†è¯¥æŸ¥è¯¢ä¸å¥å­ä¸­æ‰€æœ‰è¯è¿›è¡Œç‚¹ç§¯ï¼Œè¿™é‡Œçš„è¯å°±æ˜¯é”®ã€‚ æ‰€ä»¥æŸ¥è¯¢å’Œé”®çš„ç»„åˆç»™äº†æˆ‘ä»¬æƒé‡ï¼Œæ¥ç€å†å°†è¿™äº›æƒé‡ä¸ä½œä¸ºå€¼çš„æ‰€æœ‰å•è¯ç›¸ä¹˜ã€‚\né€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç†è§£æŸ¥è¯¢ã€é”®ã€å€¼åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼š $$ softmax(QK)=W \\\\ WV=Y $$ ä¸€ç§æ¯”è¾ƒæ„Ÿæ€§çš„ç†è§£ï¼šæƒ³è¦å¾—åˆ°æŸä¸ª $V$ å¯¹åº”çš„æŸä¸ªå¯èƒ½çš„ç›¸ä¼¼ä¿¡æ¯éœ€è¦å…ˆ $Q$ è¿™ä¸ª $V$ çš„ $K$ ï¼Œ$QK$ å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¹‹åç»è¿‡ softmax å¹³æ»‘åå¾—åˆ°æ¦‚ç‡ $W $ï¼Œç„¶å $WV$ åå¾—åˆ°æœ€ç»ˆçš„ç›¸ä¼¼ä¿¡æ¯ $Y$ ã€‚\nAttention æœºåˆ¶ åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æŸ¥è¯¢ $q$ å’Œé”® $k_i$ æ£€ç´¢æŸä¸ªå€¼ $v_i$ ã€‚æ³¨æ„åŠ›ä¸è¿™ç§æ•°æ®åº“å–å€¼æŠ€æœ¯ç±»ä¼¼ï¼Œä½†æ˜¯ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œçš„ã€‚\n$$ attention(q,k,v)=\\sum_isimilarity(q,k_i)v_i $$ æ³¨æ„åŠ›æœºåˆ¶æµ‹é‡æŸ¥è¯¢ $q$ å’Œæ¯ä¸ªé”®å€¼ $k_i$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚ è¿”å›æ¯ä¸ªé”®å€¼çš„æƒé‡ä»£è¡¨è¿™ç§ç›¸ä¼¼æ€§ã€‚ æœ€åï¼Œè¿”å›æ‰€æœ‰å€¼çš„åŠ æƒç»„åˆä½œä¸ºè¾“å‡ºã€‚ Mask æ©ç  åœ¨æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼Œè¿™ç±»ä»»åŠ¡æˆ‘ä»¬ä¸€æ¬¡åªèƒ½çœ‹åˆ°ä¸€ä¸ªå•è¯ã€‚æ­¤æ—¶æ³¨æ„åŠ›åªèƒ½æ”¾åœ¨ä¸‹ä¸€ä¸ªè¯ä¸Šï¼Œä¸èƒ½æ”¾åœ¨ç¬¬äºŒä¸ªè¯æˆ–åé¢çš„è¯ä¸Šã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ³¨æ„åŠ›ä¸èƒ½æœ‰éå¹³å‡¡çš„è¶…å¯¹è§’çº¿åˆ†é‡ã€‚\næˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æ©ç çŸ©é˜µæ¥ä¿®æ­£æ³¨æ„åŠ›ï¼Œä»¥æ¶ˆé™¤ç¥ç»ç½‘ç»œå¯¹æœªæ¥çš„äº†è§£ã€‚\nMulti-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ â€œå°ç¾é•¿å¾—å¾ˆæ¼‚äº®è€Œä¸”äººè¿˜å¾ˆå¥½â€ ã€‚è¿™é‡Œâ€œäººâ€è¿™ä¸ªè¯ï¼Œåœ¨è¯­æ³•ä¸Šä¸â€œå°ç¾â€å’Œâ€œå¥½â€è¿™äº›è¯å­˜åœ¨æŸç§æ„ä¹‰æˆ–å…³è”ã€‚è¿™å¥è¯ä¸­â€œäººâ€è¿™ä¸ªè¯éœ€è¦ç†è§£ä¸ºâ€œäººå“â€ï¼Œè¯´çš„æ˜¯å°ç¾çš„äººå“å¾ˆå¥½ã€‚ä»…ä»…ä½¿ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½æ— æ³•æ­£ç¡®è¯†åˆ«è¿™ä¸‰ä¸ªè¯ä¹‹é—´çš„å…³è”ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸â€œäººâ€ç›¸å…³çš„è¯ã€‚è¿™å‡å°‘äº†æ³¨æ„åŠ›å¯»æ‰¾æ‰€æœ‰é‡è¦è¯çš„è´Ÿæ‹…ï¼Œå¢åŠ æ‰¾åˆ°æ›´å¤šç›¸å…³è¯çš„æœºä¼šã€‚\nä½ç½®ç¼–ç  åœ¨ä»»ä½•å¥å­ä¸­ï¼Œå•è¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‡ºç°éƒ½è•´å«ç€é‡è¦æ„ä¹‰ã€‚å¦‚æœå¥å­ä¸­çš„å•è¯ä¹±ä¸ƒå…«ç³Ÿï¼Œé‚£ä¹ˆè¿™å¥è¯å¾ˆå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ä½†æ˜¯å½“ Transformer åŠ è½½å¥å­æ—¶ï¼Œå®ƒä¸ä¼šæŒ‰é¡ºåºåŠ è½½ï¼Œè€Œæ˜¯å¹¶è¡ŒåŠ è½½ã€‚ç”±äº Transformer æ¶æ„åœ¨å¹¶è¡ŒåŠ è½½æ—¶ä¸åŒ…æ‹¬å•è¯çš„é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ˜ç¡®å®šä¹‰å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚è¿™æœ‰åŠ©äº Transformer ç†è§£å¥å­è¯ä¸è¯ä¹‹é—´çš„ä½ç½®ã€‚è¿™å°±æ˜¯ä½ç½®åµŒå…¥æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚ä½ç½®åµŒå…¥æ˜¯ä¸€ç§å®šä¹‰å•è¯ä½ç½®çš„å‘é‡ç¼–ç ã€‚åœ¨è¿›å…¥æ³¨æ„åŠ›ç½‘ç»œä¹‹å‰ï¼Œå°†æ­¤ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚\nä½œè€…ä½¿ç”¨äº¤æ›¿æ­£ä½™å¼¦å‡½æ•°æ¥å®šä¹‰ä½ç½®åµŒå…¥ï¼š\nä»£ç å®ç° 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data import math import copy # å¤šå¤´æ³¨æ„åŠ› class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super(MultiHeadAttention, self).__init__() assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def scaled_dot_product_attention(self, Q, K, V, mask=None): attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, -1e9) attn_probs = torch.softmax(attn_scores, dim=-1) output = torch.matmul(attn_probs, V) return output def split_heads(self, x): batch_size, seq_length, d_model = x.size() return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) def combine_heads(self, x): batch_size, _, seq_length, d_k = x.size() return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model) def forward(self, Q, K, V, mask=None): Q = self.split_heads(self.W_q(Q)) K = self.split_heads(self.W_k(K)) V = self.split_heads(self.W_v(V)) attn_output = self.scaled_dot_product_attention(Q, K, V, mask) output = self.W_o(self.combine_heads(attn_output)) return output # ä½ç½®å‰é¦ˆç½‘ç»œ class PositionWiseFeedForward(nn.Module): def __init__(self, d_model, d_ff): super(PositionWiseFeedForward, self).__init__() self.fc1 = nn.Linear(d_model, d_ff) self.fc2 = nn.Linear(d_ff, d_model) self.relu = nn.ReLU() def forward(self, x): return self.fc2(self.relu(self.fc1(x))) # ä½ç½®ç¼–ç  class PositionalEncoding(nn.Module): def __init__(self, d_model, max_seq_length): super(PositionalEncoding, self).__init__() pe = torch.zeros(max_seq_length, d_model) position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) self.register_buffer('pe', pe.unsqueeze(0)) def forward(self, x): return x + self.pe[:, :x.size(1)] # ç¼–ç å™¨ class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super(EncoderLayer, self).__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.feed_forward = PositionWiseFeedForward(d_model, d_ff) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, mask): attn_output = self.self_attn(x, x, x, mask) x = self.norm1(x + self.dropout(attn_output)) ff_output = self.feed_forward(x) x = self.norm2(x + self.dropout(ff_output)) return x # è§£ç å™¨ class DecoderLayer(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super(DecoderLayer, self).__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.cross_attn = MultiHeadAttention(d_model, num_heads) self.feed_forward = PositionWiseFeedForward(d_model, d_ff) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.norm3 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, enc_output, src_mask, tgt_mask): attn_output = self.self_attn(x, x, x, tgt_mask) x = self.norm1(x + self.dropout(attn_output)) attn_output = self.cross_attn(x, enc_output, enc_output, src_mask) x = self.norm2(x + self.dropout(attn_output)) ff_output = self.feed_forward(x) x = self.norm3(x + self.dropout(ff_output)) return x class Transformer(nn.Module): def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout): super(Transformer, self).__init__() self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) self.positional_encoding = PositionalEncoding(d_model, max_seq_length) self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) self.fc = nn.Linear(d_model, tgt_vocab_size) self.dropout = nn.Dropout(dropout) def generate_mask(self, src, tgt): src_mask = (src != 0).unsqueeze(1).unsqueeze(2) tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) seq_length = tgt.size(1) nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool() tgt_mask = tgt_mask \u0026 nopeak_mask return src_mask, tgt_mask def forward(self, src, tgt): src_mask, tgt_mask = self.generate_mask(src, tgt) src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src))) tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))) enc_output = src_embedded for enc_layer in self.encoder_layers: enc_output = enc_layer(enc_output, src_mask) dec_output = tgt_embedded for dec_layer in self.decoder_layers: dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask) output = self.fc(dec_output) return output if __name__ == '__main__': src_vocab_size = 5000 tgt_vocab_size = 5000 d_model = 512 num_heads = 8 num_layers = 6 d_ff = 2048 max_seq_length = 100 dropout = 0.1 transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout) # ç”Ÿæˆéšæœºæ ·æœ¬æ•°æ® src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)) tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)) criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) transformer.train() for epoch in range(100): optimizer.zero_grad() output = transformer(src_data, tgt_data[:, :-1]) loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1)) loss.backward() optimizer.step() print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\") ","wordCount":"840","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-08-14T00:00:00Z","dateModified":"2024-08-14T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nlp/attention-is-all-you-need/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;Â»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">ğŸ˜º Is All You Needâ€”â€”Transformerè¡¥å……</h1><div class=post-meta><span title='2024-08-14 00:00:00 +0000 UTC'>August 14, 2024</span>&nbsp;Â·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/NLP/Attention-Is-All-You-Need.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#å…³äºæœ¬æ–‡åŠ¨æœº>å…³äºæœ¬æ–‡åŠ¨æœº</a></li><li><a href=#qkv-çš„ç†è§£>Qï¼ŒKï¼ŒV çš„ç†è§£</a></li><li><a href=#attention-æœºåˆ¶>Attention æœºåˆ¶</a></li><li><a href=#mask-æ©ç >Mask æ©ç </a></li><li><a href=#multi-head-attention-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶>Multi-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶</a></li><li><a href=#ä½ç½®ç¼–ç >ä½ç½®ç¼–ç </a></li><li><a href=#ä»£ç å®ç°>ä»£ç å®ç°</a></li></ul></nav></div></details></div><div class=post-content><h2 id=å…³äºæœ¬æ–‡åŠ¨æœº>å…³äºæœ¬æ–‡åŠ¨æœº<a hidden class=anchor aria-hidden=true href=#å…³äºæœ¬æ–‡åŠ¨æœº>#</a></h2><p>Transformerä¸»è¦å†…å®¹è¯·è§ <a href=https://kurongtohsaka.github.io/posts/cs224n/lesson_9/>Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)</a>ï¼Œå¯¹ Transformer å·²ç»è¿›è¡Œæ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»å’Œè®²è§£äº†ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€äº›ç»†èŠ‚é—®é¢˜ä¸å¥½åœ¨è¯¥ç¯‡æ–‡ç« æåŠï¼Œæ‰€ä»¥å•å¼€ä¸€ç¯‡è®¨è®ºã€‚</p><h2 id=qkv-çš„ç†è§£>Qï¼ŒKï¼ŒV çš„ç†è§£<a hidden class=anchor aria-hidden=true href=#qkv-çš„ç†è§£>#</a></h2><p>å‡è®¾æˆ‘ä»¬æƒ³è®©æ‰€æœ‰çš„è¯éƒ½ä¸ç¬¬ä¸€ä¸ªè¯ $v_1$ ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥è®© $v_1$ ä½œä¸ºæŸ¥è¯¢ã€‚ ç„¶åï¼Œå°†è¯¥æŸ¥è¯¢ä¸å¥å­ä¸­æ‰€æœ‰è¯è¿›è¡Œç‚¹ç§¯ï¼Œè¿™é‡Œçš„è¯å°±æ˜¯é”®ã€‚ æ‰€ä»¥æŸ¥è¯¢å’Œé”®çš„ç»„åˆç»™äº†æˆ‘ä»¬æƒé‡ï¼Œæ¥ç€å†å°†è¿™äº›æƒé‡ä¸ä½œä¸ºå€¼çš„æ‰€æœ‰å•è¯ç›¸ä¹˜ã€‚</p><p>é€šè¿‡ä¸‹é¢çš„å…¬å¼å¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç†è§£æŸ¥è¯¢ã€é”®ã€å€¼åˆ†åˆ«ä»£è¡¨ä»€ä¹ˆæ„æ€ï¼š</p>$$
softmax(QK)=W \\
WV=Y
$$<p>ä¸€ç§æ¯”è¾ƒæ„Ÿæ€§çš„ç†è§£ï¼šæƒ³è¦å¾—åˆ°æŸä¸ª $V$ å¯¹åº”çš„æŸä¸ªå¯èƒ½çš„ç›¸ä¼¼ä¿¡æ¯éœ€è¦å…ˆ $Q$ è¿™ä¸ª $V$ çš„ $K$ ï¼Œ$QK$ å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼Œä¹‹åç»è¿‡ softmax å¹³æ»‘åå¾—åˆ°æ¦‚ç‡ $W $ï¼Œç„¶å $WV$ åå¾—åˆ°æœ€ç»ˆçš„ç›¸ä¼¼ä¿¡æ¯ $Y$ ã€‚</p><h2 id=attention-æœºåˆ¶>Attention æœºåˆ¶<a hidden class=anchor aria-hidden=true href=#attention-æœºåˆ¶>#</a></h2><p>åœ¨æ•°æ®åº“ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æƒ³é€šè¿‡æŸ¥è¯¢ $q$ å’Œé”® $k_i$ æ£€ç´¢æŸä¸ªå€¼ $v_i$ ã€‚æ³¨æ„åŠ›ä¸è¿™ç§æ•°æ®åº“å–å€¼æŠ€æœ¯ç±»ä¼¼ï¼Œä½†æ˜¯ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œçš„ã€‚</p>$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$<ul><li>æ³¨æ„åŠ›æœºåˆ¶æµ‹é‡æŸ¥è¯¢ $q$ å’Œæ¯ä¸ªé”®å€¼ $k_i$ ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li><li>è¿”å›æ¯ä¸ªé”®å€¼çš„æƒé‡ä»£è¡¨è¿™ç§ç›¸ä¼¼æ€§ã€‚</li><li>æœ€åï¼Œè¿”å›æ‰€æœ‰å€¼çš„åŠ æƒç»„åˆä½œä¸ºè¾“å‡ºã€‚</li></ul><h2 id=mask-æ©ç >Mask æ©ç <a hidden class=anchor aria-hidden=true href=#mask-æ©ç >#</a></h2><p>åœ¨æœºå™¨ç¿»è¯‘æˆ–æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å‡ºç°çš„æ¦‚ç‡ï¼Œè¿™ç±»ä»»åŠ¡æˆ‘ä»¬ä¸€æ¬¡åªèƒ½çœ‹åˆ°ä¸€ä¸ªå•è¯ã€‚æ­¤æ—¶æ³¨æ„åŠ›åªèƒ½æ”¾åœ¨ä¸‹ä¸€ä¸ªè¯ä¸Šï¼Œä¸èƒ½æ”¾åœ¨ç¬¬äºŒä¸ªè¯æˆ–åé¢çš„è¯ä¸Šã€‚ç®€è€Œè¨€ä¹‹ï¼Œæ³¨æ„åŠ›ä¸èƒ½æœ‰éå¹³å‡¡çš„è¶…å¯¹è§’çº¿åˆ†é‡ã€‚</p><p>æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ æ©ç çŸ©é˜µæ¥ä¿®æ­£æ³¨æ„åŠ›ï¼Œä»¥æ¶ˆé™¤ç¥ç»ç½‘ç»œå¯¹æœªæ¥çš„äº†è§£ã€‚</p><h2 id=multi-head-attention-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶>Multi-head Attention å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶<a hidden class=anchor aria-hidden=true href=#multi-head-attention-å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶>#</a></h2><p>â€œå°ç¾é•¿å¾—å¾ˆæ¼‚äº®è€Œä¸”äººè¿˜å¾ˆå¥½â€ ã€‚è¿™é‡Œâ€œäººâ€è¿™ä¸ªè¯ï¼Œåœ¨è¯­æ³•ä¸Šä¸â€œå°ç¾â€å’Œâ€œå¥½â€è¿™äº›è¯å­˜åœ¨æŸç§æ„ä¹‰æˆ–å…³è”ã€‚è¿™å¥è¯ä¸­â€œäººâ€è¿™ä¸ªè¯éœ€è¦ç†è§£ä¸ºâ€œäººå“â€ï¼Œè¯´çš„æ˜¯å°ç¾çš„äººå“å¾ˆå¥½ã€‚ä»…ä»…ä½¿ç”¨ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¯èƒ½æ— æ³•æ­£ç¡®è¯†åˆ«è¿™ä¸‰ä¸ªè¯ä¹‹é—´çš„å…³è”ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºä¸â€œäººâ€ç›¸å…³çš„è¯ã€‚è¿™å‡å°‘äº†æ³¨æ„åŠ›å¯»æ‰¾æ‰€æœ‰é‡è¦è¯çš„è´Ÿæ‹…ï¼Œå¢åŠ æ‰¾åˆ°æ›´å¤šç›¸å…³è¯çš„æœºä¼šã€‚</p><h2 id=ä½ç½®ç¼–ç >ä½ç½®ç¼–ç <a hidden class=anchor aria-hidden=true href=#ä½ç½®ç¼–ç >#</a></h2><p>åœ¨ä»»ä½•å¥å­ä¸­ï¼Œå•è¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°å‡ºç°éƒ½è•´å«ç€é‡è¦æ„ä¹‰ã€‚å¦‚æœå¥å­ä¸­çš„å•è¯ä¹±ä¸ƒå…«ç³Ÿï¼Œé‚£ä¹ˆè¿™å¥è¯å¾ˆå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ä½†æ˜¯å½“ Transformer åŠ è½½å¥å­æ—¶ï¼Œå®ƒä¸ä¼šæŒ‰é¡ºåºåŠ è½½ï¼Œè€Œæ˜¯å¹¶è¡ŒåŠ è½½ã€‚ç”±äº Transformer æ¶æ„åœ¨å¹¶è¡ŒåŠ è½½æ—¶ä¸åŒ…æ‹¬å•è¯çš„é¡ºåºï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æ˜ç¡®å®šä¹‰å•è¯åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚è¿™æœ‰åŠ©äº Transformer ç†è§£å¥å­è¯ä¸è¯ä¹‹é—´çš„ä½ç½®ã€‚è¿™å°±æ˜¯ä½ç½®åµŒå…¥æ´¾ä¸Šç”¨åœºçš„åœ°æ–¹ã€‚ä½ç½®åµŒå…¥æ˜¯ä¸€ç§å®šä¹‰å•è¯ä½ç½®çš„å‘é‡ç¼–ç ã€‚åœ¨è¿›å…¥æ³¨æ„åŠ›ç½‘ç»œä¹‹å‰ï¼Œå°†æ­¤ä½ç½®åµŒå…¥æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚</p><p>ä½œè€…ä½¿ç”¨äº¤æ›¿æ­£ä½™å¼¦å‡½æ•°æ¥å®šä¹‰ä½ç½®åµŒå…¥ï¼š</p><p><img loading=lazy src=/img/NLP/img16.png alt></p><h2 id=ä»£ç å®ç°>ä»£ç å®ç°<a hidden class=anchor aria-hidden=true href=#ä»£ç å®ç°>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>  1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>  2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>  3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>  4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5>  5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6>  6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7>  7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8>  8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9>  9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10> 10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11> 11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12> 12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13> 13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14> 14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15> 15</a>
</span><span class=lnt id=hl-0-16><a class=lnlinks href=#hl-0-16> 16</a>
</span><span class=lnt id=hl-0-17><a class=lnlinks href=#hl-0-17> 17</a>
</span><span class=lnt id=hl-0-18><a class=lnlinks href=#hl-0-18> 18</a>
</span><span class=lnt id=hl-0-19><a class=lnlinks href=#hl-0-19> 19</a>
</span><span class=lnt id=hl-0-20><a class=lnlinks href=#hl-0-20> 20</a>
</span><span class=lnt id=hl-0-21><a class=lnlinks href=#hl-0-21> 21</a>
</span><span class=lnt id=hl-0-22><a class=lnlinks href=#hl-0-22> 22</a>
</span><span class=lnt id=hl-0-23><a class=lnlinks href=#hl-0-23> 23</a>
</span><span class=lnt id=hl-0-24><a class=lnlinks href=#hl-0-24> 24</a>
</span><span class=lnt id=hl-0-25><a class=lnlinks href=#hl-0-25> 25</a>
</span><span class=lnt id=hl-0-26><a class=lnlinks href=#hl-0-26> 26</a>
</span><span class=lnt id=hl-0-27><a class=lnlinks href=#hl-0-27> 27</a>
</span><span class=lnt id=hl-0-28><a class=lnlinks href=#hl-0-28> 28</a>
</span><span class=lnt id=hl-0-29><a class=lnlinks href=#hl-0-29> 29</a>
</span><span class=lnt id=hl-0-30><a class=lnlinks href=#hl-0-30> 30</a>
</span><span class=lnt id=hl-0-31><a class=lnlinks href=#hl-0-31> 31</a>
</span><span class=lnt id=hl-0-32><a class=lnlinks href=#hl-0-32> 32</a>
</span><span class=lnt id=hl-0-33><a class=lnlinks href=#hl-0-33> 33</a>
</span><span class=lnt id=hl-0-34><a class=lnlinks href=#hl-0-34> 34</a>
</span><span class=lnt id=hl-0-35><a class=lnlinks href=#hl-0-35> 35</a>
</span><span class=lnt id=hl-0-36><a class=lnlinks href=#hl-0-36> 36</a>
</span><span class=lnt id=hl-0-37><a class=lnlinks href=#hl-0-37> 37</a>
</span><span class=lnt id=hl-0-38><a class=lnlinks href=#hl-0-38> 38</a>
</span><span class=lnt id=hl-0-39><a class=lnlinks href=#hl-0-39> 39</a>
</span><span class=lnt id=hl-0-40><a class=lnlinks href=#hl-0-40> 40</a>
</span><span class=lnt id=hl-0-41><a class=lnlinks href=#hl-0-41> 41</a>
</span><span class=lnt id=hl-0-42><a class=lnlinks href=#hl-0-42> 42</a>
</span><span class=lnt id=hl-0-43><a class=lnlinks href=#hl-0-43> 43</a>
</span><span class=lnt id=hl-0-44><a class=lnlinks href=#hl-0-44> 44</a>
</span><span class=lnt id=hl-0-45><a class=lnlinks href=#hl-0-45> 45</a>
</span><span class=lnt id=hl-0-46><a class=lnlinks href=#hl-0-46> 46</a>
</span><span class=lnt id=hl-0-47><a class=lnlinks href=#hl-0-47> 47</a>
</span><span class=lnt id=hl-0-48><a class=lnlinks href=#hl-0-48> 48</a>
</span><span class=lnt id=hl-0-49><a class=lnlinks href=#hl-0-49> 49</a>
</span><span class=lnt id=hl-0-50><a class=lnlinks href=#hl-0-50> 50</a>
</span><span class=lnt id=hl-0-51><a class=lnlinks href=#hl-0-51> 51</a>
</span><span class=lnt id=hl-0-52><a class=lnlinks href=#hl-0-52> 52</a>
</span><span class=lnt id=hl-0-53><a class=lnlinks href=#hl-0-53> 53</a>
</span><span class=lnt id=hl-0-54><a class=lnlinks href=#hl-0-54> 54</a>
</span><span class=lnt id=hl-0-55><a class=lnlinks href=#hl-0-55> 55</a>
</span><span class=lnt id=hl-0-56><a class=lnlinks href=#hl-0-56> 56</a>
</span><span class=lnt id=hl-0-57><a class=lnlinks href=#hl-0-57> 57</a>
</span><span class=lnt id=hl-0-58><a class=lnlinks href=#hl-0-58> 58</a>
</span><span class=lnt id=hl-0-59><a class=lnlinks href=#hl-0-59> 59</a>
</span><span class=lnt id=hl-0-60><a class=lnlinks href=#hl-0-60> 60</a>
</span><span class=lnt id=hl-0-61><a class=lnlinks href=#hl-0-61> 61</a>
</span><span class=lnt id=hl-0-62><a class=lnlinks href=#hl-0-62> 62</a>
</span><span class=lnt id=hl-0-63><a class=lnlinks href=#hl-0-63> 63</a>
</span><span class=lnt id=hl-0-64><a class=lnlinks href=#hl-0-64> 64</a>
</span><span class=lnt id=hl-0-65><a class=lnlinks href=#hl-0-65> 65</a>
</span><span class=lnt id=hl-0-66><a class=lnlinks href=#hl-0-66> 66</a>
</span><span class=lnt id=hl-0-67><a class=lnlinks href=#hl-0-67> 67</a>
</span><span class=lnt id=hl-0-68><a class=lnlinks href=#hl-0-68> 68</a>
</span><span class=lnt id=hl-0-69><a class=lnlinks href=#hl-0-69> 69</a>
</span><span class=lnt id=hl-0-70><a class=lnlinks href=#hl-0-70> 70</a>
</span><span class=lnt id=hl-0-71><a class=lnlinks href=#hl-0-71> 71</a>
</span><span class=lnt id=hl-0-72><a class=lnlinks href=#hl-0-72> 72</a>
</span><span class=lnt id=hl-0-73><a class=lnlinks href=#hl-0-73> 73</a>
</span><span class=lnt id=hl-0-74><a class=lnlinks href=#hl-0-74> 74</a>
</span><span class=lnt id=hl-0-75><a class=lnlinks href=#hl-0-75> 75</a>
</span><span class=lnt id=hl-0-76><a class=lnlinks href=#hl-0-76> 76</a>
</span><span class=lnt id=hl-0-77><a class=lnlinks href=#hl-0-77> 77</a>
</span><span class=lnt id=hl-0-78><a class=lnlinks href=#hl-0-78> 78</a>
</span><span class=lnt id=hl-0-79><a class=lnlinks href=#hl-0-79> 79</a>
</span><span class=lnt id=hl-0-80><a class=lnlinks href=#hl-0-80> 80</a>
</span><span class=lnt id=hl-0-81><a class=lnlinks href=#hl-0-81> 81</a>
</span><span class=lnt id=hl-0-82><a class=lnlinks href=#hl-0-82> 82</a>
</span><span class=lnt id=hl-0-83><a class=lnlinks href=#hl-0-83> 83</a>
</span><span class=lnt id=hl-0-84><a class=lnlinks href=#hl-0-84> 84</a>
</span><span class=lnt id=hl-0-85><a class=lnlinks href=#hl-0-85> 85</a>
</span><span class=lnt id=hl-0-86><a class=lnlinks href=#hl-0-86> 86</a>
</span><span class=lnt id=hl-0-87><a class=lnlinks href=#hl-0-87> 87</a>
</span><span class=lnt id=hl-0-88><a class=lnlinks href=#hl-0-88> 88</a>
</span><span class=lnt id=hl-0-89><a class=lnlinks href=#hl-0-89> 89</a>
</span><span class=lnt id=hl-0-90><a class=lnlinks href=#hl-0-90> 90</a>
</span><span class=lnt id=hl-0-91><a class=lnlinks href=#hl-0-91> 91</a>
</span><span class=lnt id=hl-0-92><a class=lnlinks href=#hl-0-92> 92</a>
</span><span class=lnt id=hl-0-93><a class=lnlinks href=#hl-0-93> 93</a>
</span><span class=lnt id=hl-0-94><a class=lnlinks href=#hl-0-94> 94</a>
</span><span class=lnt id=hl-0-95><a class=lnlinks href=#hl-0-95> 95</a>
</span><span class=lnt id=hl-0-96><a class=lnlinks href=#hl-0-96> 96</a>
</span><span class=lnt id=hl-0-97><a class=lnlinks href=#hl-0-97> 97</a>
</span><span class=lnt id=hl-0-98><a class=lnlinks href=#hl-0-98> 98</a>
</span><span class=lnt id=hl-0-99><a class=lnlinks href=#hl-0-99> 99</a>
</span><span class=lnt id=hl-0-100><a class=lnlinks href=#hl-0-100>100</a>
</span><span class=lnt id=hl-0-101><a class=lnlinks href=#hl-0-101>101</a>
</span><span class=lnt id=hl-0-102><a class=lnlinks href=#hl-0-102>102</a>
</span><span class=lnt id=hl-0-103><a class=lnlinks href=#hl-0-103>103</a>
</span><span class=lnt id=hl-0-104><a class=lnlinks href=#hl-0-104>104</a>
</span><span class=lnt id=hl-0-105><a class=lnlinks href=#hl-0-105>105</a>
</span><span class=lnt id=hl-0-106><a class=lnlinks href=#hl-0-106>106</a>
</span><span class=lnt id=hl-0-107><a class=lnlinks href=#hl-0-107>107</a>
</span><span class=lnt id=hl-0-108><a class=lnlinks href=#hl-0-108>108</a>
</span><span class=lnt id=hl-0-109><a class=lnlinks href=#hl-0-109>109</a>
</span><span class=lnt id=hl-0-110><a class=lnlinks href=#hl-0-110>110</a>
</span><span class=lnt id=hl-0-111><a class=lnlinks href=#hl-0-111>111</a>
</span><span class=lnt id=hl-0-112><a class=lnlinks href=#hl-0-112>112</a>
</span><span class=lnt id=hl-0-113><a class=lnlinks href=#hl-0-113>113</a>
</span><span class=lnt id=hl-0-114><a class=lnlinks href=#hl-0-114>114</a>
</span><span class=lnt id=hl-0-115><a class=lnlinks href=#hl-0-115>115</a>
</span><span class=lnt id=hl-0-116><a class=lnlinks href=#hl-0-116>116</a>
</span><span class=lnt id=hl-0-117><a class=lnlinks href=#hl-0-117>117</a>
</span><span class=lnt id=hl-0-118><a class=lnlinks href=#hl-0-118>118</a>
</span><span class=lnt id=hl-0-119><a class=lnlinks href=#hl-0-119>119</a>
</span><span class=lnt id=hl-0-120><a class=lnlinks href=#hl-0-120>120</a>
</span><span class=lnt id=hl-0-121><a class=lnlinks href=#hl-0-121>121</a>
</span><span class=lnt id=hl-0-122><a class=lnlinks href=#hl-0-122>122</a>
</span><span class=lnt id=hl-0-123><a class=lnlinks href=#hl-0-123>123</a>
</span><span class=lnt id=hl-0-124><a class=lnlinks href=#hl-0-124>124</a>
</span><span class=lnt id=hl-0-125><a class=lnlinks href=#hl-0-125>125</a>
</span><span class=lnt id=hl-0-126><a class=lnlinks href=#hl-0-126>126</a>
</span><span class=lnt id=hl-0-127><a class=lnlinks href=#hl-0-127>127</a>
</span><span class=lnt id=hl-0-128><a class=lnlinks href=#hl-0-128>128</a>
</span><span class=lnt id=hl-0-129><a class=lnlinks href=#hl-0-129>129</a>
</span><span class=lnt id=hl-0-130><a class=lnlinks href=#hl-0-130>130</a>
</span><span class=lnt id=hl-0-131><a class=lnlinks href=#hl-0-131>131</a>
</span><span class=lnt id=hl-0-132><a class=lnlinks href=#hl-0-132>132</a>
</span><span class=lnt id=hl-0-133><a class=lnlinks href=#hl-0-133>133</a>
</span><span class=lnt id=hl-0-134><a class=lnlinks href=#hl-0-134>134</a>
</span><span class=lnt id=hl-0-135><a class=lnlinks href=#hl-0-135>135</a>
</span><span class=lnt id=hl-0-136><a class=lnlinks href=#hl-0-136>136</a>
</span><span class=lnt id=hl-0-137><a class=lnlinks href=#hl-0-137>137</a>
</span><span class=lnt id=hl-0-138><a class=lnlinks href=#hl-0-138>138</a>
</span><span class=lnt id=hl-0-139><a class=lnlinks href=#hl-0-139>139</a>
</span><span class=lnt id=hl-0-140><a class=lnlinks href=#hl-0-140>140</a>
</span><span class=lnt id=hl-0-141><a class=lnlinks href=#hl-0-141>141</a>
</span><span class=lnt id=hl-0-142><a class=lnlinks href=#hl-0-142>142</a>
</span><span class=lnt id=hl-0-143><a class=lnlinks href=#hl-0-143>143</a>
</span><span class=lnt id=hl-0-144><a class=lnlinks href=#hl-0-144>144</a>
</span><span class=lnt id=hl-0-145><a class=lnlinks href=#hl-0-145>145</a>
</span><span class=lnt id=hl-0-146><a class=lnlinks href=#hl-0-146>146</a>
</span><span class=lnt id=hl-0-147><a class=lnlinks href=#hl-0-147>147</a>
</span><span class=lnt id=hl-0-148><a class=lnlinks href=#hl-0-148>148</a>
</span><span class=lnt id=hl-0-149><a class=lnlinks href=#hl-0-149>149</a>
</span><span class=lnt id=hl-0-150><a class=lnlinks href=#hl-0-150>150</a>
</span><span class=lnt id=hl-0-151><a class=lnlinks href=#hl-0-151>151</a>
</span><span class=lnt id=hl-0-152><a class=lnlinks href=#hl-0-152>152</a>
</span><span class=lnt id=hl-0-153><a class=lnlinks href=#hl-0-153>153</a>
</span><span class=lnt id=hl-0-154><a class=lnlinks href=#hl-0-154>154</a>
</span><span class=lnt id=hl-0-155><a class=lnlinks href=#hl-0-155>155</a>
</span><span class=lnt id=hl-0-156><a class=lnlinks href=#hl-0-156>156</a>
</span><span class=lnt id=hl-0-157><a class=lnlinks href=#hl-0-157>157</a>
</span><span class=lnt id=hl-0-158><a class=lnlinks href=#hl-0-158>158</a>
</span><span class=lnt id=hl-0-159><a class=lnlinks href=#hl-0-159>159</a>
</span><span class=lnt id=hl-0-160><a class=lnlinks href=#hl-0-160>160</a>
</span><span class=lnt id=hl-0-161><a class=lnlinks href=#hl-0-161>161</a>
</span><span class=lnt id=hl-0-162><a class=lnlinks href=#hl-0-162>162</a>
</span><span class=lnt id=hl-0-163><a class=lnlinks href=#hl-0-163>163</a>
</span><span class=lnt id=hl-0-164><a class=lnlinks href=#hl-0-164>164</a>
</span><span class=lnt id=hl-0-165><a class=lnlinks href=#hl-0-165>165</a>
</span><span class=lnt id=hl-0-166><a class=lnlinks href=#hl-0-166>166</a>
</span><span class=lnt id=hl-0-167><a class=lnlinks href=#hl-0-167>167</a>
</span><span class=lnt id=hl-0-168><a class=lnlinks href=#hl-0-168>168</a>
</span><span class=lnt id=hl-0-169><a class=lnlinks href=#hl-0-169>169</a>
</span><span class=lnt id=hl-0-170><a class=lnlinks href=#hl-0-170>170</a>
</span><span class=lnt id=hl-0-171><a class=lnlinks href=#hl-0-171>171</a>
</span><span class=lnt id=hl-0-172><a class=lnlinks href=#hl-0-172>172</a>
</span><span class=lnt id=hl-0-173><a class=lnlinks href=#hl-0-173>173</a>
</span><span class=lnt id=hl-0-174><a class=lnlinks href=#hl-0-174>174</a>
</span><span class=lnt id=hl-0-175><a class=lnlinks href=#hl-0-175>175</a>
</span><span class=lnt id=hl-0-176><a class=lnlinks href=#hl-0-176>176</a>
</span><span class=lnt id=hl-0-177><a class=lnlinks href=#hl-0-177>177</a>
</span><span class=lnt id=hl-0-178><a class=lnlinks href=#hl-0-178>178</a>
</span><span class=lnt id=hl-0-179><a class=lnlinks href=#hl-0-179>179</a>
</span><span class=lnt id=hl-0-180><a class=lnlinks href=#hl-0-180>180</a>
</span><span class=lnt id=hl-0-181><a class=lnlinks href=#hl-0-181>181</a>
</span><span class=lnt id=hl-0-182><a class=lnlinks href=#hl-0-182>182</a>
</span><span class=lnt id=hl-0-183><a class=lnlinks href=#hl-0-183>183</a>
</span><span class=lnt id=hl-0-184><a class=lnlinks href=#hl-0-184>184</a>
</span><span class=lnt id=hl-0-185><a class=lnlinks href=#hl-0-185>185</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.optim</span> <span class=k>as</span> <span class=nn>optim</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.utils.data</span> <span class=k>as</span> <span class=nn>data</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>copy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># å¤šå¤´æ³¨æ„åŠ›</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>MultiHeadAttention</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>num_heads</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;d_model must be divisible by num_heads&#34;</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>scaled_dot_product_attention</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_probs</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>combine_heads</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>d_k</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>Q</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>K</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>V</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>combine_heads</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ä½ç½®å‰é¦ˆç½‘ç»œ      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionWiseFeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionWiseFeedForward</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=c1># ä½ç½®ç¼–ç       </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_seq_length</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=o>-</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;pe&#39;</span><span class=p>,</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:,</span> <span class=p>:</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ç¼–ç å™¨      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>EncoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>PositionWiseFeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>ff_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>ff_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># è§£ç å™¨      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>DecoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>PositionWiseFeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>ff_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>ff_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Transformer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Transformer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>positional_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>EncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>DecoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate_mask</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>src</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>tgt</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_length</span> <span class=o>=</span> <span class=n>tgt</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>nopeak_mask</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>),</span> <span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>bool</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_mask</span> <span class=o>=</span> <span class=n>tgt_mask</span> <span class=o>&amp;</span> <span class=n>nopeak_mask</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_mask</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>src_embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>positional_encoding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encoder_embedding</span><span class=p>(</span><span class=n>src</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>positional_encoding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>decoder_embedding</span><span class=p>(</span><span class=n>tgt</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>enc_output</span> <span class=o>=</span> <span class=n>src_embedded</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>enc_layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>enc_output</span> <span class=o>=</span> <span class=n>enc_layer</span><span class=p>(</span><span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>dec_output</span> <span class=o>=</span> <span class=n>tgt_embedded</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>dec_layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder_layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>dec_output</span> <span class=o>=</span> <span class=n>dec_layer</span><span class=p>(</span><span class=n>dec_output</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>dec_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>src_vocab_size</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl>  <span class=n>tgt_vocab_size</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl>  <span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>  <span class=n>num_heads</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>  <span class=n>num_layers</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>  <span class=n>d_ff</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>  <span class=n>max_seq_length</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>  <span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>transformer</span> <span class=o>=</span> <span class=n>Transformer</span><span class=p>(</span><span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># ç”Ÿæˆéšæœºæ ·æœ¬æ•°æ®</span>
</span></span><span class=line><span class=cl>  <span class=n>src_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>,</span> <span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>))</span>  
</span></span><span class=line><span class=cl>  <span class=n>tgt_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>ignore_index</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>transformer</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.0001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.98</span><span class=p>),</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>transformer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=n>output</span> <span class=o>=</span> <span class=n>transformer</span><span class=p>(</span><span class=n>src_data</span><span class=p>,</span> <span class=n>tgt_data</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>),</span> <span class=n>tgt_data</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch: </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>, Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/papernotes/%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E6%9D%8E%E5%86%AC%E6%A2%85/><span class=title>Â« Prev</span><br><span>ã€Šå®ä½“å…³ç³»æŠ½å–æ–¹æ³•ç ”ç©¶ç»¼è¿°ã€‹ç¬”è®°</span>
</a><a class=next href=http://localhost:1313/posts/cs224n/lesson_9/><span class=title>Next Â»</span><br><span>Lecture 9: Transformer</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>