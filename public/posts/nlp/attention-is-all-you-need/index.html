<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>😺 Is All You Need——Transformer补充 | KurongBlog</title>
<meta name=keywords content="NLP"><meta name=description content="关于本文动机
Transformer主要内容请见 Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。
Q，K，V 的理解
假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。
通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思：

$$
softmax(QK)=W \\
WV=Y
$$
一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。
Attention 机制
在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$
注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。
返回每个键值的权重代表这种相似性。
最后，返回所有值的加权组合作为输出。

Mask 掩码
在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。
我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。
Multi-head Attention 多头注意力机制
“小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。
位置编码
在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/nlp/attention-is-all-you-need/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/nlp/attention-is-all-you-need/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="😺 Is All You Need——Transformer补充"><meta property="og:description" content="关于本文动机
Transformer主要内容请见 Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。
Q，K，V 的理解
假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。
通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思：

$$
softmax(QK)=W \\
WV=Y
$$
一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。
Attention 机制
在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$
注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。
返回每个键值的权重代表这种相似性。
最后，返回所有值的加权组合作为输出。

Mask 掩码
在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。
我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。
Multi-head Attention 多头注意力机制
“小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。
位置编码
在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/nlp/attention-is-all-you-need/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-14T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="😺 Is All You Need——Transformer补充"><meta name=twitter:description content="关于本文动机
Transformer主要内容请见 Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。
Q，K，V 的理解
假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。
通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思：

$$
softmax(QK)=W \\
WV=Y
$$
一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。
Attention 机制
在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。
$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$
注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。
返回每个键值的权重代表这种相似性。
最后，返回所有值的加权组合作为输出。

Mask 掩码
在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。
我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。
Multi-head Attention 多头注意力机制
“小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。
位置编码
在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"😺 Is All You Need——Transformer补充","item":"http://localhost:1313/posts/nlp/attention-is-all-you-need/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"😺 Is All You Need——Transformer补充","name":"😺 Is All You Need——Transformer补充","description":"关于本文动机 Transformer主要内容请见 Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。\nQ，K，V 的理解 假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。\n通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思： $$ softmax(QK)=W \\\\ WV=Y $$ 一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。\nAttention 机制 在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。\n$$ attention(q,k,v)=\\sum_isimilarity(q,k_i)v_i $$ 注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。 返回每个键值的权重代表这种相似性。 最后，返回所有值的加权组合作为输出。 Mask 掩码 在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。\n我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。\nMulti-head Attention 多头注意力机制 “小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。\n位置编码 在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。\n","keywords":["NLP"],"articleBody":"关于本文动机 Transformer主要内容请见 Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。\nQ，K，V 的理解 假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。\n通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思： $$ softmax(QK)=W \\\\ WV=Y $$ 一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。\nAttention 机制 在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。\n$$ attention(q,k,v)=\\sum_isimilarity(q,k_i)v_i $$ 注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。 返回每个键值的权重代表这种相似性。 最后，返回所有值的加权组合作为输出。 Mask 掩码 在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。\n我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。\nMulti-head Attention 多头注意力机制 “小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。\n位置编码 在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。\n作者使用交替正余弦函数来定义位置嵌入：\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data import math import copy # 多头注意力 class MultiHeadAttention(nn.Module): def __init__(self, d_model, num_heads): super(MultiHeadAttention, self).__init__() assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\" self.d_model = d_model self.num_heads = num_heads self.d_k = d_model // num_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def scaled_dot_product_attention(self, Q, K, V, mask=None): attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, -1e9) attn_probs = torch.softmax(attn_scores, dim=-1) output = torch.matmul(attn_probs, V) return output def split_heads(self, x): batch_size, seq_length, d_model = x.size() return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) def combine_heads(self, x): batch_size, _, seq_length, d_k = x.size() return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model) def forward(self, Q, K, V, mask=None): Q = self.split_heads(self.W_q(Q)) K = self.split_heads(self.W_k(K)) V = self.split_heads(self.W_v(V)) attn_output = self.scaled_dot_product_attention(Q, K, V, mask) output = self.W_o(self.combine_heads(attn_output)) return output # 位置前馈网络 class PositionWiseFeedForward(nn.Module): def __init__(self, d_model, d_ff): super(PositionWiseFeedForward, self).__init__() self.fc1 = nn.Linear(d_model, d_ff) self.fc2 = nn.Linear(d_ff, d_model) self.relu = nn.ReLU() def forward(self, x): return self.fc2(self.relu(self.fc1(x))) # 位置编码 class PositionalEncoding(nn.Module): def __init__(self, d_model, max_seq_length): super(PositionalEncoding, self).__init__() pe = torch.zeros(max_seq_length, d_model) position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) self.register_buffer('pe', pe.unsqueeze(0)) def forward(self, x): return x + self.pe[:, :x.size(1)] # 编码器 class EncoderLayer(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super(EncoderLayer, self).__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.feed_forward = PositionWiseFeedForward(d_model, d_ff) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, mask): attn_output = self.self_attn(x, x, x, mask) x = self.norm1(x + self.dropout(attn_output)) ff_output = self.feed_forward(x) x = self.norm2(x + self.dropout(ff_output)) return x # 解码器 class DecoderLayer(nn.Module): def __init__(self, d_model, num_heads, d_ff, dropout): super(DecoderLayer, self).__init__() self.self_attn = MultiHeadAttention(d_model, num_heads) self.cross_attn = MultiHeadAttention(d_model, num_heads) self.feed_forward = PositionWiseFeedForward(d_model, d_ff) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.norm3 = nn.LayerNorm(d_model) self.dropout = nn.Dropout(dropout) def forward(self, x, enc_output, src_mask, tgt_mask): attn_output = self.self_attn(x, x, x, tgt_mask) x = self.norm1(x + self.dropout(attn_output)) attn_output = self.cross_attn(x, enc_output, enc_output, src_mask) x = self.norm2(x + self.dropout(attn_output)) ff_output = self.feed_forward(x) x = self.norm3(x + self.dropout(ff_output)) return x class Transformer(nn.Module): def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout): super(Transformer, self).__init__() self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) self.positional_encoding = PositionalEncoding(d_model, max_seq_length) self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) self.fc = nn.Linear(d_model, tgt_vocab_size) self.dropout = nn.Dropout(dropout) def generate_mask(self, src, tgt): src_mask = (src != 0).unsqueeze(1).unsqueeze(2) tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) seq_length = tgt.size(1) nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool() tgt_mask = tgt_mask \u0026 nopeak_mask return src_mask, tgt_mask def forward(self, src, tgt): src_mask, tgt_mask = self.generate_mask(src, tgt) src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src))) tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt))) enc_output = src_embedded for enc_layer in self.encoder_layers: enc_output = enc_layer(enc_output, src_mask) dec_output = tgt_embedded for dec_layer in self.decoder_layers: dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask) output = self.fc(dec_output) return output if __name__ == '__main__': src_vocab_size = 5000 tgt_vocab_size = 5000 d_model = 512 num_heads = 8 num_layers = 6 d_ff = 2048 max_seq_length = 100 dropout = 0.1 transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout) # 生成随机样本数据 src_data = torch.randint(1, src_vocab_size, (64, max_seq_length)) tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length)) criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9) transformer.train() for epoch in range(100): optimizer.zero_grad() output = transformer(src_data, tgt_data[:, :-1]) loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1)) loss.backward() optimizer.step() print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\") ","wordCount":"840","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-08-14T00:00:00Z","dateModified":"2024-08-14T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/nlp/attention-is-all-you-need/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">😺 Is All You Need——Transformer补充</h1><div class=post-meta><span title='2024-08-14 00:00:00 +0000 UTC'>August 14, 2024</span>&nbsp;·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/NLP/Attention-Is-All-You-Need.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#关于本文动机>关于本文动机</a></li><li><a href=#qkv-的理解>Q，K，V 的理解</a></li><li><a href=#attention-机制>Attention 机制</a></li><li><a href=#mask-掩码>Mask 掩码</a></li><li><a href=#multi-head-attention-多头注意力机制>Multi-head Attention 多头注意力机制</a></li><li><a href=#位置编码>位置编码</a></li><li><a href=#代码实现>代码实现</a></li></ul></nav></div></details></div><div class=post-content><h2 id=关于本文动机>关于本文动机<a hidden class=anchor aria-hidden=true href=#关于本文动机>#</a></h2><p>Transformer主要内容请见 <a href=https://kurongtohsaka.github.io/posts/cs224n/lesson_9/>Lecture 9: Transformer | KurongBlog (kurongtohsaka.github.io)</a>，对 Transformer 已经进行比较详细的介绍和讲解了，但还是有一些细节问题不好在该篇文章提及，所以单开一篇讨论。</p><h2 id=qkv-的理解>Q，K，V 的理解<a hidden class=anchor aria-hidden=true href=#qkv-的理解>#</a></h2><p>假设我们想让所有的词都与第一个词 $v_1$ 相似，我们可以让 $v_1$ 作为查询。 然后，将该查询与句子中所有词进行点积，这里的词就是键。 所以查询和键的组合给了我们权重，接着再将这些权重与作为值的所有单词相乘。</p><p>通过下面的公式可以理解这个过程，并理解查询、键、值分别代表什么意思：</p>$$
softmax(QK)=W \\
WV=Y
$$<p>一种比较感性的理解：想要得到某个 $V$ 对应的某个可能的相似信息需要先 $Q$ 这个 $V$ 的 $K$ ，$QK$ 得到注意力分数，之后经过 softmax 平滑后得到概率 $W $，然后 $WV$ 后得到最终的相似信息 $Y$ 。</p><h2 id=attention-机制>Attention 机制<a hidden class=anchor aria-hidden=true href=#attention-机制>#</a></h2><p>在数据库中，如果我们想通过查询 $q$ 和键 $k_i$ 检索某个值 $v_i$ 。注意力与这种数据库取值技术类似，但是以概率的方式进行的。</p>$$
attention(q,k,v)=\sum_isimilarity(q,k_i)v_i
$$<ul><li>注意力机制测量查询 $q$ 和每个键值 $k_i$ 之间的相似性。</li><li>返回每个键值的权重代表这种相似性。</li><li>最后，返回所有值的加权组合作为输出。</li></ul><h2 id=mask-掩码>Mask 掩码<a hidden class=anchor aria-hidden=true href=#mask-掩码>#</a></h2><p>在机器翻译或文本生成任务中，我们经常需要预测下一个单词出现的概率，这类任务我们一次只能看到一个单词。此时注意力只能放在下一个词上，不能放在第二个词或后面的词上。简而言之，注意力不能有非平凡的超对角线分量。</p><p>我们可以通过添加掩码矩阵来修正注意力，以消除神经网络对未来的了解。</p><h2 id=multi-head-attention-多头注意力机制>Multi-head Attention 多头注意力机制<a hidden class=anchor aria-hidden=true href=#multi-head-attention-多头注意力机制>#</a></h2><p>“小美长得很漂亮而且人还很好” 。这里“人”这个词，在语法上与“小美”和“好”这些词存在某种意义或关联。这句话中“人”这个词需要理解为“人品”，说的是小美的人品很好。仅仅使用一个注意力机制可能无法正确识别这三个词之间的关联，这种情况下，使用多个注意力可以更好地表示与“人”相关的词。这减少了注意力寻找所有重要词的负担，增加找到更多相关词的机会。</p><h2 id=位置编码>位置编码<a hidden class=anchor aria-hidden=true href=#位置编码>#</a></h2><p>在任何句子中，单词一个接一个地出现都蕴含着重要意义。如果句子中的单词乱七八糟，那么这句话很可能没有意义。但是当 Transformer 加载句子时，它不会按顺序加载，而是并行加载。由于 Transformer 架构在并行加载时不包括单词的顺序，因此我们必须明确定义单词在句子中的位置。这有助于 Transformer 理解句子词与词之间的位置。这就是位置嵌入派上用场的地方。位置嵌入是一种定义单词位置的向量编码。在进入注意力网络之前，将此位置嵌入添加到输入嵌入中。</p><p>作者使用交替正余弦函数来定义位置嵌入：</p><p><img loading=lazy src=/img/NLP/img16.png alt></p><h2 id=代码实现>代码实现<a hidden class=anchor aria-hidden=true href=#代码实现>#</a></h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>  1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>  2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>  3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>  4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5>  5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6>  6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7>  7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8>  8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9>  9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10> 10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11> 11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12> 12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13> 13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14> 14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15> 15</a>
</span><span class=lnt id=hl-0-16><a class=lnlinks href=#hl-0-16> 16</a>
</span><span class=lnt id=hl-0-17><a class=lnlinks href=#hl-0-17> 17</a>
</span><span class=lnt id=hl-0-18><a class=lnlinks href=#hl-0-18> 18</a>
</span><span class=lnt id=hl-0-19><a class=lnlinks href=#hl-0-19> 19</a>
</span><span class=lnt id=hl-0-20><a class=lnlinks href=#hl-0-20> 20</a>
</span><span class=lnt id=hl-0-21><a class=lnlinks href=#hl-0-21> 21</a>
</span><span class=lnt id=hl-0-22><a class=lnlinks href=#hl-0-22> 22</a>
</span><span class=lnt id=hl-0-23><a class=lnlinks href=#hl-0-23> 23</a>
</span><span class=lnt id=hl-0-24><a class=lnlinks href=#hl-0-24> 24</a>
</span><span class=lnt id=hl-0-25><a class=lnlinks href=#hl-0-25> 25</a>
</span><span class=lnt id=hl-0-26><a class=lnlinks href=#hl-0-26> 26</a>
</span><span class=lnt id=hl-0-27><a class=lnlinks href=#hl-0-27> 27</a>
</span><span class=lnt id=hl-0-28><a class=lnlinks href=#hl-0-28> 28</a>
</span><span class=lnt id=hl-0-29><a class=lnlinks href=#hl-0-29> 29</a>
</span><span class=lnt id=hl-0-30><a class=lnlinks href=#hl-0-30> 30</a>
</span><span class=lnt id=hl-0-31><a class=lnlinks href=#hl-0-31> 31</a>
</span><span class=lnt id=hl-0-32><a class=lnlinks href=#hl-0-32> 32</a>
</span><span class=lnt id=hl-0-33><a class=lnlinks href=#hl-0-33> 33</a>
</span><span class=lnt id=hl-0-34><a class=lnlinks href=#hl-0-34> 34</a>
</span><span class=lnt id=hl-0-35><a class=lnlinks href=#hl-0-35> 35</a>
</span><span class=lnt id=hl-0-36><a class=lnlinks href=#hl-0-36> 36</a>
</span><span class=lnt id=hl-0-37><a class=lnlinks href=#hl-0-37> 37</a>
</span><span class=lnt id=hl-0-38><a class=lnlinks href=#hl-0-38> 38</a>
</span><span class=lnt id=hl-0-39><a class=lnlinks href=#hl-0-39> 39</a>
</span><span class=lnt id=hl-0-40><a class=lnlinks href=#hl-0-40> 40</a>
</span><span class=lnt id=hl-0-41><a class=lnlinks href=#hl-0-41> 41</a>
</span><span class=lnt id=hl-0-42><a class=lnlinks href=#hl-0-42> 42</a>
</span><span class=lnt id=hl-0-43><a class=lnlinks href=#hl-0-43> 43</a>
</span><span class=lnt id=hl-0-44><a class=lnlinks href=#hl-0-44> 44</a>
</span><span class=lnt id=hl-0-45><a class=lnlinks href=#hl-0-45> 45</a>
</span><span class=lnt id=hl-0-46><a class=lnlinks href=#hl-0-46> 46</a>
</span><span class=lnt id=hl-0-47><a class=lnlinks href=#hl-0-47> 47</a>
</span><span class=lnt id=hl-0-48><a class=lnlinks href=#hl-0-48> 48</a>
</span><span class=lnt id=hl-0-49><a class=lnlinks href=#hl-0-49> 49</a>
</span><span class=lnt id=hl-0-50><a class=lnlinks href=#hl-0-50> 50</a>
</span><span class=lnt id=hl-0-51><a class=lnlinks href=#hl-0-51> 51</a>
</span><span class=lnt id=hl-0-52><a class=lnlinks href=#hl-0-52> 52</a>
</span><span class=lnt id=hl-0-53><a class=lnlinks href=#hl-0-53> 53</a>
</span><span class=lnt id=hl-0-54><a class=lnlinks href=#hl-0-54> 54</a>
</span><span class=lnt id=hl-0-55><a class=lnlinks href=#hl-0-55> 55</a>
</span><span class=lnt id=hl-0-56><a class=lnlinks href=#hl-0-56> 56</a>
</span><span class=lnt id=hl-0-57><a class=lnlinks href=#hl-0-57> 57</a>
</span><span class=lnt id=hl-0-58><a class=lnlinks href=#hl-0-58> 58</a>
</span><span class=lnt id=hl-0-59><a class=lnlinks href=#hl-0-59> 59</a>
</span><span class=lnt id=hl-0-60><a class=lnlinks href=#hl-0-60> 60</a>
</span><span class=lnt id=hl-0-61><a class=lnlinks href=#hl-0-61> 61</a>
</span><span class=lnt id=hl-0-62><a class=lnlinks href=#hl-0-62> 62</a>
</span><span class=lnt id=hl-0-63><a class=lnlinks href=#hl-0-63> 63</a>
</span><span class=lnt id=hl-0-64><a class=lnlinks href=#hl-0-64> 64</a>
</span><span class=lnt id=hl-0-65><a class=lnlinks href=#hl-0-65> 65</a>
</span><span class=lnt id=hl-0-66><a class=lnlinks href=#hl-0-66> 66</a>
</span><span class=lnt id=hl-0-67><a class=lnlinks href=#hl-0-67> 67</a>
</span><span class=lnt id=hl-0-68><a class=lnlinks href=#hl-0-68> 68</a>
</span><span class=lnt id=hl-0-69><a class=lnlinks href=#hl-0-69> 69</a>
</span><span class=lnt id=hl-0-70><a class=lnlinks href=#hl-0-70> 70</a>
</span><span class=lnt id=hl-0-71><a class=lnlinks href=#hl-0-71> 71</a>
</span><span class=lnt id=hl-0-72><a class=lnlinks href=#hl-0-72> 72</a>
</span><span class=lnt id=hl-0-73><a class=lnlinks href=#hl-0-73> 73</a>
</span><span class=lnt id=hl-0-74><a class=lnlinks href=#hl-0-74> 74</a>
</span><span class=lnt id=hl-0-75><a class=lnlinks href=#hl-0-75> 75</a>
</span><span class=lnt id=hl-0-76><a class=lnlinks href=#hl-0-76> 76</a>
</span><span class=lnt id=hl-0-77><a class=lnlinks href=#hl-0-77> 77</a>
</span><span class=lnt id=hl-0-78><a class=lnlinks href=#hl-0-78> 78</a>
</span><span class=lnt id=hl-0-79><a class=lnlinks href=#hl-0-79> 79</a>
</span><span class=lnt id=hl-0-80><a class=lnlinks href=#hl-0-80> 80</a>
</span><span class=lnt id=hl-0-81><a class=lnlinks href=#hl-0-81> 81</a>
</span><span class=lnt id=hl-0-82><a class=lnlinks href=#hl-0-82> 82</a>
</span><span class=lnt id=hl-0-83><a class=lnlinks href=#hl-0-83> 83</a>
</span><span class=lnt id=hl-0-84><a class=lnlinks href=#hl-0-84> 84</a>
</span><span class=lnt id=hl-0-85><a class=lnlinks href=#hl-0-85> 85</a>
</span><span class=lnt id=hl-0-86><a class=lnlinks href=#hl-0-86> 86</a>
</span><span class=lnt id=hl-0-87><a class=lnlinks href=#hl-0-87> 87</a>
</span><span class=lnt id=hl-0-88><a class=lnlinks href=#hl-0-88> 88</a>
</span><span class=lnt id=hl-0-89><a class=lnlinks href=#hl-0-89> 89</a>
</span><span class=lnt id=hl-0-90><a class=lnlinks href=#hl-0-90> 90</a>
</span><span class=lnt id=hl-0-91><a class=lnlinks href=#hl-0-91> 91</a>
</span><span class=lnt id=hl-0-92><a class=lnlinks href=#hl-0-92> 92</a>
</span><span class=lnt id=hl-0-93><a class=lnlinks href=#hl-0-93> 93</a>
</span><span class=lnt id=hl-0-94><a class=lnlinks href=#hl-0-94> 94</a>
</span><span class=lnt id=hl-0-95><a class=lnlinks href=#hl-0-95> 95</a>
</span><span class=lnt id=hl-0-96><a class=lnlinks href=#hl-0-96> 96</a>
</span><span class=lnt id=hl-0-97><a class=lnlinks href=#hl-0-97> 97</a>
</span><span class=lnt id=hl-0-98><a class=lnlinks href=#hl-0-98> 98</a>
</span><span class=lnt id=hl-0-99><a class=lnlinks href=#hl-0-99> 99</a>
</span><span class=lnt id=hl-0-100><a class=lnlinks href=#hl-0-100>100</a>
</span><span class=lnt id=hl-0-101><a class=lnlinks href=#hl-0-101>101</a>
</span><span class=lnt id=hl-0-102><a class=lnlinks href=#hl-0-102>102</a>
</span><span class=lnt id=hl-0-103><a class=lnlinks href=#hl-0-103>103</a>
</span><span class=lnt id=hl-0-104><a class=lnlinks href=#hl-0-104>104</a>
</span><span class=lnt id=hl-0-105><a class=lnlinks href=#hl-0-105>105</a>
</span><span class=lnt id=hl-0-106><a class=lnlinks href=#hl-0-106>106</a>
</span><span class=lnt id=hl-0-107><a class=lnlinks href=#hl-0-107>107</a>
</span><span class=lnt id=hl-0-108><a class=lnlinks href=#hl-0-108>108</a>
</span><span class=lnt id=hl-0-109><a class=lnlinks href=#hl-0-109>109</a>
</span><span class=lnt id=hl-0-110><a class=lnlinks href=#hl-0-110>110</a>
</span><span class=lnt id=hl-0-111><a class=lnlinks href=#hl-0-111>111</a>
</span><span class=lnt id=hl-0-112><a class=lnlinks href=#hl-0-112>112</a>
</span><span class=lnt id=hl-0-113><a class=lnlinks href=#hl-0-113>113</a>
</span><span class=lnt id=hl-0-114><a class=lnlinks href=#hl-0-114>114</a>
</span><span class=lnt id=hl-0-115><a class=lnlinks href=#hl-0-115>115</a>
</span><span class=lnt id=hl-0-116><a class=lnlinks href=#hl-0-116>116</a>
</span><span class=lnt id=hl-0-117><a class=lnlinks href=#hl-0-117>117</a>
</span><span class=lnt id=hl-0-118><a class=lnlinks href=#hl-0-118>118</a>
</span><span class=lnt id=hl-0-119><a class=lnlinks href=#hl-0-119>119</a>
</span><span class=lnt id=hl-0-120><a class=lnlinks href=#hl-0-120>120</a>
</span><span class=lnt id=hl-0-121><a class=lnlinks href=#hl-0-121>121</a>
</span><span class=lnt id=hl-0-122><a class=lnlinks href=#hl-0-122>122</a>
</span><span class=lnt id=hl-0-123><a class=lnlinks href=#hl-0-123>123</a>
</span><span class=lnt id=hl-0-124><a class=lnlinks href=#hl-0-124>124</a>
</span><span class=lnt id=hl-0-125><a class=lnlinks href=#hl-0-125>125</a>
</span><span class=lnt id=hl-0-126><a class=lnlinks href=#hl-0-126>126</a>
</span><span class=lnt id=hl-0-127><a class=lnlinks href=#hl-0-127>127</a>
</span><span class=lnt id=hl-0-128><a class=lnlinks href=#hl-0-128>128</a>
</span><span class=lnt id=hl-0-129><a class=lnlinks href=#hl-0-129>129</a>
</span><span class=lnt id=hl-0-130><a class=lnlinks href=#hl-0-130>130</a>
</span><span class=lnt id=hl-0-131><a class=lnlinks href=#hl-0-131>131</a>
</span><span class=lnt id=hl-0-132><a class=lnlinks href=#hl-0-132>132</a>
</span><span class=lnt id=hl-0-133><a class=lnlinks href=#hl-0-133>133</a>
</span><span class=lnt id=hl-0-134><a class=lnlinks href=#hl-0-134>134</a>
</span><span class=lnt id=hl-0-135><a class=lnlinks href=#hl-0-135>135</a>
</span><span class=lnt id=hl-0-136><a class=lnlinks href=#hl-0-136>136</a>
</span><span class=lnt id=hl-0-137><a class=lnlinks href=#hl-0-137>137</a>
</span><span class=lnt id=hl-0-138><a class=lnlinks href=#hl-0-138>138</a>
</span><span class=lnt id=hl-0-139><a class=lnlinks href=#hl-0-139>139</a>
</span><span class=lnt id=hl-0-140><a class=lnlinks href=#hl-0-140>140</a>
</span><span class=lnt id=hl-0-141><a class=lnlinks href=#hl-0-141>141</a>
</span><span class=lnt id=hl-0-142><a class=lnlinks href=#hl-0-142>142</a>
</span><span class=lnt id=hl-0-143><a class=lnlinks href=#hl-0-143>143</a>
</span><span class=lnt id=hl-0-144><a class=lnlinks href=#hl-0-144>144</a>
</span><span class=lnt id=hl-0-145><a class=lnlinks href=#hl-0-145>145</a>
</span><span class=lnt id=hl-0-146><a class=lnlinks href=#hl-0-146>146</a>
</span><span class=lnt id=hl-0-147><a class=lnlinks href=#hl-0-147>147</a>
</span><span class=lnt id=hl-0-148><a class=lnlinks href=#hl-0-148>148</a>
</span><span class=lnt id=hl-0-149><a class=lnlinks href=#hl-0-149>149</a>
</span><span class=lnt id=hl-0-150><a class=lnlinks href=#hl-0-150>150</a>
</span><span class=lnt id=hl-0-151><a class=lnlinks href=#hl-0-151>151</a>
</span><span class=lnt id=hl-0-152><a class=lnlinks href=#hl-0-152>152</a>
</span><span class=lnt id=hl-0-153><a class=lnlinks href=#hl-0-153>153</a>
</span><span class=lnt id=hl-0-154><a class=lnlinks href=#hl-0-154>154</a>
</span><span class=lnt id=hl-0-155><a class=lnlinks href=#hl-0-155>155</a>
</span><span class=lnt id=hl-0-156><a class=lnlinks href=#hl-0-156>156</a>
</span><span class=lnt id=hl-0-157><a class=lnlinks href=#hl-0-157>157</a>
</span><span class=lnt id=hl-0-158><a class=lnlinks href=#hl-0-158>158</a>
</span><span class=lnt id=hl-0-159><a class=lnlinks href=#hl-0-159>159</a>
</span><span class=lnt id=hl-0-160><a class=lnlinks href=#hl-0-160>160</a>
</span><span class=lnt id=hl-0-161><a class=lnlinks href=#hl-0-161>161</a>
</span><span class=lnt id=hl-0-162><a class=lnlinks href=#hl-0-162>162</a>
</span><span class=lnt id=hl-0-163><a class=lnlinks href=#hl-0-163>163</a>
</span><span class=lnt id=hl-0-164><a class=lnlinks href=#hl-0-164>164</a>
</span><span class=lnt id=hl-0-165><a class=lnlinks href=#hl-0-165>165</a>
</span><span class=lnt id=hl-0-166><a class=lnlinks href=#hl-0-166>166</a>
</span><span class=lnt id=hl-0-167><a class=lnlinks href=#hl-0-167>167</a>
</span><span class=lnt id=hl-0-168><a class=lnlinks href=#hl-0-168>168</a>
</span><span class=lnt id=hl-0-169><a class=lnlinks href=#hl-0-169>169</a>
</span><span class=lnt id=hl-0-170><a class=lnlinks href=#hl-0-170>170</a>
</span><span class=lnt id=hl-0-171><a class=lnlinks href=#hl-0-171>171</a>
</span><span class=lnt id=hl-0-172><a class=lnlinks href=#hl-0-172>172</a>
</span><span class=lnt id=hl-0-173><a class=lnlinks href=#hl-0-173>173</a>
</span><span class=lnt id=hl-0-174><a class=lnlinks href=#hl-0-174>174</a>
</span><span class=lnt id=hl-0-175><a class=lnlinks href=#hl-0-175>175</a>
</span><span class=lnt id=hl-0-176><a class=lnlinks href=#hl-0-176>176</a>
</span><span class=lnt id=hl-0-177><a class=lnlinks href=#hl-0-177>177</a>
</span><span class=lnt id=hl-0-178><a class=lnlinks href=#hl-0-178>178</a>
</span><span class=lnt id=hl-0-179><a class=lnlinks href=#hl-0-179>179</a>
</span><span class=lnt id=hl-0-180><a class=lnlinks href=#hl-0-180>180</a>
</span><span class=lnt id=hl-0-181><a class=lnlinks href=#hl-0-181>181</a>
</span><span class=lnt id=hl-0-182><a class=lnlinks href=#hl-0-182>182</a>
</span><span class=lnt id=hl-0-183><a class=lnlinks href=#hl-0-183>183</a>
</span><span class=lnt id=hl-0-184><a class=lnlinks href=#hl-0-184>184</a>
</span><span class=lnt id=hl-0-185><a class=lnlinks href=#hl-0-185>185</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.optim</span> <span class=k>as</span> <span class=nn>optim</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.utils.data</span> <span class=k>as</span> <span class=nn>data</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>copy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 多头注意力</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>MultiHeadAttention</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>num_heads</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=s2>&#34;d_model must be divisible by num_heads&#34;</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>scaled_dot_product_attention</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_probs</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>combine_heads</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>,</span> <span class=n>_</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>d_k</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>Q</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>K</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>V</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>split_heads</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>V</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>combine_heads</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 位置前馈网络      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionWiseFeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionWiseFeedForward</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_ff</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=c1># 位置编码      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>PositionalEncoding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>PositionalEncoding</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>pe</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>max_seq_length</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>position</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>div_term</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span> <span class=o>*</span> <span class=o>-</span><span class=p>(</span><span class=n>math</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=mf>10000.0</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pe</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>*</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;pe&#39;</span><span class=p>,</span> <span class=n>pe</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pe</span><span class=p>[:,</span> <span class=p>:</span><span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 编码器      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>EncoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>PositionWiseFeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>ff_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>ff_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 解码器      </span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>DecoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>PositionWiseFeedForward</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cross_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>ff_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>ff_output</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Transformer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Transformer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>positional_encoding</span> <span class=o>=</span> <span class=n>PositionalEncoding</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder_layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>EncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder_layers</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>DecoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate_mask</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>src</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_mask</span> <span class=o>=</span> <span class=p>(</span><span class=n>tgt</span> <span class=o>!=</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>seq_length</span> <span class=o>=</span> <span class=n>tgt</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>nopeak_mask</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>torch</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>,</span> <span class=n>seq_length</span><span class=p>),</span> <span class=n>diagonal</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span><span class=o>.</span><span class=n>bool</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_mask</span> <span class=o>=</span> <span class=n>tgt_mask</span> <span class=o>&amp;</span> <span class=n>nopeak_mask</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>generate_mask</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>src_embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>positional_encoding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encoder_embedding</span><span class=p>(</span><span class=n>src</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>tgt_embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>positional_encoding</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>decoder_embedding</span><span class=p>(</span><span class=n>tgt</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>enc_output</span> <span class=o>=</span> <span class=n>src_embedded</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>enc_layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder_layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>enc_output</span> <span class=o>=</span> <span class=n>enc_layer</span><span class=p>(</span><span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>dec_output</span> <span class=o>=</span> <span class=n>tgt_embedded</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>dec_layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder_layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>dec_output</span> <span class=o>=</span> <span class=n>dec_layer</span><span class=p>(</span><span class=n>dec_output</span><span class=p>,</span> <span class=n>enc_output</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc</span><span class=p>(</span><span class=n>dec_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>      
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>src_vocab_size</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl>  <span class=n>tgt_vocab_size</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl>  <span class=n>d_model</span> <span class=o>=</span> <span class=mi>512</span>
</span></span><span class=line><span class=cl>  <span class=n>num_heads</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>  <span class=n>num_layers</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl>  <span class=n>d_ff</span> <span class=o>=</span> <span class=mi>2048</span>
</span></span><span class=line><span class=cl>  <span class=n>max_seq_length</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>  <span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>transformer</span> <span class=o>=</span> <span class=n>Transformer</span><span class=p>(</span><span class=n>src_vocab_size</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>d_ff</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># 生成随机样本数据</span>
</span></span><span class=line><span class=cl>  <span class=n>src_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>src_vocab_size</span><span class=p>,</span> <span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>))</span>  
</span></span><span class=line><span class=cl>  <span class=n>tgt_data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>,</span> <span class=p>(</span><span class=mi>64</span><span class=p>,</span> <span class=n>max_seq_length</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>ignore_index</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>optimizer</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>transformer</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.0001</span><span class=p>,</span> <span class=n>betas</span><span class=o>=</span><span class=p>(</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.98</span><span class=p>),</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>transformer</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=n>output</span> <span class=o>=</span> <span class=n>transformer</span><span class=p>(</span><span class=n>src_data</span><span class=p>,</span> <span class=n>tgt_data</span><span class=p>[:,</span> <span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>      <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>tgt_vocab_size</span><span class=p>),</span> <span class=n>tgt_data</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>:]</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>      <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch: </span><span class=si>{</span><span class=n>epoch</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s2>, Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/papernotes/%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E6%9D%8E%E5%86%AC%E6%A2%85/><span class=title>« Prev</span><br><span>《实体关系抽取方法研究综述》笔记</span>
</a><a class=next href=http://localhost:1313/posts/cs224n/lesson_9/><span class=title>Next »</span><br><span>Lecture 9: Transformer</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>