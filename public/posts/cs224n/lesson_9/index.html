<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 9: Transformer | KurongBlog</title>
<meta name=keywords content="CS224N,NLP"><meta name=description content="Á¨¨‰πùËÆ≤ÔºöTransformer"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/cs224n/lesson_9/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/cs224n/lesson_9/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Lecture 9: Transformer"><meta property="og:description" content="Á¨¨‰πùËÆ≤ÔºöTransformer"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/cs224n/lesson_9/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-08-13T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Lecture 9: Transformer"><meta name=twitter:description content="Á¨¨‰πùËÆ≤ÔºöTransformer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 9: Transformer","item":"http://localhost:1313/posts/cs224n/lesson_9/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 9: Transformer","name":"Lecture 9: Transformer","description":"Á¨¨‰πùËÆ≤ÔºöTransformer","keywords":["CS224N","NLP"],"articleBody":"Issues with recurrent models Linear interaction distance RNNs are unrolled ‚Äúleft-to-right‚Äù\nProblem: RNNs take O(sequence length) steps for distant word pairs to interact\nWhat does the O Problem means ?\nHard to learn long-distance dependencies (because gradient problems! ) Linear order of words is ‚Äúbaked in‚Äù; we already know linear order isn‚Äôt the right way to think about sentences‚Ä¶ Lack of parallelizability Forward and backward passes have O(sequence length) unparallelizable operations GPUs can perform a bunch of independent computations at once, but future RNN hidden states can‚Äôt be computed in full before past RNN hidden states have been computed Self-Attention Recall: Attention operates on queries, keys, and values.\nEach query $q_i$ , key $k_i$ and value $v_i$ has follows: $$ q_i \\in \\R^d, \\ k_i \\in \\R^d, \\ v_i \\in \\R^d $$ In self-attention, the queries, keys, and values are drawn from the same source.\nFor example, if the output of the previous layer is $x_1, ..., x_T$ (one vec per word), we cloud let $v_i=k_i=q_i=x_i$ (that is, use the same vectors for all of them) The (dot product) self-attention operation is as follows: $$ e_{ij}=q_i^Tk_j \\\\ \\alpha=\\frac{exp(e_{ij})}{\\sum_{j^{'}}exp(e_{ij^{'}})} \\\\ output_i=\\sum_j\\alpha_{ij}v_j $$Fixing the first self-attention problem: sequence order Since self-attention doesn‚Äôt build in order information, we need to encode the order of the sentence in our keys, queries, and values.\nConsider representing each sequence index as a vector : $$ p_i \\in \\R^d, \\ for \\ i \\in \\{1,2,...,T\\} \\ are \\ postion \\ vectors $$ Easy to incorporate this info into our self-attention block: just add the $p_i$ to our inputs.\nLet $\\hat{v},\\hat{k},\\hat{q}$‚Äã be our old values, keys, and queries. $$ v_i = \\hat{v_i}+p_i \\\\ q_i = \\hat{q_i}+p_i \\\\ k_i = \\hat{k_i}+p_i $$Position representation vectors through sinusoids Sinusoidal position representations: concatenate sinusoidal functions of varying periods\nPros:\nPeriodicity indicates that maybe ‚Äúabsolute position‚Äù isn‚Äôt as important Maybe can extrapolate to longer sequences as periods restart Cons:\nNot learnable; also the extrapolation doesn‚Äôt really work Position representation vectors learned from scratch Learned absolute position representations: Learn a matrix $p \\in \\R^{d \\times T }$ , and let each $p_i$ be a column of that matrix.\nPros:\nFlexibility: each position gets to be learned to fit the data Cons:\nDefinitely can‚Äôt extrapolate to indices outside $1,...,T$ Fixing the second self-attention problem: Nonlinearities Easy fix: add a feed-forward network to post-process each output vector\nFixing the third self-attention problem: Mask To use self-attention in decoders, we need to ensure we can‚Äôt peek at the future.\nTo enable parallelization, we mask out attention to future words by setting attention scores to $-\\infty$\noverall, necessities for a self-attention building block The Transformer Encoder Key-Query-Value Attention Let‚Äôs look at how key-query-value attention is computed, in matrices.\nLet $X=[x_1;...;x_T] \\in \\R^{T \\times d}$ be the concatenation of input vectors First, note that $XK \\in \\R^{T \\times d}, \\ XQ \\in \\R^{T \\times d}, \\ XV \\in \\R^{T \\times d}$‚Äã The output is defined as $output=softmax(XQ(XK)^T)\\times XV$ Multi-headed attention Residual connections ËßÅÊàëÁöÑËøôÁØáÊñáÁ´† ÊÆãÂ∑ÆËøûÊé• | KurongBlog (705248010.github.io)\nLayer normalization Scaled Dot Product ‚ÄúScaled Dot Product‚Äù attention is a final variation to aid in Transformer training.\nWhen dimensionality $ùëë$‚Äã becomes large, dot products between vectors tend to become large.\nBecause of this, inputs to the softmax function can be large, making the gradients small The Transformer Encoder-Decoder The Transformer Decoder Cross-attention ","wordCount":"551","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-08-13T00:00:00Z","dateModified":"2024-08-13T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/cs224n/lesson_9/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 9: Transformer</h1><div class=post-description>Á¨¨‰πùËÆ≤ÔºöTransformer</div><div class=post-meta><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;¬∑&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/CS224N/lesson_9.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#issues-with-recurrent-models>Issues with recurrent models</a><ul><li><a href=#linear-interaction-distance>Linear interaction distance</a></li><li><a href=#lack-of-parallelizability>Lack of parallelizability</a></li></ul></li><li><a href=#self-attention>Self-Attention</a><ul><li><a href=#fixing-the-first-self-attention-problem-sequence-order>Fixing the first self-attention problem: sequence order</a></li><li><a href=#fixing-the-second-self-attention-problem-nonlinearities>Fixing the second self-attention problem: Nonlinearities</a></li><li><a href=#fixing-the-third-self-attention-problem--mask>Fixing the third self-attention problem: Mask</a></li><li><a href=#overall-necessities-for-a-self-attention-building-block>overall, necessities for a self-attention building block</a></li></ul></li><li><a href=#the-transformer-encoder>The Transformer Encoder</a><ul><li><a href=#key-query-value-attention>Key-Query-Value Attention</a></li><li><a href=#multi-headed-attention>Multi-headed attention</a></li><li><a href=#residual-connections>Residual connections</a></li><li><a href=#layer-normalization>Layer normalization</a></li><li><a href=#scaled-dot-product>Scaled Dot Product</a></li></ul></li><li><a href=#the-transformer-encoder-decoder>The Transformer Encoder-Decoder</a></li><li><a href=#the-transformer-decoder>The Transformer Decoder</a><ul><li><a href=#cross-attention>Cross-attention</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=issues-with-recurrent-models>Issues with recurrent models<a hidden class=anchor aria-hidden=true href=#issues-with-recurrent-models>#</a></h2><h3 id=linear-interaction-distance>Linear interaction distance<a hidden class=anchor aria-hidden=true href=#linear-interaction-distance>#</a></h3><ul><li><p>RNNs are unrolled ‚Äúleft-to-right‚Äù</p></li><li><p><strong>Problem:</strong> RNNs take <strong>O(sequence length)</strong> steps for distant word pairs to interact</p></li></ul><p><img loading=lazy src=/img/CS224N/lesson_9/img1.png alt></p><p>What does the <strong>O Problem</strong> means ?</p><ul><li>Hard to learn long-distance dependencies (because gradient problems! )</li><li>Linear order of words is ‚Äúbaked in‚Äù; we already know linear order isn‚Äôt the right way to think about sentences‚Ä¶</li></ul><h3 id=lack-of-parallelizability>Lack of parallelizability<a hidden class=anchor aria-hidden=true href=#lack-of-parallelizability>#</a></h3><ul><li>Forward and backward passes have O(sequence length) unparallelizable operations<ul><li>GPUs can perform a bunch of independent computations at once, but future RNN hidden states can‚Äôt be computed in full before past RNN hidden states have been computed</li></ul></li></ul><h2 id=self-attention>Self-Attention<a hidden class=anchor aria-hidden=true href=#self-attention>#</a></h2><p>Recall: Attention operates on <strong>queries</strong>, <strong>keys</strong>, and <strong>values.</strong></p><ul><li>Each query $q_i$ , key $k_i$ and value $v_i$ has follows:
$$
q_i \in \R^d, \ k_i \in \R^d, \ v_i \in \R^d
$$</li></ul><p>In self-attention, the queries, keys, and values are drawn from the same source.</p><ul><li>For example, if the output of the previous layer is $x_1, ..., x_T$ (one vec per word), we cloud let $v_i=k_i=q_i=x_i$ (that is, use the same vectors for all of them)</li></ul><p>The (dot product) self-attention operation is as follows:</p>$$
e_{ij}=q_i^Tk_j \\
\alpha=\frac{exp(e_{ij})}{\sum_{j^{'}}exp(e_{ij^{'}})} \\
output_i=\sum_j\alpha_{ij}v_j
$$<h3 id=fixing-the-first-self-attention-problem-sequence-order>Fixing the first self-attention problem: sequence order<a hidden class=anchor aria-hidden=true href=#fixing-the-first-self-attention-problem-sequence-order>#</a></h3><p>Since self-attention doesn‚Äôt build in order information, we need to encode the order of the sentence in our keys, queries, and values.</p><p>Consider representing each <strong>sequence index</strong> as a <strong>vector</strong> :</p>$$
p_i \in \R^d, \ for \ i \in \{1,2,...,T\} \ are \ postion \ vectors
$$<p>Easy to incorporate this info into our self-attention block: just add the $p_i$ to our inputs.</p><p>Let $\hat{v},\hat{k},\hat{q}$‚Äã be our old values, keys, and queries.</p>$$
v_i = \hat{v_i}+p_i \\
q_i = \hat{q_i}+p_i \\
k_i = \hat{k_i}+p_i
$$<h4 id=position-representation-vectors-through-sinusoids>Position representation vectors through sinusoids<a hidden class=anchor aria-hidden=true href=#position-representation-vectors-through-sinusoids>#</a></h4><p><strong>Sinusoidal position representations:</strong> concatenate sinusoidal functions of varying periods</p><p><img loading=lazy src=/img/CS224N/lesson_9/img2.png alt></p><p>Pros:</p><ul><li>Periodicity indicates that maybe ‚Äúabsolute position‚Äù isn‚Äôt as important</li><li>Maybe can extrapolate to longer sequences as periods restart</li></ul><p>Cons:</p><ul><li>Not learnable; also the extrapolation doesn‚Äôt really work</li></ul><h4 id=position-representation-vectors-learned-from-scratch>Position representation vectors learned from scratch<a hidden class=anchor aria-hidden=true href=#position-representation-vectors-learned-from-scratch>#</a></h4><p><strong>Learned absolute position representations:</strong> Learn a matrix $p \in \R^{d \times T }$ , and let each $p_i$ be a column of that matrix.</p><p>Pros:</p><ul><li>Flexibility: each position gets to be learned to fit the data</li></ul><p>Cons:</p><ul><li>Definitely can‚Äôt extrapolate to indices outside $1,...,T$</li></ul><h3 id=fixing-the-second-self-attention-problem-nonlinearities>Fixing the second self-attention problem: Nonlinearities<a hidden class=anchor aria-hidden=true href=#fixing-the-second-self-attention-problem-nonlinearities>#</a></h3><p>Easy fix: add a <strong>feed-forward network</strong> to post-process each output vector</p><h3 id=fixing-the-third-self-attention-problem--mask>Fixing the third self-attention problem: Mask<a hidden class=anchor aria-hidden=true href=#fixing-the-third-self-attention-problem--mask>#</a></h3><p>To use self-attention in decoders, we need to ensure we can‚Äôt peek at the future.</p><p>To enable parallelization, we mask out attention to future words by setting attention scores to $-\infty$</p><p><img loading=lazy src=/img/CS224N/lesson_9/img3.png alt></p><h3 id=overall-necessities-for-a-self-attention-building-block>overall, necessities for a self-attention building block<a hidden class=anchor aria-hidden=true href=#overall-necessities-for-a-self-attention-building-block>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_9/img4.png alt></p><h2 id=the-transformer-encoder>The Transformer Encoder<a hidden class=anchor aria-hidden=true href=#the-transformer-encoder>#</a></h2><h3 id=key-query-value-attention>Key-Query-Value Attention<a hidden class=anchor aria-hidden=true href=#key-query-value-attention>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_9/img5.png alt></p><p>Let‚Äôs look at how key-query-value attention is computed, in matrices.</p><ul><li>Let $X=[x_1;...;x_T] \in \R^{T \times d}$ be the concatenation of input vectors</li><li>First, note that $XK \in \R^{T \times d}, \ XQ \in \R^{T \times d}, \ XV \in \R^{T \times d}$‚Äã</li><li>The output is defined as $output=softmax(XQ(XK)^T)\times XV$</li></ul><p><img loading=lazy src=/img/CS224N/lesson_9/img6.png alt></p><h3 id=multi-headed-attention>Multi-headed attention<a hidden class=anchor aria-hidden=true href=#multi-headed-attention>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_9/img7.png alt></p><p><img loading=lazy src=/img/CS224N/lesson_9/img8.png alt></p><h3 id=residual-connections>Residual connections<a hidden class=anchor aria-hidden=true href=#residual-connections>#</a></h3><p>ËßÅÊàëÁöÑËøôÁØáÊñáÁ´† <a href=https://705248010.github.io/posts/nlp/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/>ÊÆãÂ∑ÆËøûÊé• | KurongBlog (705248010.github.io)</a></p><h3 id=layer-normalization>Layer normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_9/img9.png alt></p><h3 id=scaled-dot-product>Scaled Dot Product<a hidden class=anchor aria-hidden=true href=#scaled-dot-product>#</a></h3><p><strong>‚ÄúScaled Dot Product‚Äù</strong> attention is a final variation to aid in Transformer training.</p><p>When dimensionality $ùëë$‚Äã becomes large, dot products between vectors tend to become large.</p><ul><li>Because of this, inputs to the softmax function can be large, making the gradients small</li></ul><p><img loading=lazy src=/img/CS224N/lesson_9/img10.png alt></p><h2 id=the-transformer-encoder-decoder>The Transformer Encoder-Decoder<a hidden class=anchor aria-hidden=true href=#the-transformer-encoder-decoder>#</a></h2><p><img loading=lazy src=/img/CS224N/lesson_9/img11.png alt></p><p><img loading=lazy src=/img/CS224N/lesson_9/img12.png alt></p><h2 id=the-transformer-decoder>The Transformer Decoder<a hidden class=anchor aria-hidden=true href=#the-transformer-decoder>#</a></h2><h3 id=cross-attention>Cross-attention<a hidden class=anchor aria-hidden=true href=#cross-attention>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_9/img13.png alt></p><p><img loading=lazy src=/img/CS224N/lesson_9/img14.png alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/cs224n/>CS224N</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp/attention-is-all-you-need/><span class=title>¬´ Prev</span><br><span>üò∫ Is All You Need‚Äî‚ÄîTransformerË°•ÂÖÖ</span>
</a><a class=next href=http://localhost:1313/posts/weeklyworking/2024_08_06-2024_08_11/><span class=title>Next ¬ª</span><br><span>8.06-8.11Ôºö‰∏ÄÂë®ÊÄªÁªì</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>