<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 8: Attention | KurongBlog</title>
<meta name=keywords content="CS224N,NLP"><meta name=description content="第八讲：注意力机制"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/cs224n/lesson_8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/cs224n/lesson_8/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Lecture 8: Attention"><meta property="og:description" content="第八讲：注意力机制"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/cs224n/lesson_8/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-27T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Lecture 8: Attention"><meta name=twitter:description content="第八讲：注意力机制"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 8: Attention","item":"http://localhost:1313/posts/cs224n/lesson_8/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 8: Attention","name":"Lecture 8: Attention","description":"第八讲：注意力机制","keywords":["CS224N","NLP"],"articleBody":"Sequence-to-sequence with attention Attention: in equations We have encoder hidden states $h_1,...,h_N \\in \\R^h$\nOn timestep $t$​ , we have decoder hidden state $s_t \\in \\R^h$\nWe get the attention scores $e^t$ for this step: $$ e^t=[s^T_th_1,...,s^T_th_N] \\in \\R^N $$ We take softmax to get the attention distribution $\\alpha^t$​ for this step (this is a probability distribution and sums to 1) $$ \\alpha^t=softmax(e^t) \\in \\R^N $$ We use $\\alpha^t$ to take a weighted sum of the encoder hidden states to get the attention output $a_i$ $$ a_i=\\sum^N_{i=1}\\alpha_i^th_i \\in \\R^h $$ Finally we concatenate the attention output $a_t$ with the decoder hidden state $s_t$​ and proceed as in the non-attention seq2seq model $$ [a_t;s_t] \\in \\R^{2h} $$ Attention is great Attention significantly improves NMT performance Attention provides more “human-like” model of the MT process Attention solves the bottleneck problem Attention helps with the vanishing gradient problem Provides shortcut to faraway states Attention provides some interpretability By inspecting attention distribution, we can see what the decoder was focusing on There are several attention variants Attention variants Attention is a general Deep Learning technique More general definition of attention:\nGiven a set of vector values, and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query. The weighted sum is a selective summary of the information contained in the values, where the query determines which values to focus on.\nAttention is a way to obtain a fixed-size representation of an arbitrary set of representations (the values), dependent on some other representation (the query).\n","wordCount":"259","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-07-27T00:00:00Z","dateModified":"2024-07-27T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/cs224n/lesson_8/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 8: Attention</h1><div class=post-description>第八讲：注意力机制</div><div class=post-meta><span title='2024-07-27 00:00:00 +0000 UTC'>July 27, 2024</span>&nbsp;·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/CS224N/lesson_8.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#sequence-to-sequence-with-attention>Sequence-to-sequence with attention</a><ul><li><a href=#attention-in-equations>Attention: in equations</a></li><li><a href=#attention-is-great>Attention is great</a></li><li><a href=#there-are-several-attention-variants>There are several attention variants</a></li><li><a href=#attention-variants>Attention variants</a></li><li><a href=#attention-is-a-general-deep-learning-technique>Attention is a general Deep Learning technique</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=sequence-to-sequence-with-attention>Sequence-to-sequence with attention<a hidden class=anchor aria-hidden=true href=#sequence-to-sequence-with-attention>#</a></h2><p><img loading=lazy src=/img/CS224N/lesson_8/img1.png alt></p><h3 id=attention-in-equations>Attention: in equations<a hidden class=anchor aria-hidden=true href=#attention-in-equations>#</a></h3><ul><li><p>We have encoder hidden states $h_1,...,h_N \in \R^h$</p></li><li><p>On timestep $t$​ , we have decoder hidden state $s_t \in \R^h$</p></li><li><p>We get the attention scores $e^t$ for this step:</p>$$
e^t=[s^T_th_1,...,s^T_th_N] \in \R^N
$$</li><li><p>We take softmax to get the attention distribution $\alpha^t$​ for this step (this is a probability distribution and sums to 1)</p>$$
\alpha^t=softmax(e^t) \in \R^N
$$</li><li><p>We use $\alpha^t$ to take a weighted sum of the encoder hidden states to get the attention output $a_i$</p>$$
a_i=\sum^N_{i=1}\alpha_i^th_i \in \R^h
$$</li><li><p>Finally we concatenate the attention output $a_t$ with the decoder hidden state $s_t$​ and proceed as in the non-attention seq2seq model</p>$$
[a_t;s_t] \in \R^{2h}
$$</li></ul><h3 id=attention-is-great>Attention is great<a hidden class=anchor aria-hidden=true href=#attention-is-great>#</a></h3><ul><li>Attention significantly improves NMT performance</li><li>Attention provides more “human-like” model of the MT process</li><li>Attention solves the bottleneck problem</li><li>Attention helps with the vanishing gradient problem<ul><li>Provides shortcut to faraway states</li></ul></li><li>Attention provides some interpretability<ul><li>By inspecting attention distribution, we can see what the decoder was focusing on</li></ul></li></ul><h3 id=there-are-several-attention-variants>There are several attention variants<a hidden class=anchor aria-hidden=true href=#there-are-several-attention-variants>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_8/img2.png alt></p><h3 id=attention-variants>Attention variants<a hidden class=anchor aria-hidden=true href=#attention-variants>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_8/img3.png alt></p><h3 id=attention-is-a-general-deep-learning-technique>Attention is a general Deep Learning technique<a hidden class=anchor aria-hidden=true href=#attention-is-a-general-deep-learning-technique>#</a></h3><p>More general definition of attention:</p><ul><li>Given a set of vector <strong>values</strong>, and a vector <strong>query</strong>, <strong>attention</strong> is a technique to compute a weighted sum of the values, dependent on the query.</li></ul><p>The weighted sum is a <strong>selective summary</strong> of the information contained in the values, where the query determines which values to focus on.</p><p>Attention is a way to obtain a <strong>fixed-size representation of an arbitrary set of representations</strong> (the values), dependent on some other representation (the query).</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/cs224n/>CS224N</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/papernotes/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E8%B5%B5%E7%BB%A7%E8%B4%B5/><span class=title>« Prev</span><br><span>《中文命名实体识别研究综述》笔记</span>
</a><a class=next href=http://localhost:1313/posts/cs224n/lesson_7/><span class=title>Next »</span><br><span>Lecture 7: Machine Translation and Sequence to Sequence</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>