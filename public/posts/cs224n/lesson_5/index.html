<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 5: Language Models and Recurrent Neural Network | KurongBlog</title>
<meta name=keywords content="CS224N,NLP"><meta name=description content="第五讲：语言模型和循环神经网络"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/cs224n/lesson_5/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/cs224n/lesson_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Lecture 5: Language Models and Recurrent Neural Network"><meta property="og:description" content="第五讲：语言模型和循环神经网络"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/cs224n/lesson_5/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-05T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-05T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Lecture 5: Language Models and Recurrent Neural Network"><meta name=twitter:description content="第五讲：语言模型和循环神经网络"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 5: Language Models and Recurrent Neural Network","item":"http://localhost:1313/posts/cs224n/lesson_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 5: Language Models and Recurrent Neural Network","name":"Lecture 5: Language Models and Recurrent Neural Network","description":"第五讲：语言模型和循环神经网络","keywords":["CS224N","NLP"],"articleBody":"Basic Tricks on NN L2 Regularization A full loss function includes regularization over all parameters $\\theta$ , e.g., L2 regularization: $$ J(\\theta)=f(x)+\\lambda \\sum_k \\theta^2_k $$ Regularization produces models that generalize well when we have a “big” model.\nDropout Training time: at each instance of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0 Test time: halve the model weights (now twice as many) This prevents feature co-adaptation Can be thought of as a form of model bagging (i.e., like an ensemble model) Nowadays usually thought of as strong, feature-dependent regularizer Vectorization Always try to use vectors and matrices rather than for loops.\nNon-linearities, old and new For building a deep network, the first thing you should try is ReLU — it trains quickly and performs well due to good gradient backflow.\nParameter Initialization You normally must initialize weights to small random values. To avoid symmetries that prevent learning/specialization.\nInitialize hidden layer biases to 0 and output (or reconstruction) biases to optimal value if weights were 0 (e.g., mean target or inverse sigmoid of mean target)\nInitialize all other weights ~ Uniform($–r$, $r$), with $r$ chosen so numbers get neither too big or too small\nXavier initialization has variance inversely proportional to fan-in $n_{in}$ (previous layer size) and fan-out $n_{out}$ (next layer size): $$ Var(W_i)=\\frac{2}{n_{in}+n_{out}} $$ Optimizers Usually, plain SGD will work just fine These models give differential per-parameter learning rates Adagrad RMSprop Adam: A fairly good, safe place to begin in many cases SparseAdam Learning Rates You can just use a constant learning rate. It must be order of magnitude right – try powers of 10 Too big: model may diverge or not converge Too small: your model may not have trained by the assignment deadline By a formula: $lr=lr_0e^{-kt}$ , for epoch $t$ There are fancier methods like cyclic learning rates Language Modeling Language Modeling is the task of predicting what word comes next. A system that does this is called a Language Model.\nn-gram Language Models A n-gram is a chunk of $n$​ consecutive words.\nunigrams: “the”, “students”, “opened”, ”their” bigrams: “the students”, “students opened”, “opened their” trigrams: “the students opened”, “students opened their” 4-grams: “the students opened their” We make a Markov assumption 马尔可夫假设: $x^{t+1}$ depends only on the preceding $n-1$ words\nSo we can get n-gram and (n-1)-gram probabilities by counting them in corpus of text.\nExample Sparsity Problems with n-gram Note: Increasing n makes sparsity problems worse. Typically, we can’t have $n$​ bigger than 5.\nStorage Problems with n-gram Recurrent Neural Networks (RNN) RNN Advantages:\nCan process any length input Computation for step $t$​ can (in theory) use information from many steps back Model size doesn’t increase for longer input context Same weights applied on every timestep, so there is symmetry in how inputs are processed RNN Disadvantages:\nRecurrent computation is slow In practice, difficult to access information from many steps back A Simple RNN Language Model ","wordCount":"488","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-07-05T00:00:00Z","dateModified":"2024-07-05T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/cs224n/lesson_5/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 5: Language Models and Recurrent Neural Network</h1><div class=post-description>第五讲：语言模型和循环神经网络</div><div class=post-meta><span title='2024-07-05 00:00:00 +0000 UTC'>July 5, 2024</span>&nbsp;·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/CS224N/lesson_5.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#basic-tricks-on-nn>Basic Tricks on NN</a><ul><li><a href=#l2-regularization>L2 Regularization</a></li><li><a href=#dropout>Dropout</a></li><li><a href=#vectorization>Vectorization</a></li><li><a href=#non-linearities-old-and-new>Non-linearities, old and new</a></li><li><a href=#parameter-initialization>Parameter Initialization</a></li><li><a href=#optimizers>Optimizers</a></li><li><a href=#learning-rates>Learning Rates</a></li></ul></li><li><a href=#language-modeling>Language Modeling</a><ul><li><a href=#n-gram-language-models>n-gram Language Models</a></li><li><a href=#example>Example</a></li><li><a href=#sparsity-problems-with-n-gram>Sparsity Problems with n-gram</a></li><li><a href=#storage-problems-with-n-gram>Storage Problems with n-gram</a></li></ul></li><li><a href=#recurrent-neural-networks-rnn>Recurrent Neural Networks (RNN)</a><ul><li><a href=#a-simple-rnn-language-model>A Simple RNN Language Model</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=basic-tricks-on-nn>Basic Tricks on NN<a hidden class=anchor aria-hidden=true href=#basic-tricks-on-nn>#</a></h2><h3 id=l2-regularization>L2 Regularization<a hidden class=anchor aria-hidden=true href=#l2-regularization>#</a></h3><p>A full loss function includes regularization over all parameters $\theta$ , e.g., L2 regularization:</p>$$
J(\theta)=f(x)+\lambda \sum_k \theta^2_k
$$<p>Regularization produces models that generalize well when we have a “big” model.</p><h3 id=dropout>Dropout<a hidden class=anchor aria-hidden=true href=#dropout>#</a></h3><ul><li>Training time: at each instance of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0</li><li>Test time: halve the model weights (now twice as many)</li><li>This prevents feature co-adaptation</li><li>Can be thought of as a form of model bagging (i.e., like an ensemble model)</li><li>Nowadays usually thought of as strong, feature-dependent regularizer</li></ul><h3 id=vectorization>Vectorization<a hidden class=anchor aria-hidden=true href=#vectorization>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_5/img1.png alt></p><p>Always try to use vectors and matrices rather than for loops.</p><h3 id=non-linearities-old-and-new>Non-linearities, old and new<a hidden class=anchor aria-hidden=true href=#non-linearities-old-and-new>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_5/img2.png alt></p><p>For building a deep network, the first thing you should try is ReLU — it trains quickly and performs well due to good gradient backflow.</p><h3 id=parameter-initialization>Parameter Initialization<a hidden class=anchor aria-hidden=true href=#parameter-initialization>#</a></h3><ul><li><p>You normally must initialize weights to small random values. To avoid symmetries that prevent learning/specialization.</p></li><li><p>Initialize hidden layer biases to 0 and output (or reconstruction) biases to optimal value if weights were 0 (e.g., mean target or inverse sigmoid of mean target)</p></li><li><p>Initialize all other weights ~ Uniform($–r$, $r$), with $r$ chosen so numbers get neither too big or too small</p></li><li><p>Xavier initialization has variance inversely proportional to fan-in $n_{in}$ (previous layer size) and fan-out $n_{out}$ (next layer size):</p>$$
Var(W_i)=\frac{2}{n_{in}+n_{out}}
$$</li></ul><h3 id=optimizers>Optimizers<a hidden class=anchor aria-hidden=true href=#optimizers>#</a></h3><ul><li>Usually, plain SGD will work just fine</li><li>These models give differential per-parameter learning rates<ul><li>Adagrad</li><li>RMSprop</li><li><strong>Adam: A fairly good, safe place to begin in many cases</strong></li><li>SparseAdam</li></ul></li></ul><h3 id=learning-rates>Learning Rates<a hidden class=anchor aria-hidden=true href=#learning-rates>#</a></h3><ul><li>You can just use a constant learning rate. It must be order of magnitude right – try powers of 10<ul><li>Too big: model may diverge or not converge</li><li>Too small: your model may not have trained by the assignment deadline</li></ul></li><li>By a formula: $lr=lr_0e^{-kt}$ , for epoch $t$</li><li>There are fancier methods like cyclic learning rates</li></ul><h2 id=language-modeling>Language Modeling<a hidden class=anchor aria-hidden=true href=#language-modeling>#</a></h2><p>Language Modeling is the task of predicting what word comes next. A system that does this is called a Language Model.</p><h3 id=n-gram-language-models>n-gram Language Models<a hidden class=anchor aria-hidden=true href=#n-gram-language-models>#</a></h3><p>A <code>n-gram</code> is a chunk of $n$​ consecutive words.</p><ul><li>unigrams: “the”, “students”, “opened”, ”their”</li><li>bigrams: “the students”, “students opened”, “opened their”</li><li>trigrams: “the students opened”, “students opened their”</li><li>4-grams: “the students opened their”</li></ul><p>We make a Markov assumption 马尔可夫假设: $x^{t+1}$ depends only on the preceding $n-1$ words</p><p><img loading=lazy src=/img/CS224N/lesson_5/img3.png alt></p><p>So we can get n-gram and (n-1)-gram probabilities by counting them in corpus of text.</p><p><img loading=lazy src=/img/CS224N/lesson_5/img4.png alt></p><h3 id=example>Example<a hidden class=anchor aria-hidden=true href=#example>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_5/img5.png alt></p><h3 id=sparsity-problems-with-n-gram>Sparsity Problems with n-gram<a hidden class=anchor aria-hidden=true href=#sparsity-problems-with-n-gram>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_5/img6.png alt></p><p>Note: Increasing n makes sparsity problems worse. Typically, we can’t have $n$​ bigger than 5.</p><h3 id=storage-problems-with-n-gram>Storage Problems with n-gram<a hidden class=anchor aria-hidden=true href=#storage-problems-with-n-gram>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_5/img7.png alt></p><h2 id=recurrent-neural-networks-rnn>Recurrent Neural Networks (RNN)<a hidden class=anchor aria-hidden=true href=#recurrent-neural-networks-rnn>#</a></h2><p><img loading=lazy src=/img/CS224N/lesson_5/img8.png alt></p><p>RNN Advantages:</p><ul><li>Can process any length input</li><li>Computation for step $t$​ can (in theory) use information from many steps back</li><li>Model size doesn’t increase for longer input context</li><li>Same weights applied on every timestep, so there is symmetry in how inputs are processed</li></ul><p>RNN Disadvantages:</p><ul><li>Recurrent computation is slow</li><li>In practice, difficult to access information from many steps back</li></ul><h3 id=a-simple-rnn-language-model>A Simple RNN Language Model<a hidden class=anchor aria-hidden=true href=#a-simple-rnn-language-model>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_5/img9.png alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/cs224n/>CS224N</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp/rnn/><span class=title>« Prev</span><br><span>RNN速成（一）</span>
</a><a class=next href=http://localhost:1313/posts/nlp/ner/><span class=title>Next »</span><br><span>Named Entity Recognition 相关概念与技术</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>