<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 7: Machine Translation and Sequence to Sequence | KurongBlog</title>
<meta name=keywords content="CS224N,NLP"><meta name=description content="第七讲：机器翻译与Seq2seq"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/cs224n/lesson_7/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/cs224n/lesson_7/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Lecture 7: Machine Translation and Sequence to Sequence"><meta property="og:description" content="第七讲：机器翻译与Seq2seq"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/cs224n/lesson_7/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-13T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Lecture 7: Machine Translation and Sequence to Sequence"><meta name=twitter:description content="第七讲：机器翻译与Seq2seq"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 7: Machine Translation and Sequence to Sequence","item":"http://localhost:1313/posts/cs224n/lesson_7/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 7: Machine Translation and Sequence to Sequence","name":"Lecture 7: Machine Translation and Sequence to Sequence","description":"第七讲：机器翻译与Seq2seq","keywords":["CS224N","NLP"],"articleBody":"Machine Translation Machine Translation is the task of translating a sentence $x$ from one language to a sentence $y$​ in another language.\nSimple History:\n1990s-2010s: Statistical Machine Translation After 2014: Neural Machine Translation Sequence to Sequence Model The sequence-to-sequence model is an example of a Conditional Language Model\nLanguage Model because the decoder is predicting the next word of the target sentence $y$ Conditional because its predictions are also conditioned on the source sentence $x$ Multi-layer RNNs in practice High-performing RNNs are usually multi-layer.\nFor example: In a 2017 paper, Britz et al. find that for Neural Machine Translation, 2 to 4 layers is best for the encoder RNN, and 4 layers is best for the decoder RNN.\nTransformer-based networks (e.g., BERT) are usually deeper, like 12 or 24 layers.\nGreedy decoding We saw how to generate (or “decode”) the target sentence by taking argmax on each step of the decoder. This is greedy decoding (take most probable word on each step)\nBut this way has some problems. Like this:\nExhaustive search decoding Ideally, we want to find a (length T) translation y that maximizes\nWe could try computing all possible sequences $y$​\nThis means that on each step t of the decoder, we’re tracking $V^t$ possible partial translations, where $V$​ is vocab size This $O(V^T)$​ complexity is far too expensive! Beam search decoding Core idea: On each step of decoder, keep track of the $k$ most probable partial translations (which we call hypotheses)\n$k$ is the beam size (in practice around 5 to 10) Beam search is not guaranteed to find optimal solution, but much more efficient than exhaustive search.\nIn greedy decoding, usually we decode until the model produces an “” token.\nIn beam search decoding, different hypotheses may produce tokens on different timesteps.\nWe have our list of completed hypotheses.\nProblem with this: longer hypotheses have lower scores.\nThe bottleneck problem ","wordCount":"315","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-07-13T00:00:00Z","dateModified":"2024-07-13T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/cs224n/lesson_7/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 7: Machine Translation and Sequence to Sequence</h1><div class=post-description>第七讲：机器翻译与Seq2seq</div><div class=post-meta><span title='2024-07-13 00:00:00 +0000 UTC'>July 13, 2024</span>&nbsp;·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/CS224N/lesson_7.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#machine-translation>Machine Translation</a></li><li><a href=#sequence-to-sequence-model>Sequence to Sequence Model</a><ul><li><a href=#multi-layer-rnns-in-practice>Multi-layer RNNs in practice</a></li><li><a href=#greedy-decoding>Greedy decoding</a></li><li><a href=#exhaustive-search-decoding>Exhaustive search decoding</a></li><li><a href=#beam-search-decoding>Beam search decoding</a></li><li><a href=#the-bottleneck-problem>The bottleneck problem</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=machine-translation>Machine Translation<a hidden class=anchor aria-hidden=true href=#machine-translation>#</a></h2><p>Machine Translation is the task of translating a sentence $x$ from one language to a sentence $y$​ in another language.</p><p>Simple History:</p><ul><li>1990s-2010s: Statistical Machine Translation</li><li>After 2014: Neural Machine Translation</li></ul><h2 id=sequence-to-sequence-model>Sequence to Sequence Model<a hidden class=anchor aria-hidden=true href=#sequence-to-sequence-model>#</a></h2><p>The sequence-to-sequence model is an example of a Conditional Language Model</p><ul><li>Language Model because the decoder is predicting the next word of the target sentence $y$</li><li>Conditional because its predictions are also conditioned on the source sentence $x$</li></ul><p><img loading=lazy src=/img/CS224N/lesson_7/img1.png alt></p><h3 id=multi-layer-rnns-in-practice>Multi-layer RNNs in practice<a hidden class=anchor aria-hidden=true href=#multi-layer-rnns-in-practice>#</a></h3><p>High-performing RNNs are usually multi-layer.</p><p>For example: In a 2017 paper, Britz et al. find that for Neural Machine Translation, 2 to 4 layers is best for the encoder RNN, and 4 layers is best for the decoder RNN.</p><p>Transformer-based networks (e.g., BERT) are usually deeper, like 12 or 24 layers.</p><h3 id=greedy-decoding>Greedy decoding<a hidden class=anchor aria-hidden=true href=#greedy-decoding>#</a></h3><p>We saw how to generate (or “decode”) the target sentence by taking argmax on each step of the decoder. This is <strong>greedy decoding</strong> (take most probable word on each step)</p><p>But this way has some problems. Like this:</p><p><img loading=lazy src=/img/CS224N/lesson_7/img2.png alt></p><h3 id=exhaustive-search-decoding>Exhaustive search decoding<a hidden class=anchor aria-hidden=true href=#exhaustive-search-decoding>#</a></h3><p>Ideally, we want to find a (length T) translation y that maximizes</p><p><img loading=lazy src=/img/CS224N/lesson_7/img3.png alt></p><p>We could try computing all possible sequences $y$​</p><ul><li>This means that on each step t of the decoder, we’re tracking $V^t$ possible partial translations, where $V$​ is vocab size</li><li>This $O(V^T)$​ complexity is far too expensive!</li></ul><h3 id=beam-search-decoding>Beam search decoding<a hidden class=anchor aria-hidden=true href=#beam-search-decoding>#</a></h3><p>Core idea: On each step of decoder, keep track of the $k$ most probable partial translations (which we call hypotheses)</p><ul><li>$k$ is the beam size (in practice around 5 to 10)</li></ul><p><img loading=lazy src=/img/CS224N/lesson_7/img4.png alt></p><p>Beam search is not guaranteed to find optimal solution, but much more efficient than exhaustive search.</p><p>In greedy decoding, usually we decode until the model produces an “&lt;END>” token.</p><p>In beam search decoding, different hypotheses may produce &lt;END> tokens on different timesteps.</p><p>We have our list of completed hypotheses.</p><p><img loading=lazy src=/img/CS224N/lesson_7/img5.png alt></p><p>Problem with this: longer hypotheses have lower scores.</p><p><img loading=lazy src=/img/CS224N/lesson_7/img6.png alt></p><h3 id=the-bottleneck-problem>The bottleneck problem<a hidden class=anchor aria-hidden=true href=#the-bottleneck-problem>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_7/img7.png alt></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/cs224n/>CS224N</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/cs224n/lesson_8/><span class=title>« Prev</span><br><span>Lecture 8: Attention</span>
</a><a class=next href=http://localhost:1313/posts/nlp/rnn%E9%80%9F%E6%88%90%E4%BA%8C/><span class=title>Next »</span><br><span>RNN速成（二）</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>