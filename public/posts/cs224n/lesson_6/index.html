<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lecture 6: Long Short-Term Memory RNNs | KurongBlog</title>
<meta name=keywords content="CS224N,NLP"><meta name=description content="第六讲：LSTM"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/cs224n/lesson_6/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/cs224n/lesson_6/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Lecture 6: Long Short-Term Memory RNNs"><meta property="og:description" content="第六讲：LSTM"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/cs224n/lesson_6/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-11T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-11T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Lecture 6: Long Short-Term Memory RNNs"><meta name=twitter:description content="第六讲：LSTM"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Lecture 6: Long Short-Term Memory RNNs","item":"http://localhost:1313/posts/cs224n/lesson_6/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Lecture 6: Long Short-Term Memory RNNs","name":"Lecture 6: Long Short-Term Memory RNNs","description":"第六讲：LSTM","keywords":["CS224N","NLP"],"articleBody":"Training an RNN Language Model Get a big corpus of text which is a sequence of words $x^{(1)},...,x^{(T)}$ Feed into RNN-LM; compute output distribution $\\hat y ^{(t)}$ for every timestep $t$​ Backpropagation for RNNs Problems with Vanishing and Exploding Gradients Vanishing gradient intuition Why is vanishing gradient a problem? 来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多 因此，模型权重只会根据近期效应而不是长期效应进行更新 If gradient is small, the model can’t learn this dependency. So, the model is unable to predict similar long distance dependencies at test time.\nWhy is exploding gradient a problem? This can cause bad updates: we take too large a step and reach a weird and bad parameter configuration (with large loss) In the worst case, this will result in Inf or NaN in your network (then you have to restart training from an earlier checkpoint) Gradient clipping: solution for exploding gradient Gradient clipping 梯度裁剪: if the norm of the gradient is greater than some threshold, scale it down before applying SGD update Intuition: take a step in the same direction, but a smaller step Long Short-Term Memory RNNs On step $t$ , there is a hidden state $h^{(t)}$ and a cell state $c^{(t)}$​\nBoth are vectors length $n$ The cell stores long-term information The LSTM can read, erase, and write information from the cell The selection of which information is erased/written/read is controlled by three corresponding gates\nThe gates are also vectors length $n$ On each timestep, each element of the gates can be $open (1), closed (0)$, or somewhere in-between The gates are dynamic: their value is computed based on the current context 遗忘门：控制上一个单元状态的保存与遗忘 输入门：控制写入单元格的新单元内容的哪些部分 输出门：控制单元的哪些内容输出到隐藏状态 新单元内容：这是要写入单元的新内容 单元状态：删除(“忘记”)上次单元状态中的一些内容，并写入(“输入”)一些新的单元内容 隐藏状态：从单元中读取(“output”)一些内容 How does LSTM solve vanishing gradients? The LSTM architecture makes it easier for the RNN to preserve information over many timesteps.\nLSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies\n","wordCount":"306","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-07-11T00:00:00Z","dateModified":"2024-07-11T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/cs224n/lesson_6/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Lecture 6: Long Short-Term Memory RNNs</h1><div class=post-description>第六讲：LSTM</div><div class=post-meta><span title='2024-07-11 00:00:00 +0000 UTC'>July 11, 2024</span>&nbsp;·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/CS224N/lesson_6.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#training-an-rnn-language-model>Training an RNN Language Model</a><ul><li><a href=#backpropagation-for-rnns>Backpropagation for RNNs</a></li></ul></li><li><a href=#problems-with-vanishing-and-exploding-gradients>Problems with Vanishing and Exploding Gradients</a><ul><li><a href=#vanishing-gradient-intuition>Vanishing gradient intuition</a></li><li><a href=#why-is-vanishing-gradient-a-problem>Why is vanishing gradient a problem?</a></li><li><a href=#why-is-exploding-gradient-a-problem>Why is exploding gradient a problem?</a></li><li><a href=#gradient-clipping-solution-for-exploding-gradient>Gradient clipping: solution for exploding gradient</a></li></ul></li><li><a href=#long-short-term-memory-rnns>Long Short-Term Memory RNNs</a><ul><li><a href=#how-does-lstm-solve-vanishing-gradients>How does LSTM solve vanishing gradients?</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=training-an-rnn-language-model>Training an RNN Language Model<a hidden class=anchor aria-hidden=true href=#training-an-rnn-language-model>#</a></h2><ul><li>Get a <strong>big corpus of text</strong> which is a sequence of words $x^{(1)},...,x^{(T)}$</li><li>Feed into RNN-LM; compute output distribution $\hat y ^{(t)}$ <strong>for every timestep $t$​</strong></li></ul><p><img loading=lazy src=/img/CS224N/lesson_6/img1.png alt></p><h3 id=backpropagation-for-rnns>Backpropagation for RNNs<a hidden class=anchor aria-hidden=true href=#backpropagation-for-rnns>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_6/img2.png alt></p><h2 id=problems-with-vanishing-and-exploding-gradients>Problems with Vanishing and Exploding Gradients<a hidden class=anchor aria-hidden=true href=#problems-with-vanishing-and-exploding-gradients>#</a></h2><h3 id=vanishing-gradient-intuition>Vanishing gradient intuition<a hidden class=anchor aria-hidden=true href=#vanishing-gradient-intuition>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_6/img3.png alt></p><h3 id=why-is-vanishing-gradient-a-problem>Why is vanishing gradient a problem?<a hidden class=anchor aria-hidden=true href=#why-is-vanishing-gradient-a-problem>#</a></h3><p><img loading=lazy src=/img/CS224N/lesson_6/img4.png alt></p><blockquote><ul><li>来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多</li><li>因此，模型权重只会根据近期效应而不是长期效应进行更新</li></ul></blockquote><p>If gradient is small, the model can’t learn this dependency. So, the model is unable to predict similar long distance dependencies at test time.</p><h3 id=why-is-exploding-gradient-a-problem>Why is exploding gradient a problem?<a hidden class=anchor aria-hidden=true href=#why-is-exploding-gradient-a-problem>#</a></h3><ul><li>This can cause bad updates: we take too large a step and reach a weird and bad parameter configuration (with large loss)</li><li>In the worst case, this will result in Inf or NaN in your network (then you have to restart training from an earlier checkpoint)</li></ul><h3 id=gradient-clipping-solution-for-exploding-gradient>Gradient clipping: solution for exploding gradient<a hidden class=anchor aria-hidden=true href=#gradient-clipping-solution-for-exploding-gradient>#</a></h3><ul><li>Gradient clipping 梯度裁剪: if the norm of the gradient is greater than some threshold, scale it down before applying SGD update</li><li>Intuition: take a step in the same direction, but a smaller step</li></ul><h2 id=long-short-term-memory-rnns>Long Short-Term Memory RNNs<a hidden class=anchor aria-hidden=true href=#long-short-term-memory-rnns>#</a></h2><p>On step $t$ , there is a <strong>hidden state</strong> $h^{(t)}$ and a <strong>cell state</strong> $c^{(t)}$​</p><ul><li>Both are vectors length $n$</li><li>The cell stores <strong>long-term information</strong></li><li>The LSTM can <strong>read, erase, and write</strong> information from the cell</li></ul><p>The selection of which information is erased/written/read is controlled by three corresponding <strong>gates</strong></p><ul><li>The gates are also vectors length $n$</li><li>On each timestep, each element of the gates can be $open (1), closed (0)$, or somewhere in-between</li><li>The gates are dynamic: their value is computed based on the current context</li></ul><p><img loading=lazy src=/img/CS224N/lesson_6/img5.png alt></p><blockquote><ul><li><strong>遗忘门</strong>：控制上一个单元状态的保存与遗忘</li><li><strong>输入门</strong>：控制写入单元格的新单元内容的哪些部分</li><li><strong>输出门</strong>：控制单元的哪些内容输出到隐藏状态</li><li><strong>新单元内容</strong>：这是要写入单元的新内容</li><li><strong>单元状态</strong>：删除(“忘记”)上次单元状态中的一些内容，并写入(“输入”)一些新的单元内容</li><li><strong>隐藏状态</strong>：从单元中读取(“output”)一些内容</li></ul></blockquote><p><img loading=lazy src=/img/CS224N/lesson_6/img6.png alt></p><h3 id=how-does-lstm-solve-vanishing-gradients>How does LSTM solve vanishing gradients?<a hidden class=anchor aria-hidden=true href=#how-does-lstm-solve-vanishing-gradients>#</a></h3><ul><li><p>The LSTM architecture makes it easier for the RNN to preserve information over many timesteps.</p></li><li><p>LSTM doesn’t guarantee that there is no vanishing/exploding gradient, but it does provide an easier way for the model to learn long-distance dependencies</p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/cs224n/>CS224N</a></li><li><a href=http://localhost:1313/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/nlp/rnn%E9%80%9F%E6%88%90%E4%BA%8C/><span class=title>« Prev</span><br><span>RNN速成（二）</span>
</a><a class=next href=http://localhost:1313/posts/nlp/rnn/><span class=title>Next »</span><br><span>RNN速成（一）</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>