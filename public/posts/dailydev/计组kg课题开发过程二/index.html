<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>《计组KG》课题开发过程（二） | KurongBlog</title>
<meta name=keywords content="Daily Dev,Project Dev"><meta name=description content="前言
自从上次记录已经过去了一个月，整个课题进展不大。原因一个是暑期有点摆，另一个是关系抽取确实比较繁琐。不管怎么说，来记录下吧。
NER 数据集下的模型训练
首先需要声明的是该阶段的模型不参与于最后 KG 的构建，目的仅仅是跑通模型训练、验证的过程，为后续阶段提供便利。
NER 数据集
该部分信息在完成 RE 部分后可能会发生些微变动，仅作参考，后续会做调整。
共标记5147条中文语句，实体共标注1472个。
下面是各个标签下的数量统计：

  
      
          Label
          Count
      
  
  
      
          TECH
          388
      
      
          COMP
          382
      
      
          STOR
          170
      
      
          DATA
          133
      
      
          INST
          105
      
      
          ARCH
          71
      
      
          IO
          61
      
      
          PERF
          54
      
      
          PROG
          52
      
      
          CORP
          17
      
      
          ALG
          16
      
      
          PROT
          15
      
      
          PER
          4
      
      
          GRP
          4
      
  

模型选择
模型有两大类：

传统深度学习方法

CNN-CRF
BiLSTM-CRF


BERT 系预训练模型，输出层为 CRF 或 MLP+Softmax

BERT：BERT 是一个双向 Transformer 模型，通过掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）任务进行预训练
RoBERTa：RoBERTa 是对 BERT 的优化版本，移除了 NSP 任务，并采用了动态掩码策略
ALBERT：ALBERT 是 BERT 的轻量级版本，通过参数共享和嵌入参数因子化来减少模型大小
XLM-RoBERTa：XLM-RoBERTa 是针对多语言的预训练模型，基于 RoBERTa 和 XLM 的结合



这里选择的是 XLM-RoBERTa，预训练模型选择的是 FacebookAI/xlm-roberta-large-finetuned-conll03-english · Hugging Face"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%BA%8C/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%BA%8C/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="《计组KG》课题开发过程（二）"><meta property="og:description" content="前言
自从上次记录已经过去了一个月，整个课题进展不大。原因一个是暑期有点摆，另一个是关系抽取确实比较繁琐。不管怎么说，来记录下吧。
NER 数据集下的模型训练
首先需要声明的是该阶段的模型不参与于最后 KG 的构建，目的仅仅是跑通模型训练、验证的过程，为后续阶段提供便利。
NER 数据集
该部分信息在完成 RE 部分后可能会发生些微变动，仅作参考，后续会做调整。
共标记5147条中文语句，实体共标注1472个。
下面是各个标签下的数量统计：

  
      
          Label
          Count
      
  
  
      
          TECH
          388
      
      
          COMP
          382
      
      
          STOR
          170
      
      
          DATA
          133
      
      
          INST
          105
      
      
          ARCH
          71
      
      
          IO
          61
      
      
          PERF
          54
      
      
          PROG
          52
      
      
          CORP
          17
      
      
          ALG
          16
      
      
          PROT
          15
      
      
          PER
          4
      
      
          GRP
          4
      
  

模型选择
模型有两大类：

传统深度学习方法

CNN-CRF
BiLSTM-CRF


BERT 系预训练模型，输出层为 CRF 或 MLP+Softmax

BERT：BERT 是一个双向 Transformer 模型，通过掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）任务进行预训练
RoBERTa：RoBERTa 是对 BERT 的优化版本，移除了 NSP 任务，并采用了动态掩码策略
ALBERT：ALBERT 是 BERT 的轻量级版本，通过参数共享和嵌入参数因子化来减少模型大小
XLM-RoBERTa：XLM-RoBERTa 是针对多语言的预训练模型，基于 RoBERTa 和 XLM 的结合



这里选择的是 XLM-RoBERTa，预训练模型选择的是 FacebookAI/xlm-roberta-large-finetuned-conll03-english · Hugging Face"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%BA%8C/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-09-14T00:00:00+00:00"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="《计组KG》课题开发过程（二）"><meta name=twitter:description content="前言
自从上次记录已经过去了一个月，整个课题进展不大。原因一个是暑期有点摆，另一个是关系抽取确实比较繁琐。不管怎么说，来记录下吧。
NER 数据集下的模型训练
首先需要声明的是该阶段的模型不参与于最后 KG 的构建，目的仅仅是跑通模型训练、验证的过程，为后续阶段提供便利。
NER 数据集
该部分信息在完成 RE 部分后可能会发生些微变动，仅作参考，后续会做调整。
共标记5147条中文语句，实体共标注1472个。
下面是各个标签下的数量统计：

  
      
          Label
          Count
      
  
  
      
          TECH
          388
      
      
          COMP
          382
      
      
          STOR
          170
      
      
          DATA
          133
      
      
          INST
          105
      
      
          ARCH
          71
      
      
          IO
          61
      
      
          PERF
          54
      
      
          PROG
          52
      
      
          CORP
          17
      
      
          ALG
          16
      
      
          PROT
          15
      
      
          PER
          4
      
      
          GRP
          4
      
  

模型选择
模型有两大类：

传统深度学习方法

CNN-CRF
BiLSTM-CRF


BERT 系预训练模型，输出层为 CRF 或 MLP+Softmax

BERT：BERT 是一个双向 Transformer 模型，通过掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）任务进行预训练
RoBERTa：RoBERTa 是对 BERT 的优化版本，移除了 NSP 任务，并采用了动态掩码策略
ALBERT：ALBERT 是 BERT 的轻量级版本，通过参数共享和嵌入参数因子化来减少模型大小
XLM-RoBERTa：XLM-RoBERTa 是针对多语言的预训练模型，基于 RoBERTa 和 XLM 的结合



这里选择的是 XLM-RoBERTa，预训练模型选择的是 FacebookAI/xlm-roberta-large-finetuned-conll03-english · Hugging Face"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"《计组KG》课题开发过程（二）","item":"http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%BA%8C/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"《计组KG》课题开发过程（二）","name":"《计组KG》课题开发过程（二）","description":"前言 自从上次记录已经过去了一个月，整个课题进展不大。原因一个是暑期有点摆，另一个是关系抽取确实比较繁琐。不管怎么说，来记录下吧。\nNER 数据集下的模型训练 首先需要声明的是该阶段的模型不参与于最后 KG 的构建，目的仅仅是跑通模型训练、验证的过程，为后续阶段提供便利。\nNER 数据集 该部分信息在完成 RE 部分后可能会发生些微变动，仅作参考，后续会做调整。\n共标记5147条中文语句，实体共标注1472个。\n下面是各个标签下的数量统计：\nLabel Count TECH 388 COMP 382 STOR 170 DATA 133 INST 105 ARCH 71 IO 61 PERF 54 PROG 52 CORP 17 ALG 16 PROT 15 PER 4 GRP 4 模型选择 模型有两大类：\n传统深度学习方法 CNN-CRF BiLSTM-CRF BERT 系预训练模型，输出层为 CRF 或 MLP+Softmax BERT：BERT 是一个双向 Transformer 模型，通过掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）任务进行预训练 RoBERTa：RoBERTa 是对 BERT 的优化版本，移除了 NSP 任务，并采用了动态掩码策略 ALBERT：ALBERT 是 BERT 的轻量级版本，通过参数共享和嵌入参数因子化来减少模型大小 XLM-RoBERTa：XLM-RoBERTa 是针对多语言的预训练模型，基于 RoBERTa 和 XLM 的结合 这里选择的是 XLM-RoBERTa，预训练模型选择的是 FacebookAI/xlm-roberta-large-finetuned-conll03-english · Hugging Face\n","keywords":["Daily Dev","Project Dev"],"articleBody":"前言 自从上次记录已经过去了一个月，整个课题进展不大。原因一个是暑期有点摆，另一个是关系抽取确实比较繁琐。不管怎么说，来记录下吧。\nNER 数据集下的模型训练 首先需要声明的是该阶段的模型不参与于最后 KG 的构建，目的仅仅是跑通模型训练、验证的过程，为后续阶段提供便利。\nNER 数据集 该部分信息在完成 RE 部分后可能会发生些微变动，仅作参考，后续会做调整。\n共标记5147条中文语句，实体共标注1472个。\n下面是各个标签下的数量统计：\nLabel Count TECH 388 COMP 382 STOR 170 DATA 133 INST 105 ARCH 71 IO 61 PERF 54 PROG 52 CORP 17 ALG 16 PROT 15 PER 4 GRP 4 模型选择 模型有两大类：\n传统深度学习方法 CNN-CRF BiLSTM-CRF BERT 系预训练模型，输出层为 CRF 或 MLP+Softmax BERT：BERT 是一个双向 Transformer 模型，通过掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）任务进行预训练 RoBERTa：RoBERTa 是对 BERT 的优化版本，移除了 NSP 任务，并采用了动态掩码策略 ALBERT：ALBERT 是 BERT 的轻量级版本，通过参数共享和嵌入参数因子化来减少模型大小 XLM-RoBERTa：XLM-RoBERTa 是针对多语言的预训练模型，基于 RoBERTa 和 XLM 的结合 这里选择的是 XLM-RoBERTa，预训练模型选择的是 FacebookAI/xlm-roberta-large-finetuned-conll03-english · Hugging Face\n模型代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class XLMRobertaForNER(XLMRobertaPreTrainedModel): def __init__(self, config, use_bilstm=False, use_crf=True): super(XLMRobertaForNER, self).__init__(config) self.use_bilstm = use_bilstm self.use_crf = use_crf self.xlm_robert = XLMRobertaModel(config, add_pooling_layer=False) self.dropout = nn.Dropout(0.1) self.cls = nn.Linear(config.hidden_size, config.num_labels) if self.use_bilstm: self.bilstm = nn.LSTM( input_size=config.hidden_size, hidden_size=config.hidden_size, num_layers=1, batch_first=True, bidirectional=True ) if self.use_crf: self.crf = CRF(config.num_labels, batch_first=True) else: self.softmax = nn.Softmax(dim=1) self.init_weights() def forward(self, input_ids, attention_mask=None, labels=None): outputs = self.xlm_robert(input_ids, attention_mask=attention_mask) sequence_output = outputs[0] if self.use_bilstm: bilstm_output, _ = self.bilstm(sequence_output) sequence_output = torch.cat((sequence_output, bilstm_output), dim=2) # 合并BiLSTM的输出和初始输入 sequence_output = self.dropout(sequence_output) emissions = self.cls(sequence_output) if self.use_crf: if labels is not None: labels = torch.where(labels==-100, torch.tensor(0).to(labels.device), labels) loss = -self.crf(emissions, labels, mask=attention_mask.byte(), reduction='mean') return loss else: prediction = self.crf.decode(emissions, mask=attention_mask.byte()) return prediction else: logits = self.softmax(emissions) return logits 训练过程 初始化 DataLoader、Tokenizer ； 修改 Model Config、Training Params ； 初始化 Model，启动训练 Loop 。 以下是本次训练所选取的各项参数：\n1 2 3 4 5 6 7 8 9 10 11 training_params = { 'model_name': xlm_roberta_model_name, 'weight_decay': 1e-2, 'warmup_steps': 500, 'warmup_proportion': 0.1, 'learning_rate': 5e-5, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-8, 'num_train_epochs': 16, } Loss 函数的计算方式在代码中：\n1 loss = -self.crf(emissions, labels, mask=attention_mask.byte(), reduction='mean') Optimizer 选择的是 AdamW，同时运用了 Learning Rate Scheduler 来完成暖机。\n验证结果 关于这部分，数据不保证准确、全面，因此只作为参考，切勿用于其他用途。\n只使用1000条小批次 NER 训练集，经过16轮训练后，各 base 模型在 NER 测试集上的泛化性能：\nModel Precision Recall F1 BERT-CRF 74.1% 81.8% 74.6% ALBERT-CRF 70.7% 73.4% 68.6% XLM-RoBERTa-CRF 88.1% 95.4% 90.3% 各项数据都是 XLM-RoBERTa-CRF 大幅领先。\nXLM-RoBERTa-CRF 所选取的预训练模型经过了 Facebook 团队在超过 2.5TB 的多语言大规模数据集上的训练，模型参数达 0.56B 。\n关于这部分数据目前来看缺陷相当大：\n数据不全：无法反映数据集的完整信息，模型也更容易出现过拟合、测试集泛化性能一般 模型选择不具有代表性：只有 BERT 系，没有其他结构类型的模型，如 CNN、BiLSTM 等 没有做消融实验：没有 Softmax、CRF 层的对比 性能指标选取以及模型性能差距过大：在做实验时，所选取的这些指标都是 Samples 上的，即整个数据集上的指标，不能完整反应模型性能。而且这些模型的性能差距不是在5个点内，都10个点往上了，所以仅作参考是没啥问题的 RE 数据集的构建 关系定义 关系名称 英文名称 说明 包含 contain A 包含 B 的内容（单向） 顺序 sequence 学习 A 前需先学 B，或学习 A 后支持 B（单向） 同义 synonymy 名称不同但指同一内容（双向） 相关 related A 与 B 存在相关性（双向） 特性 attribute 包括功能、性能、方法等特点（单向） 预标注前的准备工作 该部分信息在进行预标注过程中可能会发生变动，仅作参考，后续会做调整。\n对302条文本进行了关系标注，其中184条用于训练、60条用于验证、58条用于测试。\n下面是一个简单的统计：\nContain Related Attribute Synonymy Sequence 188 183 158 82 84 预标注 模型选择的 BiLSTM-MAML 。\nMAML（Model-Agnostic Meta-Learning）：是一种元学习算法，同样是一种 Few-Shot Learning 算法。核心思想是通过学习模型参数的初始值，使得模型能够在面对新任务时快速适应。\nMAML 算法在使得模型快速适应的同时，也增加了训练阶段的计算量，这会导致训练时间的延长，不利于后续模型验证。所以决定选用结构更简单、同时性能较好的 BiLSTM 模型。\n（模型的训练、标注效果）\nNER-RE 数据集的构建 （暂定）\nKG 的展示 （暂定）\n","wordCount":"423","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-09-14T00:00:00Z","dateModified":"2024-09-14T00:00:00Z","author":{"@type":"Person","name":"Kurong"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%BA%8C/"},"publisher":{"@type":"Organization","name":"KurongBlog","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">《计组KG》课题开发过程（二）</h1><div class=post-meta><span title='2024-09-14 00:00:00 +0000 UTC'>September 14, 2024</span>&nbsp;·&nbsp;Kurong&nbsp;|&nbsp;<a href=https://github.com/KurongTohsaka/KurongTohsaka.github.io/content/posts/DailyDev/%e3%80%8a%e8%ae%a1%e7%bb%84KG%e3%80%8b%e8%af%be%e9%a2%98%e5%bc%80%e5%8f%91%e8%bf%87%e7%a8%8b%ef%bc%88%e4%ba%8c%ef%bc%89.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#前言>前言</a></li><li><a href=#ner-数据集下的模型训练>NER 数据集下的模型训练</a><ul><li><a href=#ner-数据集>NER 数据集</a></li><li><a href=#模型选择>模型选择</a></li><li><a href=#训练过程>训练过程</a></li><li><a href=#验证结果>验证结果</a></li></ul></li><li><a href=#re-数据集的构建>RE 数据集的构建</a><ul><li><a href=#关系定义>关系定义</a></li><li><a href=#预标注前的准备工作>预标注前的准备工作</a></li><li><a href=#预标注>预标注</a></li></ul></li><li><a href=#ner-re-数据集的构建>NER-RE 数据集的构建</a></li><li><a href=#kg-的展示>KG 的展示</a></li></ul></nav></div></details></div><div class=post-content><h2 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h2><p>自从上次记录已经过去了一个月，整个课题进展不大。原因一个是暑期有点摆，另一个是关系抽取确实比较繁琐。不管怎么说，来记录下吧。</p><h2 id=ner-数据集下的模型训练>NER 数据集下的模型训练<a hidden class=anchor aria-hidden=true href=#ner-数据集下的模型训练>#</a></h2><p>首先需要声明的是该阶段的模型不参与于最后 KG 的构建，目的仅仅是跑通模型训练、验证的过程，为后续阶段提供便利。</p><h3 id=ner-数据集>NER 数据集<a hidden class=anchor aria-hidden=true href=#ner-数据集>#</a></h3><p><strong>该部分信息在完成 RE 部分后可能会发生些微变动，仅作参考，后续会做调整。</strong></p><p>共标记5147条中文语句，实体共标注1472个。</p><p>下面是各个标签下的数量统计：</p><table><thead><tr><th style=text-align:left><strong>Label</strong></th><th style=text-align:left><strong>Count</strong></th></tr></thead><tbody><tr><td style=text-align:left>TECH</td><td style=text-align:left>388</td></tr><tr><td style=text-align:left>COMP</td><td style=text-align:left>382</td></tr><tr><td style=text-align:left>STOR</td><td style=text-align:left>170</td></tr><tr><td style=text-align:left>DATA</td><td style=text-align:left>133</td></tr><tr><td style=text-align:left>INST</td><td style=text-align:left>105</td></tr><tr><td style=text-align:left>ARCH</td><td style=text-align:left>71</td></tr><tr><td style=text-align:left>IO</td><td style=text-align:left>61</td></tr><tr><td style=text-align:left>PERF</td><td style=text-align:left>54</td></tr><tr><td style=text-align:left>PROG</td><td style=text-align:left>52</td></tr><tr><td style=text-align:left>CORP</td><td style=text-align:left>17</td></tr><tr><td style=text-align:left>ALG</td><td style=text-align:left>16</td></tr><tr><td style=text-align:left>PROT</td><td style=text-align:left>15</td></tr><tr><td style=text-align:left>PER</td><td style=text-align:left>4</td></tr><tr><td style=text-align:left>GRP</td><td style=text-align:left>4</td></tr></tbody></table><h3 id=模型选择>模型选择<a hidden class=anchor aria-hidden=true href=#模型选择>#</a></h3><p>模型有两大类：</p><ul><li>传统深度学习方法<ul><li>CNN-CRF</li><li>BiLSTM-CRF</li></ul></li><li>BERT 系预训练模型，输出层为 CRF 或 MLP+Softmax<ul><li>BERT：BERT 是一个双向 Transformer 模型，通过掩码语言模型（Masked Language Model, MLM）和下一句预测（Next Sentence Prediction, NSP）任务进行预训练</li><li>RoBERTa：RoBERTa 是对 BERT 的优化版本，移除了 NSP 任务，并采用了动态掩码策略</li><li>ALBERT：ALBERT 是 BERT 的轻量级版本，通过参数共享和嵌入参数因子化来减少模型大小</li><li>XLM-RoBERTa：XLM-RoBERTa 是针对多语言的预训练模型，基于 RoBERTa 和 XLM 的结合</li></ul></li></ul><p>这里选择的是 XLM-RoBERTa，预训练模型选择的是 <a href=https://huggingface.co/FacebookAI/xlm-roberta-large-finetuned-conll03-english>FacebookAI/xlm-roberta-large-finetuned-conll03-english · Hugging Face</a></p><p>模型代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1> 1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2> 2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3> 3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4> 4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5> 5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6> 6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7> 7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8> 8</a>
</span><span class=lnt id=hl-0-9><a class=lnlinks href=#hl-0-9> 9</a>
</span><span class=lnt id=hl-0-10><a class=lnlinks href=#hl-0-10>10</a>
</span><span class=lnt id=hl-0-11><a class=lnlinks href=#hl-0-11>11</a>
</span><span class=lnt id=hl-0-12><a class=lnlinks href=#hl-0-12>12</a>
</span><span class=lnt id=hl-0-13><a class=lnlinks href=#hl-0-13>13</a>
</span><span class=lnt id=hl-0-14><a class=lnlinks href=#hl-0-14>14</a>
</span><span class=lnt id=hl-0-15><a class=lnlinks href=#hl-0-15>15</a>
</span><span class=lnt id=hl-0-16><a class=lnlinks href=#hl-0-16>16</a>
</span><span class=lnt id=hl-0-17><a class=lnlinks href=#hl-0-17>17</a>
</span><span class=lnt id=hl-0-18><a class=lnlinks href=#hl-0-18>18</a>
</span><span class=lnt id=hl-0-19><a class=lnlinks href=#hl-0-19>19</a>
</span><span class=lnt id=hl-0-20><a class=lnlinks href=#hl-0-20>20</a>
</span><span class=lnt id=hl-0-21><a class=lnlinks href=#hl-0-21>21</a>
</span><span class=lnt id=hl-0-22><a class=lnlinks href=#hl-0-22>22</a>
</span><span class=lnt id=hl-0-23><a class=lnlinks href=#hl-0-23>23</a>
</span><span class=lnt id=hl-0-24><a class=lnlinks href=#hl-0-24>24</a>
</span><span class=lnt id=hl-0-25><a class=lnlinks href=#hl-0-25>25</a>
</span><span class=lnt id=hl-0-26><a class=lnlinks href=#hl-0-26>26</a>
</span><span class=lnt id=hl-0-27><a class=lnlinks href=#hl-0-27>27</a>
</span><span class=lnt id=hl-0-28><a class=lnlinks href=#hl-0-28>28</a>
</span><span class=lnt id=hl-0-29><a class=lnlinks href=#hl-0-29>29</a>
</span><span class=lnt id=hl-0-30><a class=lnlinks href=#hl-0-30>30</a>
</span><span class=lnt id=hl-0-31><a class=lnlinks href=#hl-0-31>31</a>
</span><span class=lnt id=hl-0-32><a class=lnlinks href=#hl-0-32>32</a>
</span><span class=lnt id=hl-0-33><a class=lnlinks href=#hl-0-33>33</a>
</span><span class=lnt id=hl-0-34><a class=lnlinks href=#hl-0-34>34</a>
</span><span class=lnt id=hl-0-35><a class=lnlinks href=#hl-0-35>35</a>
</span><span class=lnt id=hl-0-36><a class=lnlinks href=#hl-0-36>36</a>
</span><span class=lnt id=hl-0-37><a class=lnlinks href=#hl-0-37>37</a>
</span><span class=lnt id=hl-0-38><a class=lnlinks href=#hl-0-38>38</a>
</span><span class=lnt id=hl-0-39><a class=lnlinks href=#hl-0-39>39</a>
</span><span class=lnt id=hl-0-40><a class=lnlinks href=#hl-0-40>40</a>
</span><span class=lnt id=hl-0-41><a class=lnlinks href=#hl-0-41>41</a>
</span><span class=lnt id=hl-0-42><a class=lnlinks href=#hl-0-42>42</a>
</span><span class=lnt id=hl-0-43><a class=lnlinks href=#hl-0-43>43</a>
</span><span class=lnt id=hl-0-44><a class=lnlinks href=#hl-0-44>44</a>
</span><span class=lnt id=hl-0-45><a class=lnlinks href=#hl-0-45>45</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>XLMRobertaForNER</span><span class=p>(</span><span class=n>XLMRobertaPreTrainedModel</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>,</span> <span class=n>use_bilstm</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>use_crf</span><span class=o>=</span><span class=kc>True</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>XLMRobertaForNER</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_bilstm</span> <span class=o>=</span> <span class=n>use_bilstm</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>use_crf</span> <span class=o>=</span> <span class=n>use_crf</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>xlm_robert</span> <span class=o>=</span> <span class=n>XLMRobertaModel</span><span class=p>(</span><span class=n>config</span><span class=p>,</span> <span class=n>add_pooling_layer</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cls</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>num_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_bilstm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>bilstm</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LSTM</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>input_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>hidden_size</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>num_layers</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>bidirectional</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_crf</span><span class=p>:</span>
</span></span><span class=line><span class=cl>          	<span class=bp>self</span><span class=o>.</span><span class=n>crf</span> <span class=o>=</span> <span class=n>CRF</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>num_labels</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>          	<span class=bp>self</span><span class=o>.</span><span class=n>softmax</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Softmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>init_weights</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>labels</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>outputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>xlm_robert</span><span class=p>(</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>sequence_output</span> <span class=o>=</span> <span class=n>outputs</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_bilstm</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>bilstm_output</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>bilstm</span><span class=p>(</span><span class=n>sequence_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>sequence_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>sequence_output</span><span class=p>,</span> <span class=n>bilstm_output</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># 合并BiLSTM的输出和初始输入</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>sequence_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>sequence_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>emissions</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cls</span><span class=p>(</span><span class=n>sequence_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>use_crf</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>labels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>labels</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>labels</span><span class=o>==-</span><span class=mi>100</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>labels</span><span class=o>.</span><span class=n>device</span><span class=p>),</span> <span class=n>labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>crf</span><span class=p>(</span><span class=n>emissions</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=o>.</span><span class=n>byte</span><span class=p>(),</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>return</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              	<span class=n>prediction</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>crf</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>emissions</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=o>.</span><span class=n>byte</span><span class=p>())</span>
</span></span><span class=line><span class=cl>              	<span class=k>return</span> <span class=n>prediction</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>emissions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>logits</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=训练过程>训练过程<a hidden class=anchor aria-hidden=true href=#训练过程>#</a></h3><ol><li>初始化 DataLoader、Tokenizer ；</li><li>修改 Model Config、Training Params ；</li><li>初始化 Model，启动训练 Loop 。</li></ol><p>以下是本次训练所选取的各项参数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>training_params</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;model_name&#39;</span><span class=p>:</span> <span class=n>xlm_roberta_model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;weight_decay&#39;</span><span class=p>:</span> <span class=mf>1e-2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;warmup_steps&#39;</span><span class=p>:</span> <span class=mi>500</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;warmup_proportion&#39;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;learning_rate&#39;</span><span class=p>:</span> <span class=mf>5e-5</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;adam_beta1&#39;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;adam_beta2&#39;</span><span class=p>:</span> <span class=mf>0.999</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;adam_epsilon&#39;</span><span class=p>:</span> <span class=mf>1e-8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;num_train_epochs&#39;</span><span class=p>:</span> <span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Loss 函数的计算方式在代码中：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=bp>self</span><span class=o>.</span><span class=n>crf</span><span class=p>(</span><span class=n>emissions</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>attention_mask</span><span class=o>.</span><span class=n>byte</span><span class=p>(),</span> <span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;mean&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Optimizer 选择的是 AdamW，同时运用了 Learning Rate Scheduler 来完成暖机。</p><h3 id=验证结果>验证结果<a hidden class=anchor aria-hidden=true href=#验证结果>#</a></h3><p><strong>关于这部分，数据不保证准确、全面，因此只作为参考，切勿用于其他用途。</strong></p><p>只使用1000条小批次 NER 训练集，经过16轮训练后，各 base 模型在 NER 测试集上的泛化性能：</p><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:left>Precision</th><th style=text-align:left>Recall</th><th style=text-align:left>F1</th></tr></thead><tbody><tr><td style=text-align:left>BERT-CRF</td><td style=text-align:left>74.1%</td><td style=text-align:left>81.8%</td><td style=text-align:left>74.6%</td></tr><tr><td style=text-align:left>ALBERT-CRF</td><td style=text-align:left>70.7%</td><td style=text-align:left>73.4%</td><td style=text-align:left>68.6%</td></tr><tr><td style=text-align:left><strong>XLM-RoBERTa-CRF</strong></td><td style=text-align:left><strong>88.1%</strong></td><td style=text-align:left><strong>95.4%</strong></td><td style=text-align:left><strong>90.3%</strong></td></tr></tbody></table><p>各项数据都是 XLM-RoBERTa-CRF 大幅领先。</p><blockquote><p>XLM-RoBERTa-CRF 所选取的预训练模型经过了 Facebook 团队在超过 2.5TB 的多语言大规模数据集上的训练，模型参数达 0.56B 。</p></blockquote><p>关于这部分数据目前来看缺陷相当大：</p><ul><li>数据不全：无法反映数据集的完整信息，模型也更容易出现过拟合、测试集泛化性能一般</li><li>模型选择不具有代表性：只有 BERT 系，没有其他结构类型的模型，如 CNN、BiLSTM 等</li><li>没有做消融实验：没有 Softmax、CRF 层的对比</li><li>性能指标选取以及模型性能差距过大：在做实验时，所选取的这些指标都是 Samples 上的，即整个数据集上的指标，不能完整反应模型性能。而且这些模型的性能差距不是在5个点内，都10个点往上了，所以仅作参考是没啥问题的</li></ul><h2 id=re-数据集的构建>RE 数据集的构建<a hidden class=anchor aria-hidden=true href=#re-数据集的构建>#</a></h2><h3 id=关系定义>关系定义<a hidden class=anchor aria-hidden=true href=#关系定义>#</a></h3><table><thead><tr><th style=text-align:left>关系名称</th><th style=text-align:left>英文名称</th><th style=text-align:left>说明</th></tr></thead><tbody><tr><td style=text-align:left>包含</td><td style=text-align:left>contain</td><td style=text-align:left>A 包含 B 的内容（单向）</td></tr><tr><td style=text-align:left>顺序</td><td style=text-align:left>sequence</td><td style=text-align:left>学习 A 前需先学 B，或学习 A 后支持 B（单向）</td></tr><tr><td style=text-align:left>同义</td><td style=text-align:left>synonymy</td><td style=text-align:left>名称不同但指同一内容（双向）</td></tr><tr><td style=text-align:left>相关</td><td style=text-align:left>related</td><td style=text-align:left>A 与 B 存在相关性（双向）</td></tr><tr><td style=text-align:left>特性</td><td style=text-align:left>attribute</td><td style=text-align:left>包括功能、性能、方法等特点（单向）</td></tr></tbody></table><h3 id=预标注前的准备工作>预标注前的准备工作<a hidden class=anchor aria-hidden=true href=#预标注前的准备工作>#</a></h3><p><strong>该部分信息在进行预标注过程中可能会发生变动，仅作参考，后续会做调整。</strong></p><p>对302条文本进行了关系标注，其中184条用于训练、60条用于验证、58条用于测试。</p><p>下面是一个简单的统计：</p><table><thead><tr><th style=text-align:left><strong>Contain</strong></th><th style=text-align:left><strong>Related</strong></th><th style=text-align:left><strong>Attribute</strong></th><th style=text-align:left><strong>Synonymy</strong></th><th style=text-align:left><strong>Sequence</strong></th></tr></thead><tbody><tr><td style=text-align:left>188</td><td style=text-align:left>183</td><td style=text-align:left>158</td><td style=text-align:left>82</td><td style=text-align:left>84</td></tr></tbody></table><h3 id=预标注>预标注<a hidden class=anchor aria-hidden=true href=#预标注>#</a></h3><p>模型选择的 BiLSTM-MAML 。</p><blockquote><p><strong>MAML</strong>（<strong>Model-Agnostic Meta-Learning</strong>）：是一种元学习算法，同样是一种 Few-Shot Learning 算法。核心思想是通过学习模型参数的初始值，使得模型能够在面对新任务时快速适应。</p></blockquote><p>MAML 算法在使得模型快速适应的同时，也增加了训练阶段的计算量，这会导致训练时间的延长，不利于后续模型验证。所以决定选用结构更简单、同时性能较好的 BiLSTM 模型。</p><p>（模型的训练、标注效果）</p><h2 id=ner-re-数据集的构建>NER-RE 数据集的构建<a hidden class=anchor aria-hidden=true href=#ner-re-数据集的构建>#</a></h2><p>（暂定）</p><h2 id=kg-的展示>KG 的展示<a hidden class=anchor aria-hidden=true href=#kg-的展示>#</a></h2><p>（暂定）</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/daily-dev/>Daily Dev</a></li><li><a href=http://localhost:1313/tags/project-dev/>Project Dev</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/papernotes/rapl/><span class=title>« Prev</span><br><span>RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction</span>
</a><a class=next href=http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E7%95%AA%E5%A4%96%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E6%80%9D%E8%80%83%E8%BF%87%E7%A8%8B/><span class=title>Next »</span><br><span>《计组KG》课题开发番外：关系抽取的思考过程</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>