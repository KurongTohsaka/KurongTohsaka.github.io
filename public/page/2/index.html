<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.135.0"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>KurongBlog</title>
<meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="记录日常"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/index.xml><link rel=alternate type=application/json href=http://localhost:1313/index.json><link rel=alternate hreflang=en href=http://localhost:1313/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="KurongBlog"><meta property="og:description" content="记录日常"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="KurongBlog"><meta name=twitter:description content="记录日常"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"KurongBlog","url":"http://localhost:1313/","description":"记录日常","thumbnailUrl":"http://localhost:1313/%3Clink%20/%20abs%20url%3E","sameAs":["https://github.com/705248010/705248010.github.io"]}</script></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>8.06-8.11：一周总结</h2></header><div class=entry-content><p>本周工作 本周工作是从8月10号vpn重置之后才开始进行的，所以内容有点少
完成了第一版训练、验证代码 调试第一个模型，修复各种bug 学习 Transformer 相关内容 课题下周改进 先快速训练出几个模型出来，选出一个最好的模型 进行第二次及多次迭代训练，更新词典 关系抽取看相关文献 本周反思 本周算是玩爽了，课题下周尽快开展关系抽取的调研工作。</p></div><footer class=entry-footer><span title='2024-08-11 00:00:00 +0000 UTC'>August 11, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 8.06-8.11：一周总结" href=http://localhost:1313/posts/weeklyworking/2024_08_06-2024_08_11/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Encoder-Decoder 架构</h2></header><div class=entry-content><p>原理 Encoder-Decoder架构通常由两个主要部分组成：编码器（Encoder）和解码器（Decoder）。
编码器（Encoder）：编码器的任务是将输入序列（如一句话）转换为一个固定长度的上下文向量（context vector）。这个过程通常通过递归神经网络（RNN）、长短期记忆网络（LSTM）或门控循环单元（GRU）来实现。编码器逐步读取输入序列的每个元素，并将其信息压缩到上下文向量中。 解码器（Decoder）：解码器接收编码器生成的上下文向量，并将其转换为输出序列。解码器同样可以使用RNN、LSTM或GRU。解码器在生成每个输出元素时，会考虑上下文向量以及之前生成的输出元素。 作用 Encoder-Decoder架构主要用于处理需要将一个序列转换为另一个序列的任务，例如：
机器翻译：将一种语言的句子翻译成另一种语言。 文本摘要：将长文本压缩成简短摘要。 对话系统：生成对用户输入的响应。 近年来，基于Transformer的Encoder-Decoder架构（如BERT、GPT、T5等）因其更好的性能和并行计算能力，逐渐取代了传统的RNN架构。
优缺点 优点：
灵活性：可以处理不同长度的输入和输出序列。 强大的表示能力：能够捕捉输入序列中的复杂模式和关系。 缺点：
长距离依赖问题：传统RNN在处理长序列时可能会遗忘早期的信息。 计算复杂度高：训练和推理过程需要大量计算资源。</p></div><footer class=entry-footer><span title='2024-08-11 00:00:00 +0000 UTC'>August 11, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Encoder-Decoder 架构" href=http://localhost:1313/posts/nlp/encoder-decoder/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>残差连接</h2></header><div class=entry-content><p>原理 残差连接（Residual Connection）最早由何凯明等人在2015年提出的 ResNet 中引入。ResNet 通过引入残差块，使得网络可以扩展到更深的层数，并在 ImageNet 比赛中取得了显著的成功。
残差连接的核心思想是引入跳跃连接，将输入信号直接传递到网络的后续层，从而构建了一条捷径路径。这种结构允许网络学习输入和输出之间的残差，而不是直接学习输出。
残差连接可以表示为： $$ y=F(x)+x $$ 其中，$x$ 表示输入，$F(x)$ 表示经过非线性变换后的输出。
作用 解决梯度消失和梯度爆炸问题 提高训练效率 增强模型的泛化性能 例子 下图是 Transformer 论文中的模型结构图。
可以看到在每一个 Attention Layer 中都有一个 Add ，原输入和 Multi-head 变换后的输出做了一个简单的相加操作，而这就是所谓的残差连接。</p></div><footer class=entry-footer><span title='2024-08-11 00:00:00 +0000 UTC'>August 11, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 残差连接" href=http://localhost:1313/posts/nlp/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=http://localhost:1313/cover/%E5%9F%BA%E4%BA%8EMRC%E7%9A%84%E8%AE%BE%E5%A4%87%E6%95%85%E9%9A%9C%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>《基于MRC的设备故障命名实体识别方法》笔记</h2></header><div class=entry-content><p>MRC 本论文关于设备故障领域的NER任务，将 NER 的序列标注问题重构为 MRC 问题。
MRC（Machine Reading Comprehension）机器阅读理解，与文生成和机器翻译任务类似。本论文使用该方法，根堆每个实体类型，设计了相应地自然语言形式的查询，通过上下文的查询来定位并提取实体。例如，针对故障代码 the machine suffers from error[#10482] 的命名实体识别，被形式化为 “文本中提到了哪种故障代码 ” 。
人话：先通过某个文生成/机器翻译模型将错误代码转化为中文，然后通过预定的错误代码进行映射
本文的主要工作如下：
将 NER 任务转化为 MRC 问题，采用自然语言查询来实现命名实体的识别。 通过在查询中集成预设的命名实体类别信息，克服传统方法中实体类别语义信息缺少的限制，提升命名实体识别的准确性。 结合设备故障数据集的构建，开展相关的实 验对比，并进行定量分析，证明了本文方法的有效性。 模型结构 编码层为 ALBERT 特征提取为 BiLSTM 分类器为两个sigmoid，分别预测实体的起始索引、结束索引 数据集 对比试验 模型为以下几类：
结果：
消融实验</p></div><footer class=entry-footer><span title='2024-08-06 00:00:00 +0000 UTC'>August 6, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 《基于MRC的设备故障命名实体识别方法》笔记" href=http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8Emrc%E7%9A%84%E8%AE%BE%E5%A4%87%E6%95%85%E9%9A%9C%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95_%E5%BE%90%E9%B9%8F/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>7.29-8.04：一周总结</h2></header><div class=entry-content><p>本周任务 改进了数据集的标注格式，并更新了词典 完成 BERT 系、BiLSTM-CRF、CNN-CRF 等模型代码 第一版训练、验证代码进度50%，但是 vpn 流量过期，需要等到10号以后才能继续训练😅 课题下周改进 完成第一版训练、验证代码 改进现有模型结构，尽可能地从微调 BERT 的过时手段中走出 记录实验数据 继续 cs224n、Transformer 论文学习 本周反思 本周实在是有太多事，课题进展有限，预计下周会有一定进展。</p></div><footer class=entry-footer><span title='2024-08-06 00:00:00 +0000 UTC'>August 6, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 7.29-8.04：一周总结" href=http://localhost:1313/posts/weeklyworking/2024_07_29-2024_08_04/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>《计组KG》课题开发过程（一）</h2></header><div class=entry-content><p>前言 开发该课题也有一个月了，整个过程并不是很顺利，很多细节部分如果没有得到及时梳理，对以后的研究和论文写作也有坏处。基于以上和其他原因，遂决定分阶段进行记录。
数据集 深度学习项目的良好开端就是有一个优良标注的数据集。而由于本课题起源于一个极小领域下，导致数据集必须完全自建。所有工作由我一人进行，工作量不可避免的大。所以必须尽可能的减少工作量，尽量实现在课题的中后期的所有标注工作都由程序自动化解决。
计组数据集的构建分为了以下几个过程：
计组数据集来源 数据预处理 数据集的预标注 基于词典的多次迭代标注 数据集格式的转换 接下来对每一个部分进行详述。
计组数据集来源 目前数据的来源如下：
计算机组成原理第6版 (白中英)，pdf 转 txt 计算机组成原理第6版 (白中英) 课件，ppt 转 txt 数据预处理 以下是大概的预处理过程：
将所有的文本合并到一个文件，方便后续操作； 手工去掉一些与课题无关的文本和小部分错误内容； 去掉所有的空白字符（空格、换行符、制表符等）； 去掉所有的特殊字符（数字、半角符号、特殊字符）； 以中文句号进行分割，分别以整句、分词的形式输出到 json 文件中。 处理结果：
1 2 3 4 // 整句 { "0": "\u8ba1\u7b97\u673a\u7cfb\u7edf\u4e0d\u540c\u4e8e\u4e00\u822c\u7684\u7535\u5b50\u8bbe\u5907\uff0c\u5b83\u662f\u4e00\u4e2a\u7531\u786c\u4ef6\u3001\u8f6f\u4ef6\u7ec4\u6210\u7684\u590d\u6742\u7684\u81ea\u52a8\u5316\u8bbe\u5907" } 1 2 3 4 // 分词 { "0": ["\u8ba1", "\u7b97", "\u673a", "\u7cfb", "\u7edf", "\u4e0d", "\u540c", "\u4e8e", "\u4e00", "\u822c", "\u7684", "\u7535", "\u5b50", "\u8bbe", "\u5907", "\uff0c", "\u5b83", "\u662f", "\u4e00", "\u4e2a", "\u7531", "\u786c", "\u4ef6", "\u3001", "\u8f6f", "\u4ef6", "\u7ec4", "\u6210", "\u7684", "\u590d", "\u6742", "\u7684", "\u81ea", "\u52a8", "\u5316", "\u8bbe", "\u5907"] } 数据集的预标注 以上所有数据处理完后，共得到5632条文本。如果要自己一条条的标注，真就是整一个月啥也别干，所以还是要用比较省力的方式进行标注。我选择用一个在中文语料集上训练过的预训练模型进行第一轮标注，也就是预标注。
我选择了 RaNER命名实体识别-中文-通用领域-large 作为预标注阶段的预训练模型。该模型的标签如下：
实体类型 英文名 公司名 CORP 创作名 CW 其他组织名 GRP 地名 LOC 人名 PER 消费品 PROD 为什么要选择这个模型呢？我当时认为有以下几点可以考虑：
...</p></div><footer class=entry-footer><span title='2024-08-01 00:00:00 +0000 UTC'>August 1, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 《计组KG》课题开发过程（一）" href=http://localhost:1313/posts/dailydev/%E8%AE%A1%E7%BB%84kg%E8%AF%BE%E9%A2%98%E5%BC%80%E5%8F%91%E8%BF%87%E7%A8%8B%E4%B8%80/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=http://localhost:1313/cover/%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BB%E8%8D%AF%E8%AF%B4%E6%98%8E%E4%B9%A6%E5%AE%9E%E4%BD%93%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>《基于预训练模型的医药说明书实体抽取方法研究》笔记</h2></header><div class=entry-content><p>基于预训练模型的部分标签命名实体识别模型 对于输入的药品说明书文本先利用少量样本微调的预训练模型进行实体抽取，如上图所示，“云南白药成分：三七、麝香、草乌等”通过预训练模型识别出 “麝香”和“草乌”两个“成分”类型的实体。由于受标注样本数量限制，预训练模型经少量样本微调后，其召回率不高，如例子中“云南白药”“三七”两个实体未能识别，但把预训练模型识别出的部分实体及该实体的类型标签作为一种提示信息输入到后面的部分标签模型，这将有助于部分标签模型进行实体抽取任务。
部分标签模型采用平面格结构对输入文本及预训练语言模型识别的部分实体进行整合，整合信息包括字符或实体词 token、标签 tag、头部位置标记 head 和尾部位置标记 tail。部分标签模型使用 Transformer 建模平面格结构的编码输入，通过 Transformer 的自注意力机制使字符可以与潜在实体词直接交互。最后将 Transformer 提取特征表示，输入到 CRF层预测实体标签。
训练策略 对整体模型训练分两阶段：
利用少量完全标注的语料对预训练模型进行微调，再对这些完全标注语料采用标注样本实体掩盖（mask）策略进行样本数据增广，使用增广后的样本数据集对部分标签模型进行训练； 预训练模型和部分标签模型进行联合训练。 对比实验 尽管 BERT- BILSTMCRF 模型在医疗、政务等领域的命名实体识别得到广泛应用 ，但由于采用 LSTM 神经网络提取字符特征，其效果明显低于 FLAT 和本文模型，而 FLAT、 MECT4CNER 以及本文模型都采用 Transformer 网络提取特征。
MECT4CNER 是在 FLAT 的基础上结合汉字结构信息与词典信息设计的模型，但本次实验表明 MECT4CNER 应用于药品说明书命名实体识别时， 汉字结构信息未能对提高模型性能带来更多增益， 反而降低了召回率，使得 F1 值比 FLAT 模型更低。 本文模型对 FLAT 模型所提出的平面格结构进行了扩展，增加的标签信息能对提升模型性能带来增益， 从而 F1值取得了较优的效果。
FLAT模型是为中文命名实体识别设计的，它将复杂的字词格结构转换为平坦的结构。每个跨度对应于原始格中的一个字符或潜在词及其位置。FLAT利用Transformer的强大功能和精心设计的位置编码，能够充分利用格信息，并具有出色的并行能力。实验表明，FLAT在性能和效率上都优于其他基于词典的模型。
MECT4CNER模型结合了字词格和部首流，不仅具有FLAT的词边界和语义学习能力，还增加了汉字部首的结构信息。通过融合汉字的结构特征，MECT能够更好地捕捉汉字的语义信息，从而提高中文NER的性能。</p></div><footer class=entry-footer><span title='2024-07-28 00:00:00 +0000 UTC'>July 28, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 《基于预训练模型的医药说明书实体抽取方法研究》笔记" href=http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BB%E8%8D%AF%E8%AF%B4%E6%98%8E%E4%B9%A6%E5%AE%9E%E4%BD%93%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6_%E9%99%88%E4%BB%B2%E6%B0%B8/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>7.22-7.28：一周总结</h2></header><div class=entry-content><p>本周任务 本周主要还是做计组知识图谱课题。以下为本周具体进度：
把数据集分割为了训练集、验证集、测试集，对训练集以随机抽样进行进一步分割，以减小工作量 第一次标注采用预标注，之后用词典不断迭代标注 完成了第一份训练集1000条和验证集、测试集的标注，总共2000余条语料，用新标签PCC 标注 用 Adaseq 训练模型，但是遇到了诸多问题，尚未解决 课题下周改进 PCC 标签过于笼统，下周对标签进一步细化，并去除少部分无用标签 弃用 Adaseq ，国产的玩意是真不如 Transformers 好用 找一个新的预训练模型进行训练（其实已经找到了） 多模型对比训练 以上修改标签预计最少3天，用 Transformers 训练最少2天，多模型能做多少算多少
本周反思 每一步的数据尽可能的量化并记录，方便日后发论文 对课题各个工作部分理解浅显，应当在阅读部分文献后再进行</p></div><footer class=entry-footer><span title='2024-07-28 00:00:00 +0000 UTC'>July 28, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 7.22-7.28：一周总结" href=http://localhost:1313/posts/weeklyworking/2024_07_22-2024_07_28/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=http://localhost:1313/cover/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>《中文命名实体识别研究综述》笔记</h2></header><div class=entry-content><p>中文NER的难点 词边界模糊。中文没有像英文等语言一样使用空格或其他分隔符来表示词边界，这种特点导致中文命名实体识别面临着边界歧义和识别困难的问题。例如，“计算机科学与技术系”中“计算机科学与技术”是一个 复合词，边界不明确。 语义多样化。中文存在大量多义词，一个词汇可能会被用于不同的上下文中表示不同的含义。 形态特征模糊。在英语中，一些指定类型的实体的第一个字母通常是大写的，例如指定人员或地点的名称。这种信息是识别一些命名实体的位置和边界的明确特征。在中文命名实体识别中缺乏汉语形态的显式特征，增加了识别的难度。 中文语料库内容较少。命名实体识别需要大量 的标注数据来训练模型，但中文标注数据数量及质量有限，导致命名实体识别模型的训练更为困难。 CNER数据集 CNER标注方式、性能指标 BIO标注、 BMES标注以及BIOSE标注方案。
性能指标和NER一样，Precision、Recall、Accuracy、F1-score 是常用指标。
近几年的CNER模型构成 嵌入层 基于字符的模型 基于字符的模型将单词表示为字符序列的方法，它通过输入文本的字符级别表示，不需要明确的词边界信息，可以更好地处理CNER中的边界模糊问题。为解决相邻字符之间强联系的问题，Zhang 等人[40] 提出一种新的动态嵌入方法，该方法使用注意力机制来 组合嵌入层中的字符和单词向量特征。基于字符的模型存在不能携带语义信息、难以处理歧义词的缺点[42] 。
基于词的模型 基于词的模型是将中文数据集的文本以词语的形 式作为输入，借助分词系统[43] 对数据集进行分词。基于词的模型可以捕捉到词与词之间的语义关系，在处理一些长词汇的实体时具有良好的效果。基于词的模型存在分词错误和在处理不规则的词以及新词时比较困难的缺点。Ma等人[45] 使用双向 LSTM、CNN 和 CRF 的组合，提出一种 中性网络结构，自动从单词和字符级别的表示中获益， 实现了端到端的 NER。
混合模型 混合模型是将基于字符的模型和基于词的模型结合起来，由于基于字符的模型存在字与字之间语义提取缺失问题，基于词的模型存在分词错误的问题，同时将字符和词作为嵌入表示可以使模型具有较好的鲁棒性和识别精度。
基于 Transformer 的双向编码（bidirectional encoder representations from Transformer，BERT） 模型[51] 是中文命名实体识别中最常用的预训练模型， BERT模型可以考虑整个输入句子的上下文信息，有助于提高模型对命名实体的理解和识别准确性。对于给定的字符，BERT将其字符位置嵌入、句子位置嵌入和字符嵌入作为输入连接起来，然后使用掩码语言模型[52] 对输入句子进行深度双向表示预训练，以获得强大的上下文字符嵌入。
编码层 编码层主要是将嵌入层输入的文本转换成一个高 维的特征向量，方便后续的分类器对文本进行分类。
卷积神经网络 CNN最初是为计算 机视觉研究开发的，但它已被证明可以有效地捕获具有卷积运算的 n-gram 的信息语义特征[61] 。
循环神经网络 Quyang 等人[67] 提出一种用于 CNER 的深度学习模型，该模型采用双向 RNN-CRF 架构，使用连接的 n -gram 字符表示来捕获丰富的上下文信息。Dong 等人[37] 将双向 LSTM-CRF 神经网络用于 CNER，该网络同时利用字符级和部首级表示，是第一个研究 BiLSTM-CRF 架构中的中文部首级表示，并且在没有精心设计的功能的情况下获得更好的性能。
...</p></div><footer class=entry-footer><span title='2024-07-27 00:00:00 +0000 UTC'>July 27, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 《中文命名实体识别研究综述》笔记" href=http://localhost:1313/posts/papernotes/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E8%B5%B5%E7%BB%A7%E8%B4%B5/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 8: Attention</h2></header><div class=entry-content><p>Sequence-to-sequence with attention Attention: in equations We have encoder hidden states $h_1,...,h_N \in \R^h$
On timestep $t$​ , we have decoder hidden state $s_t \in \R^h$
We get the attention scores $e^t$ for this step: $$ e^t=[s^T_th_1,...,s^T_th_N] \in \R^N $$ We take softmax to get the attention distribution $\alpha^t$​ for this step (this is a probability distribution and sums to 1) $$ \alpha^t=softmax(e^t) \in \R^N $$ We use $\alpha^t$ to take a weighted sum of the encoder hidden states to get the attention output $a_i$ ...</p></div><footer class=entry-footer><span title='2024-07-27 00:00:00 +0000 UTC'>July 27, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 8: Attention" href=http://localhost:1313/posts/cs224n/lesson_8/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 7: Machine Translation and Sequence to Sequence</h2></header><div class=entry-content><p>Machine Translation Machine Translation is the task of translating a sentence $x$ from one language to a sentence $y$​ in another language.
Simple History:
1990s-2010s: Statistical Machine Translation After 2014: Neural Machine Translation Sequence to Sequence Model The sequence-to-sequence model is an example of a Conditional Language Model
Language Model because the decoder is predicting the next word of the target sentence $y$ Conditional because its predictions are also conditioned on the source sentence $x$ ...</p></div><footer class=entry-footer><span title='2024-07-13 00:00:00 +0000 UTC'>July 13, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 7: Machine Translation and Sequence to Sequence" href=http://localhost:1313/posts/cs224n/lesson_7/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RNN速成（二）</h2></header><div class=entry-content><p>LSTM 长短期记忆（Long short-term memory, LSTM）是一种特殊的 RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的 RNN，LSTM 能够在更长的序列中有更好的表现。
门结构中的激活函数 门结构中包含着 sigmoid 激活函数。sigmoid 激活函数与 tanh 函数类似，不同之处在于 sigmoid 是把值压缩到 01 之间而不是 -11 之间。这样的设置有助于更新或忘记信息，因为任何数乘以 0 都得 0，这部分信息就会剔除掉。同样的，任何数乘以 1 都得到它本身，这部分信息就会完美地保存下来。这样网络就能了解哪些数据是需要遗忘，哪些数据是需要保存。这也代表着门结构最后计算得到的是一个概率。
遗忘门 遗忘门的功能是决定应丢弃或保留哪些信息。来自前一个隐藏状态的信息和当前输入的信息同时传递到 sigmoid 函数中去，输出值介于 0 和 1 之间，越接近 0 意味着越应该丢弃，越接近 1 意味着越应该保留。
输入门 输入门用于更新细胞状态。首先将前一层隐藏状态的信息和当前输入的信息传递到 sigmoid 函数中去。将值调整到 0~1 之间来决定要更新哪些信息。0 表示不重要，1 表示重要。
细胞状态 下一步，就是计算细胞状态。首先前一层的细胞状态与遗忘向量逐点相乘。如果它乘以接近 0 的值，意味着在新的细胞状态中，这些信息是需要丢弃掉的。然后再将该值与输入门的输出值逐点相加，将神经网络发现的新信息更新到细胞状态中去。至此，就得到了更新后的细胞状态。
输出门 输出门用来确定下一个隐藏状态的值，隐藏状态包含了先前输入的信息。首先，我们将前一个隐藏状态和当前输入传递到 sigmoid 函数中，然后将新得到的细胞状态传递给 tanh 函数。
最后将 tanh 的输出与 sigmoid 的输出相乘，以确定隐藏状态应携带的信息。再将隐藏状态作为当前细胞的输出，把新的细胞状态和新的隐藏状态传递到下一个时间步长中去。
数学计算方式 ...</p></div><footer class=entry-footer><span title='2024-07-12 00:00:00 +0000 UTC'>July 12, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to RNN速成（二）" href=http://localhost:1313/posts/nlp/rnn%E9%80%9F%E6%88%90%E4%BA%8C/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 6: Long Short-Term Memory RNNs</h2></header><div class=entry-content><p>Training an RNN Language Model Get a big corpus of text which is a sequence of words $x^{(1)},...,x^{(T)}$ Feed into RNN-LM; compute output distribution $\hat y ^{(t)}$ for every timestep $t$​ Backpropagation for RNNs Problems with Vanishing and Exploding Gradients Vanishing gradient intuition Why is vanishing gradient a problem? 来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多 因此，模型权重只会根据近期效应而不是长期效应进行更新 If gradient is small, the model can’t learn this dependency. So, the model is unable to predict similar long distance dependencies at test time.
...</p></div><footer class=entry-footer><span title='2024-07-11 00:00:00 +0000 UTC'>July 11, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 6: Long Short-Term Memory RNNs" href=http://localhost:1313/posts/cs224n/lesson_6/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>RNN速成（一）</h2></header><div class=entry-content><p>基本概念 循环神经网络 (RNN) 是一种使用序列数据或时序数据的人工神经网络。其最大特点是网络中存在着环，使得信息能在网络中进行循环，实现对序列信息的存储和处理。
循环神经网络 (RNN) 的另一个显著特征是它们在每个网络层中共享参数。 虽然前馈网络的每个节点都有不同的权重，但循环神经网络在每个网络层都共享相同的权重参数。
网络结构 RNN 不是刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。
同时，RNN 还能按时间序列展开循环为如下形式：
以上架构不仅揭示了 RNN 的实质：上一个时刻的网络状态将会作用于（影响）到下一个时刻的网络状态，还表明 RNN 和序列数据密切相关。同时，RNN 要求每一个时刻都有一个输入，但是不一定每个时刻都需要有输出。
如上图所示，隐含层的计算公式如下： $$ s_t=f \ (U_{x_t}+W_{s_{t-1}}) $$ 其中， $f$ 为激活函数。
训练方法 RNN 利用随时间推移的反向传播 (BPTT) 算法来确定梯度，这与传统的反向传播略有不同，因为它特定于序列数据。 BPTT 的原理与传统的反向传播相同，模型通过计算输出层与输入层之间的误差来训练自身。 这些计算帮助我们适当地调整和拟合模型的参数。 BPTT 与传统方法的不同之处在于，BPTT 会在每个时间步长对误差求和。
通过这个过程，RNN 往往会产生两个问题，即梯度爆炸和梯度消失。 这些问题由梯度的大小定义，也就是损失函数沿着错误曲线的斜率。 如果梯度过小，它会更新权重参数，让梯度继续变小，直到变得可以忽略，即为 0。 发生这种情况时，算法就不再学习。 如果梯度过大，就会发生梯度爆炸，这会导致模型不稳定。 在这种情况下，模型权重会变得太大，并最终被表示为 NaN。
RNN的变体 这里只是一个简介，详情见 《RNN速成（二）》
LSTM 这是一种比较流行的 RNN 架构，由 Sepp Hochreiter 和 Juergen Schmidhuber 提出，用于解决梯度消失问题。LSTM 在神经网络的隐藏层中包含一些“元胞”(cell)，共有三个门：一个输入门、一个输出门和一个遗忘门。 这些门控制着预测网络中的输出所需信息的流动。
GRU 这种 RNN 变体类似于 LSTM，因为它也旨在解决 RNN 模型的短期记忆问题。 但它不使用“元胞状态”来调节信息，而是使用隐藏状态；它不使用三个门，而是两个：一个重置门和一个更新门。 类似于 LSTM 中的门，重置门和更新门控制要保留哪些信息以及保留多少信息。
...</p></div><footer class=entry-footer><span title='2024-07-06 00:00:00 +0000 UTC'>July 6, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to RNN速成（一）" href=http://localhost:1313/posts/nlp/rnn/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 5: Language Models and Recurrent Neural Network</h2></header><div class=entry-content><p>Basic Tricks on NN L2 Regularization A full loss function includes regularization over all parameters $\theta$ , e.g., L2 regularization: $$ J(\theta)=f(x)+\lambda \sum_k \theta^2_k $$ Regularization produces models that generalize well when we have a “big” model.
Dropout Training time: at each instance of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0 Test time: halve the model weights (now twice as many) This prevents feature co-adaptation Can be thought of as a form of model bagging (i.e., like an ensemble model) Nowadays usually thought of as strong, feature-dependent regularizer Vectorization ...</p></div><footer class=entry-footer><span title='2024-07-05 00:00:00 +0000 UTC'>July 5, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 5: Language Models and Recurrent Neural Network" href=http://localhost:1313/posts/cs224n/lesson_5/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Named Entity Recognition 相关概念与技术</h2></header><div class=entry-content><p>基本概念 实体：通常指文本中具有特定意义或指代性强的词或短语，如人名、地名、机构名等。 边界识别：确定实体在文本中的起始和结束位置。 分类：将识别出的实体归类到预定义的类别，如人名、地名、组织名、时间表达式、数量、货币值、百分比等。 序列标注：NER任务中常用的一种方法，将文本中的每个词标注为实体的一部分或非实体。 特征提取：从文本中提取有助于实体识别的特征，如词性、上下文信息等。 评估指标：用于衡量NER系统性能的指标，常见的有精确率（Precision）、召回率（Recall）和F1分数。 分类命名方法 BIO：这是最基本的标注方法，其中"B"代表实体的开始（Begin），“I"代表实体的内部（Inside），而"O"代表非实体（Outside）。 BIOES：这种方法在BIO的基础上增加了"E"表示实体的结束（End），和"S"表示单独的实体（Single）。 BMES：这种方法使用"B"表示实体的开始，“M"表示实体的中间（Middle），“E"表示实体的结束，“S"表示单个字符实体。 NER 的一般过程 数据准备：收集并标注一个包含目标实体的数据集。这个数据集应该包含足够的示例，以便模型能够学习如何识别和分类实体。 选择模型架构：选择一个适合任务的模型架构，如基于LSTM的序列模型或者是基于Transformers的预训练模型。 特征工程：根据需要，进行特征工程，提取有助于实体识别的特征，例如词性标注、上下文嵌入等。 模型训练：使用标注好的数据集来训练模型。这通常包括定义损失函数、选择优化器、设置学习率和训练周期等。 评估与优化：在独立的验证集上评估模型性能，使用诸如精确率、召回率和F1分数等指标，并根据结果进行模型调优。 一个小例子 以当前计组KG为例。
数据集 数据格式见 transformers/examples/pytorch/token-classification at main · huggingface/transformers (github.com)
数据来源：
通过百度百科爬虫 BaiduSpider/BaiduSpider: BaiduSpider，一个爬取百度搜索结果的爬虫，目前支持百度网页搜索，百度图片搜索，百度知道搜索，百度视频搜索，百度资讯搜索，百度文库搜索，百度经验搜索和百度百科搜索。 (github.com) 爬取计算机组合原理的相关术语 从计组教材中提取出文本 数据处理：
去掉无用字符、HTML标签等无关信息 使用Tokenizer将文本数据分解成Token 根据Token创建词汇表，每个唯一的Token对应一个唯一的索引 将文本中的Token转换为对应的索引值，以便模型能够处理 添加位置编码，以便模型能够理解Token在序列中的位置 数据集划分：
将数据集划分为训练集、验证集和测试集 格式化数据集：
使数据集符合transformer库NER任务模型的输入格式 模型 选用中文NER预训练模型：ckiplab/albert-base-chinese-ner · Hugging Face
选用peft框架微调：peft/examples/token_classification at main · huggingface/peft (github.com)
训练和测试 一般过程，略。
应用 可以应用在语料中识别实体了。
关系抽取 NER的下一步就是关系抽取了</p></div><footer class=entry-footer><span title='2024-07-05 00:00:00 +0000 UTC'>July 5, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Named Entity Recognition 相关概念与技术" href=http://localhost:1313/posts/nlp/ner/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 4: Dependency parsing</h2></header><div class=entry-content><p>Two views of linguistic structure Context-free grammars (CFGs) Phrase structure organizes words into nested constituents.
Dependency structure Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.
modify：修饰词，attach to：连接词
Why do we need sentence structure? Humans communicate complex ideas by composing words together into bigger units to convey complex meanings. Listeners need to work out what modifies [attaches to] what A model needs to understand sentence structure in order to be able to interpret language correctly Linguistic Ambiguities Prepositional phrase attachment ambiguity 介词短语附着歧义 Coordination scope ambiguity 对等范围歧义 Adjectival/Adverbial Modifier Ambiguity 形容词修饰语歧义 Verb Phrase (VP) attachment ambiguity 动词短语依存歧义 Dependency paths identify semantic relations 依赖路径识别语义关系 help extract semantic interpretation.
...</p></div><footer class=entry-footer><span title='2024-07-04 00:00:00 +0000 UTC'>July 4, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 4: Dependency parsing" href=http://localhost:1313/posts/cs224n/lesson_4/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Continuous Bag of Words</h2></header><div class=entry-content><p>CBOW CBOW是 continuous bag of words 的缩写，中文译为“连续词袋模型”。它和 skip-gram 的区别在于其通过上下文去预测中心词。
模型结构如下。
在 Projection Layer 中，会将上下文的所有词向量进行累加： $$ X_w=\sum^{2c}_{i=1}v(Context(w)_i) \in \R^m $$ 在 Output Layer 中，使用到的函数是 Hierarchical softmax 层级softmax 。
Hierarchical softmax 是利用哈夫曼树结构来减少计算量的方式。它将 word 按词频作为结点权值来构建哈夫曼树。
哈夫曼树是一种二叉树，实际上就是在不断的做二分类。
具体公式推导见Hierarchical Softmax（层次Softmax） - 知乎 (zhihu.com)。</p></div><footer class=entry-footer><span title='2024-06-29 00:00:00 +0000 UTC'>June 29, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Continuous Bag of Words" href=http://localhost:1313/posts/nlp/word2vec-variants-continuous_bag_of_words/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Skip-gram Model</h2></header><div class=entry-content><p>skip-gram 具体过程与介绍见 cs224n 系列
从 one-hot 到 lookup-table 每一个单词经过 one-hot 编码后都会得到一个 $(V,1)$ 维的词向量， $V$ 个词就是 $(V,V)$ 维方阵。这样的话，矩阵的计算量会极大，所以要降维以减小计算量。
单词在 one-hot 编码表的位置是由关系的，如果单词出现的位置column = 3，那么对应的就会选中权重矩阵的Index = 3。由此可以通过这个映射关系进行以下操作：将one-hot编码后的词向量，通过一个神经网络的隐藏层，映射到一个低纬度的空间，并且没有任何激活函数。最后得到的东西就叫做 lookup-table，或叫 word embedding 词嵌入，维度 $(V,d)$。
数学原理 一个单词的极大似然估计概率计算公式在取负号后变为： $$ J(\theta)=-\frac{1}{T}\sum^T_{t=1}\sum_{\substack{-m \le j \le m \\ j\neq0}}logP(w_{t+j} \ | \ w_t; \ \theta) $$ 代入softmax计算公式后得到目标函数或叫损失函数： $$ logP(w_o \ | \ w_c)=u_o^Tv_c-log \Bigg( \sum_{i \in V}exp(u_i^Tv_c) \Bigg) $$ 用梯度下降更新参数： $$ \frac{\partial logP(w_o \ | \ w_c)}{\partial v_c} = u_o-\sum_{j \in V}P(w_j \ | \ w_c)u_j $$ 训练结束后，对于词典中的任一索引为 $i$ 的词，我们均得到该词作为中心词和背景词的两组词向量 $vi$ , $u_i$ .
...</p></div><footer class=entry-footer><span title='2024-06-28 00:00:00 +0000 UTC'>June 28, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Skip-gram Model" href=http://localhost:1313/posts/nlp/word2vec-variants-skip-gram/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Lecture 1: Introduction and Word Vectors</h2></header><div class=entry-content><p>Meaning Definition: meaning 语义
the idea that is represented by a word, phrase, etc. the idea that a person wants to express by using words, signs, etc. the idea that is expressed in a work of writing, art, etc. WordNet Common NLP solution: Use, e.g., WordNet, a thesaurus containing lists of synonym (同义词) sets and hypernyms (上位词) (“is a” relationships).
Problems with resources like WordNet Great as a resource but missing nuance
...</p></div><footer class=entry-footer><span title='2024-06-27 00:00:00 +0000 UTC'>June 27, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 1: Introduction and Word Vectors" href=http://localhost:1313/posts/cs224n/lesson_1/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>GoogleNet V1</h2></header><div class=entry-content><p>GoogleNet V1 Abstract 本文作者在ImageNet大规模视觉识别挑战赛2014（ILSVRC14）上提出了一种代号为Inception的深度卷积神经网络结构，并在分类和检测上取得了新的最好结果。这个架构的主要特点是提高了网络内部计算资源的利用率。通过精心的手工设计，在增加了网络深度和广度的同时保持了计算预算不变。为了优化质量，架构的设计以赫布理论和多尺度处理直觉为基础。作者在ILSVRC14提交中应用的一个特例被称为GoogLeNet，一个22层的深度网络，其质量在分类和检测的背景下进行了评估。
1. Introduction 过去三年中，由于深度学习和卷积网络的发展[10]，目标分类和检测能力得到了显著提高。一个令人鼓舞的消息是，大部分的进步不仅仅是更强大硬件、更大数据集、更大模型的结果，而主要是新的想法、算法和网络结构改进的结果。例如，ILSVRC 2014竞赛中最靠前的输入除了用于检测目的的分类数据集之外，没有使用新的数据资源。本文在ILSVRC 2014中的GoogLeNet提交实际使用的参数只有两年前Krizhevsky等人[9]获胜结构参数的1/12，而结果明显更准确。在目标检测前沿，最大的收获不是来自于越来越大的深度网络的简单应用，而是来自于深度架构和经典计算机视觉的协同，像Girshick等人[6]的R-CNN算法那样。
另一个显著因素是随着移动和嵌入式设备的推动，算法的效率很重要——尤其是它们的电力和内存使用。值得注意的是，正是包含了这个因素的考虑才得出了本文中呈现的深度架构设计，而不是单纯的为了提高准确率。对于大多数实验来说，模型被设计为在一次推断中保持15亿乘加的计算预算，所以最终它们不是单纯的学术好奇心，而是能在现实世界中应用，甚至是以合理的代价在大型数据集上使用。
本文作者考虑到在终端设备上的应用所以才提出了Inception，这也影响了后续的Xception和MobileNet。（猜测）
在本文中将关注一个高效的计算机视觉深度神经网络架构，代号为Inception，它的名字来自于Lin等人[12]网络论文中的Network与著名的“we need to go deeper”网络梗图[1]的结合。在本文的案例中，单词“deep”用在两个不同的含义中：首先，在某种意义上，以“Inception module”的形式引入了一种新层次的组织方式，在更直接的意义上增加了网络的深度。一般来说，可以把Inception模型看作论文[12]的逻辑顶点同时从Arora等人[2]的理论工作中受到了鼓舞和引导。这种架构的好处在ILSVRC 2014分类和检测挑战赛中通过实验得到了验证，它明显优于目前的最好水平。
2. Related Work 从LeNet-5 [10]开始，卷积神经网络（CNN）通常有一个标准结构——堆叠的卷积层（后面可以选择有对比归一化和最大池化）后面是一个或更多的全连接层。这个基本设计的变种在图像分类论文中流行，并且目前为止在MNIST，CIFAR和更著名的ImageNet分类挑战赛中[9, 21]的已经取得了最佳结果。对于更大的数据集例如ImageNet来说，最近的趋势是增加层的数目[12]和层的大小[21, 14]，同时使用dropout[7]来解决过拟合问题。
尽管担心最大池化层会引起准确空间信息的损失，但与[9]相同的卷积网络结构也已经成功的应用于定位[9, 14]，目标检测[6, 14, 18, 5]和行人姿态估计[19]。
受灵长类视觉皮层神经科学模型的启发，Serre等人[15]使用了一系列固定的不同大小的Gabor滤波器来处理多尺度。本文使用一个了类似的策略。然而，与[15]的固定的2层深度模型相反，Inception结构中所有的滤波器是学习到的。此外，Inception层重复了很多次，在GoogLeNet模型中得到了一个22层的深度模型。
Network-in-Network是Lin等人[12]为了增加神经网络表现能力而提出的一种方法。在他们的模型中，网络中添加了额外的1 × 1卷积层，增加了网络的深度。本文的架构中大量的使用了这个方法。但是，在本文的设置中，1 × 1卷积有两个目的：最关键的是，它们主要是用来作为降维模块来移除卷积瓶颈，否则将会限制网络的大小。这不仅允许了深度的增加，而且允许网络的宽度增加但没有明显的性能损失。
1×1 的卷积层的作用：
在相同尺寸的感受野中叠加更多的卷积，能提取到更丰富的特征。
使用1x1卷积进行降维，降低了计算复杂度。
1×1 卷积没有识别高和宽维度上相邻元素构成的模式的功能。实际上，1×1 卷积的作用主要发生在 Channel 维上。值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。假设我们将 Channel 维当作特征维，将高和宽维度上的元素当成数据样本，那么 1×1 卷积层的作用与全连接层等价。
最后，目前最好的目标检测是Girshick等人[6]的基于区域的卷积神经网络（R-CNN）方法。R-CNN将整个检测问题分解为两个子问题：利用低层次的信号例如颜色，纹理以跨类别的方式来产生目标位置候选区域，然后用CNN分类器来识别那些位置上的对象类别。这样Two-Stage的方法利用了低层特征分割边界框的准确性，也利用了目前的CNN非常强大的分类能力。我们在我们的检测提交中采用了类似的方式，但探索增强这两个阶段，例如对于更高的目标边界框召回使用多盒[5]预测，并融合了更好的边界框候选区域分类方法。
3. Motivation and High Level Considerations 提高深度神经网络性能最直接的方式是增加它们的尺寸。这不仅包括增加深度——网络层次的数目——也包括它的宽度：每一层的单元数目。这是一种训练更高质量模型容易且安全的方法，尤其是在可获得大量标注的训练数据的情况下。但是这个简单方案有两个主要的缺点。
更大的尺寸通常意味着更多的参数，这会使增大的网络更容易过拟合，尤其是在训练集的标注样本有限的情况下。这是一个主要的瓶颈，因为要获得强标注数据集费时费力且代价昂贵，经常需要专家评委在各种细粒度的视觉类别进行区分，例如图1中显示的ImageNet中的类别（甚至是1000类ILSVRC的子集）。
图1: ILSVRC 2014分类挑战赛的1000类中两个不同的类别。区分这些类别需要领域知识。
均匀增加网络尺寸的另一个缺点是计算资源使用的显著增加。例如，在一个深度视觉网络中，如果两个卷积层相连，它们的滤波器数目的任何均匀增加都会引起计算量以平方增加。如果增加的能力使用时效率低下（例如，如果大多数权重结束时接近于0），那么会浪费大量的计算能力。由于计算预算总是有限的，计算资源的有效分布更偏向于尺寸无差别的增加，即使主要目标是增加性能的质量。
解决这两个问题的一个基本的方式就是引入稀疏性并将全连接层替换为稀疏的全连接层，甚至是卷积层。除了模仿生物系统之外，由于Arora等人[2]的开创性工作，这也具有更坚固的理论基础优势。他们的主要成果说明如果数据集的概率分布可以通过一个大型稀疏的深度神经网络表示，则最优的网络拓扑结构可以通过分析前一层激活的相关性统计和聚类高度相关的神经元来一层层的构建。虽然严格的数学证明需要在很强的条件下，但事实上这个声明与著名的赫布理论产生共鸣：神经元一起激发，一起连接。实践表明，基础概念甚至适用于不严格的条件下。
遗憾的是，当碰到在非均匀的稀疏数据结构上进行数值计算时，以现在的计算架构效率会非常低下。即使算法运算的数量减少100倍，查询和缓存丢失上的开销仍占主导地位：切换到稀疏矩阵可能是不可行的。随着稳定提升和高度调整的数值库的应用，差距仍在进一步扩大，数值库要求极度快速密集的矩阵乘法，利用底层的CPU或GPU硬件[16, 9]的微小细节。非均匀的稀疏模型也要求更多的复杂工程和计算基础结构。目前大多数面向视觉的机器学习系统通过采用卷积的优点来利用空域的稀疏性。然而，卷积被实现为对上一层块的密集连接的集合。为了打破对称性，提高学习水平，从论文[11]开始，ConvNets习惯上在特征维度使用随机的稀疏连接表，然而为了进一步优化并行计算，论文[9]中趋向于变回全连接。目前最新的计算机视觉架构有统一的结构。更多的滤波器和更大的批大小要求密集计算的有效使用。
这提出了下一个中间步骤是否有希望的问题：一个架构能利用滤波器水平的稀疏性，正如理论所认为的那样，但能通过利用密集矩阵计算来利用目前的硬件。稀疏矩阵乘法的大量文献（例如[3]）认为对于稀疏矩阵乘法，将稀疏矩阵聚类为相对密集的子矩阵会有更佳的性能。在不久的将来会利用类似的方法来进行非均匀深度学习架构的自动构建，这样的想法似乎并不牵强。
Inception架构开始作为案例研究，用于评估一个复杂网络拓扑构建算法的假设输出，该算法试图近似[2]中所示的视觉网络的稀疏结构，并通过密集的、容易获得的组件来覆盖假设结果。尽管是一个非常投机的事情，但与基于[12]的参考网络相比，早期可以观测到适度的收益。随着一点点调整加宽差距，作为[6]和[5]的基础网络，Inception被证明在定位上下文和目标检测中尤其有用。有趣的是，虽然大多数最初的架构选择已被质疑并分离开进行全面测试，但结果证明它们是局部最优的。然而必须谨慎：尽管Inception架构在计算机上领域取得成功，但这是否可以归因于构建其架构的指导原则仍是有疑问的。确保这一点将需要更彻底的分析和验证。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to GoogleNet V1" href=http://localhost:1313/posts/classicpapertranslation/googlenetv1/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>GoogleNet V2</h2></header><div class=entry-content><p>GoogLeNet V2 Abstract 训练深度神经网络的复杂性在于，每层输入的分布在训练过程中会发生变化，因为前面的层的参数会发生变化。通过要求较低的学习率和仔细的参数初始化减慢了训练，并且使具有饱和非线性的模型训练起来非常困难。本文将这种现象称为内部协变量转移，并通过标准化层输入来解决这个问题。本文力图使标准化成为模型架构的一部分，并为每个训练小批量数据执行标准化。批标准化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化项，在某些情况下不需要Dropout。将批量标准化应用到最先进的图像分类模型上，批标准化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批标准化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了4.9％ top-5的验证误差（和4.8％测试误差），超过了人类评估者的准确性。
1. Introduction 随机梯度下降（SGD）已经被证明是训练深度网络的有效方式，并且已经使用诸如动量（Sutskever等，2013）和Adagrad（Duchi等人，2011）等SGD变种取得了最先进的性能。SGD优化网络参数$\Theta$，以最小化损失 $$ \Theta = arg \ min_{\Theta}\frac{1}{N} \sum_{i=1}^{N}ℓ(x_i,\Theta) $$ $x_1...N$是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为$m$的小批量数据$x_1...m$。通过计算$\frac{1}{m}\sum^m_{i=1}\frac{∂ℓ(xi,Θ)}{∂Θ}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而改善。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算$m$次效率更高。
虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。
层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，据说会经历协变量转移（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。然而，协变量转移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。考虑网络计算 $$ ℓ=F_2(F_1(u,Θ_1),Θ_2) $$ $F_1$和$F_2$是任意变换，学习参数$Θ_1$，$Θ_2$以便最小化损失$ℓ$。学习$Θ_2$可以看作输入$x=F_1(u,Θ_1)$送入到子网络 $$ ℓ=F_2(x,Θ_2) $$ 例如，梯度下降步骤 $$ Θ_2←Θ_2−\frac{α}{m}\sum_{i=1}^{m}\frac{∂F_2(x_i,Θ_2)}{∂Θ_2} $$ （对于批大小$m$和学习率$α$）与输入为$x$的单独网络$F_2$完全等价。因此，输入分布特性使训练更有效——例如训练数据和测试数据之间有相同的分布——也适用于训练子网络。因此$x$的分布在时间上保持固定是有利的。然后，$Θ_2$不必重新调整来补偿x分布的变化。
子网络输入的固定分布对于子网络外的层也有积极的影响。考虑一个激活函数为$g(x)=\frac{1}{1+exp(−x)}$的层，$u$是层输入，权重矩阵$W$和偏置向量$b$是要学习的层参数，$g(x)=\frac{1}{1+exp(−x)}$。随着$|x|$的增加，$g′(x)$趋向于0。这意味着对于$x=Wu+b$的所有维度，除了那些具有小的绝对值之外，流向$u$的梯度将会消失，模型将缓慢的进行训练。然而，由于$x$受$W,b$和下面所有层的参数的影响，训练期间那些参数的改变可能会将$x$的许多维度移动到非线性的饱和状态并减慢收敛。这个影响随着网络深度的增加而放大。在实践中，饱和问题和由此产生的梯度消失通常通过使用修正线性单元(Nair & Hinton, 2010)$ ReLU(x)=max(x,0)$，仔细的初始化(Bengio & Glorot, 2010; Saxe et al., 2013)和小的学习率来解决。然而，如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。
本文把训练过程中深度网络内部结点的分布变化称为内部协变量转移，消除它可以保证更快的训练。我们提出了一种新的机制，我们称为为批标准化，它是减少内部协变量转移的一个步骤，这样做可以显著加速深度神经网络的训练。它通过标准化步骤来实现，标准化步骤修正了层输入的均值和方差。批标准化减少了梯度对参数或它们的初始值尺度上的依赖，对通过网络的梯度流动有益。这允许我们使用更高的学习率而没有发散的风险。此外，批标准化使模型正则化并减少了对Dropout(Srivastava et al., 2014)的需求。最后，批标准化通过阻止网络陷入饱和模式让使用饱和非线性成为可能。
在4.2小节，本文将批标准化应用到性能最好的ImageNet分类网络上，并且表明可以使用仅7％的训练步骤来匹配其性能，并且可以进一步超过其准确性一大截。通过使用批标准化训练的网络的集合，最终取得了top-5错误率，其改进了ImageNet分类上已知的最佳结果。
2. Towards Reducing Internal Covariate Shift 由于训练过程中网络参数的变化，本文将内部协变量转移定义为网络激活分布的变化。为了改善训练，以寻求减少内部协变量转移。随着训练的进行，通过固定层输入$x$的分布，期望提高训练速度。众所周知(LeCun et al., 1998b; Wiesler & Ney, 2011)如果对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，将采取措施实现输入的固定分布，消除内部协变量转移的不良影响。
白化，whitening。
白化的目的是去除输入数据的冗余信息。假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的。而白化的目的就是降低输入的冗余性。
输入数据集$X$，经过白化处理后，新的数据$X'$满足两个性质：
特征之间相关性较低； 所有特征具有相同的方差。 本文考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins & Kavukcuoglu)。然而，如果这些修改分散在优化步骤中，那么梯度下降步骤可能会试图以要求标准化进行更新的方式来更新参数，这会降低梯度下降步骤的影响。例如，考虑一个层，其输入$u$加上学习到的偏置$b$，通过减去在训练集上计算的激活值的均值对结果进行归一化：$\hat x=x−E[x]$，$x=u+b$，$X=x_1...N$是训练集上$x$值的集合，$E[x]=\frac{1}{N}\sum^N_{i=1}x_i$。如果梯度下降步骤忽略了$E[x]$对$b$的依赖，那它将更新$b←b+Δb$，其中$Δb∝−∂ℓ/∂\hat x$。然后$u+(b+Δb)−E[u+(b+Δb)]=u+b−E[u+b]$。因此，结合$b$的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，$b$将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。在最初的实验中已经观察到了这一点，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to GoogleNet V2" href=http://localhost:1313/posts/classicpapertranslation/googlenetv2/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Inception V3</h2></header><div class=entry-content><p>Inception V3 Abstract 对许多任务而言，卷积网络是目前最新的计算机视觉解决方案的核心。从2014年开始，深度卷积网络开始变成主流，在各种基准数据集上都取得了实质性成果。对于大多数任务而言，虽然增加的模型大小和计算成本都趋向于转化为直接的质量收益（只要提供足够的标注数据去训练），但计算效率和低参数计数仍是各种应用场景的限制因素，例如移动视觉和大数据场景。目前，我们正在探索增大网络的方法，目标是通过适当的分解卷积和积极的正则化来尽可能地有效利用增加的计算。我们在ILSVRC 2012分类挑战赛的验证集上评估了我们的方法，结果证明我们的方法超过了目前最先进的方法并取得了实质性收益：对于单一框架评估错误率为：21.2% top-1和5.6% top-5，使用的网络计算代价为每次推断需要进行50亿次乘加运算并使用不到2500万的参数。通过四个模型组合和多次评估，我们报告了3.5% top-5和17.3% top-1的错误率。
1. Introduction 从2012年Krizhevsky等人[9]赢得了ImageNet竞赛[16]起，他们的网络“AlexNet”已经成功了应用到了许多计算机视觉任务中，例如目标检测[5]，分割[12]，行人姿势评估[22]，视频分类[8]，目标跟踪[23]和超分辨率[3]。
这些成功推动了一个新研究领域，这个领域主要专注于寻找更高效运行的卷积神经网络。从2014年开始，通过利用更深更宽的网络，网络架构的质量得到了明显改善。VGGNet[18]和GoogLeNet[20]在2014 ILSVRC [16]分类挑战上取得了类似的高性能。一个有趣的发现是在分类性能上的收益趋向于转换成各种应用领域上的显著质量收益。这意味着深度卷积架构上的架构改进可以用来改善大多数越来越多地依赖于高质量、可学习视觉特征的其它计算机视觉任务的性能。网络质量的改善也导致了卷积网络在新领域的应用，在AlexNet特征不能与手工精心设计的解决方案竞争的情况下，例如，检测时的候选区域生成[4]。
尽管VGGNet[18]具有架构简洁的强有力特性，但它的成本很高：评估网络需要大量的计算。另一方面，GoogLeNet[20]的Inception架构也被设计为在内存和计算预算严格限制的情况下也能表现良好。例如，GoogleNet只使用了500万参数，与其前身AlexNet相比减少了12倍，AlexNet使用了6000万参数。此外，VGGNet使用了比AlexNet大约多3倍的参数。
Inception的计算成本也远低于VGGNet或其更高性能的后继者[6]。这使得可以在大数据场景中[17]，[13]，在大量数据需要以合理成本处理的情况下或在内存或计算能力固有地受限情况下，利用Inception网络变得可行，例如在移动视觉设定中。通过应用针对内存使用的专门解决方案[2]，[15]或通过计算技巧优化某些操作的执行[10]，可以减轻部分这些问题。但是这些方法增加了额外的复杂性。此外，这些方法也可以应用于优化Inception架构，再次扩大效率差距。
然而，Inception架构的复杂性使得更难以对网络进行更改。如果单纯地放大架构，大部分的计算收益可能会立即丢失。此外，[20]并没有提供关于导致GoogLeNet架构的各种设计决策的贡献因素的明确描述。这使得它更难以在适应新用例的同时保持其效率。例如，如果认为有必要增加一些Inception模型的能力，将滤波器组大小的数量加倍的简单变换将导致计算成本和参数数量增加4倍。这在许多实际情况下可能会被证明是禁止或不合理的，尤其是在相关收益适中的情况下。在本文中，我们从描述一些一般原则和优化思想开始，对于以有效的方式扩展卷积网络来说，这被证实是有用的。虽然我们的原则不局限于Inception类型的网络，但是在这种情况下，它们更容易观察，因为Inception类型构建块的通用结构足够灵活，可以自然地合并这些约束。这通过大量使用降维和Inception模块的并行结构来实现，这允许减轻结构变化对邻近组件的影响。但是，对于这样做需要谨慎，因为应该遵守一些指导原则来保持模型的高质量。
2. General Design Principles 这里我们将介绍一些具有卷积网络的、具有各种架构选择的、基于大规模实验的设计原则。在这一点上，以下原则的效用是推测性的，另外将来的实验证据将对于评估其准确性和有效领域是必要的。然而，严重偏移这些原则往往会导致网络质量的恶化，修正检测到的这些偏差状况通常会导致改进的架构。
避免表征瓶颈，尤其是在网络的前面。前馈网络可以由从输入层到分类器或回归器的非循环图表示。这为信息流定义了一个明确的方向，对于分离输入输出的任何切口，可以访问通过切口的信息量。应该避免极端压缩的瓶颈。一般来说，在达到用于着手任务的最终表示之前，表示大小应该从输入到输出缓慢减小。理论上，信息内容不能仅通过表示的维度来评估，因为它丢弃了诸如相关结构的重要因素，而维度仅提供信息内容的粗略估计。 更高维度的表示在网络中更容易局部处理。在卷积网络中增加每个图块的激活允许更多解耦的特征，所产生的网络将训练更快。 空间聚合可以在较低维度嵌入上完成，而不会在表示能力上造成许多或任何损失。例如，在执行更多展开（例如3×3）卷积之前，可以在空间聚合之前减小输入表示的维度，没有预期的严重不利影响。我们假设，如果在空间聚合上下文中使用输出，则相邻单元之间的强相关性会导致维度缩减期间的信息损失少得多。鉴于这些信号应该易于压缩，因此尺寸减小甚至会促进更快的学习。 平衡网络的宽度和深度。通过平衡每个阶段的滤波器数量和网络的深度可以达到网络的最佳性能，增加网络的宽度和深度可以有助于更高质量的网络。然而，如果两者并行增加，则可以达到恒定计算量的最佳改进。因此，计算预算应该在网络的深度和宽度之间以平衡方式进行分配。 虽然这些原则可能是有意义的，但并不是开箱即用的直接使用它们来提高网络质量。我们的想法是仅在不明确的情况下才明智地使用它们。
3. Factorizing Convolutions with Large Filter Size GoogLeNet网络[20]的大部分初始收益来源于大量地使用降维。这可以被视为以计算有效的方式分解卷积的特例。考虑例如1×1卷积层之后接一个3×3卷积层的情况。在视觉网络中，预期相近激活的输出是高度相关的。因此，我们可以预期，它们的激活可以在聚合之前被减少，并且这应该会导致类似的富有表现力的局部表示。
图1。
Mini网络替换5×5卷积。
在这里，我们将在各种设定中探索卷积分解的其它方法，特别是为了提高解决方案的计算效率。由于Inception网络是全卷积的，每个权重对应每个激活的一次乘法。因此，任何计算成本的降低会导致参数数量减少。这意味着，通过适当的分解，我们可以得到更多的解耦参数，从而加快训练。此外，我们可以使用计算和内存节省来增加我们网络的滤波器组的大小，同时保持我们在单个计算机上训练每个模型副本的能力。
3.1. Factorization into smaller convolutions 具有较大空间滤波器（例如5×5或7×7）的卷积在计算方面往往不成比例地昂贵。例如，具有n个滤波器的5×5卷积在具有m个滤波器的网格上比具有相同数量的滤波器的3×3卷积的计算量高25/9=2.78倍。当然，5×5滤波器在更前面的层可以捕获更远的单元激活之间、信号之间的依赖关系，因此滤波器几何尺寸的减小带来了很大的表现力。然而，我们可以询问5×5卷积是否可以被具有相同输入尺寸和输出深度的参数较小的多层网络所取代。如果我们放大5×5卷积的计算图，我们看到每个输出看起来像一个小的完全连接的网络，在其输入上滑过5×5的块（见图1）。由于我们正在构建视觉网络，所以通过两层的卷积结构再次利用平移不变性来代替全连接的组件似乎是很自然的：第一层是3×3卷积，第二层是在第一层的3×3输出网格之上的一个全连接层（见图1）。通过在输入激活网格上滑动这个小网络，用两层3×3卷积来替换5×5卷积（比较图4和5）。
该设定通过相邻块之间共享权重明显减少了参数数量。为了分析预期的计算成本节省，我们将对典型的情况进行一些简单的假设：我们可以假设$n=αm$，也就是我们想通过常数$α$因子来改变激活/单元的数量。由于5×5卷积是聚合的，$α$通常比1略大（在GoogLeNet中大约是1.5）。用两个层替换5×5层，似乎可以通过两个步骤来实现扩展：在两个步骤中通过$\sqrtα$增加滤波器数量。为了简化我们的估计，通过选择$α=1$（无扩展），如果我们单纯地滑动网络而不重新使用相邻网格图块之间的计算，我们将增加计算成本。滑动该网络可以由两个3×3的卷积层表示，其重用相邻图块之间的激活。这样，我们最终得到一个计算量减少到$\frac{9+9}{25}×$的网络，通过这种分解导致了28％的相对增益。每个参数在每个单元的激活计算中只使用一次，所以参数计数具有完全相同的节约。不过，这个设置提出了两个一般性的问题：这种替换是否会导致任何表征力的丧失？如果我们的主要目标是对计算的线性部分进行分解，是不是建议在第一层保持线性激活？我们已经进行了几个控制实验（例如参见图2），并且在分解的所有阶段中使用线性激活总是逊于使用修正线性单元。我们将这个收益归因于网络可以学习的增强的空间变化，特别是如果我们对输出激活进行批标准化[7]。当对维度减小组件使用线性激活时，可以看到类似的效果。
图2。
两个Inception模型间几个控制实验中的一个，其中一个分解为线性层+ ReLU层，另一个使用两个ReLU层。在三亿八千六百万次运算后，在验证集上前者达到了76.2% top-1准确率，后者达到了77.2% top-1的准确率。
3.2. Spatial Factorization into Asymmetric Convolutions 上述结果表明，大于3×3的卷积滤波器可能不是通常有用的，因为它们总是可以简化为3×3卷积层序列。我们仍然可以问这个问题，是否应该把它们分解成更小的，例如2×2的卷积。然而，通过使用非对称卷积，可以做出甚至比2×2更好的效果，即n×1。例如使用3×1卷积后接一个1×3卷积，相当于以与3×3卷积相同的感受野滑动两层网络（参见图3）。如果输入和输出滤波器的数量相等，那么对于相同数量的输出滤波器，两层解决方案便宜33％。相比之下，将3×3卷积分解为两个2×2卷积表示仅节省了11％的计算量。
图3。
替换3×3卷积的Mini网络。网络的更低层由带有3个输出单元的3×1构成。
在理论上，我们可以进一步论证，可以通过1×n卷积和后面接一个n×1卷积替换任何n×n卷积，并且随着n增长，计算成本节省显著增加（见图6）。实际上，我们发现，采用这种分解在前面的层次上不能很好地工作，但是对于中等网格尺寸（在m×m特征图上，其中m范围在12到20之间），其给出了非常好的结果。在这个水平上，通过使用1×7卷积，然后是7×1卷积可以获得非常好的结果。
图4。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Inception V3" href=http://localhost:1313/posts/classicpapertranslation/inceptionv3/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>neo4j常用命令</h2></header><div class=entry-content><p>neo4j启动与访问 启动neo4j
1 2 docker start test_neo4j docker exec -it test_neo4j /bin/bash 访问browser
1 http://localhost:7474/browser/ 访问database
1 2 3 neo4j://localhost:7687 auth: neo4j pw: 5225400599 CQL语法 create 创建节点 1 2 3 4 5 6 7 8 CREATE ( &lt;node-name>:&lt;label-name> { &lt;Property1-name>:&lt;Property1-Value> ........ &lt;Propertyn-name>:&lt;Propertyn-Value> } ) match 查询节点或属性 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 查询Dept下的内容 MATCH (dept:Dept) return dept # 查询Employee标签下 id=123，name="Lokesh"的节点 MATCH (p:Employee {id:123,name:"Lokesh"}) RETURN p ## 查询Employee标签下name="Lokesh"的节点，使用（where命令） MATCH (p:Employee) WHERE p.name = "Lokesh" RETURN p ## 返回一个table MATCH (dept: Dept) RETURN dept.deptno,dept.dname,dept.location match 要绑定return使用，而return不能单独使用。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to neo4j常用命令" href=http://localhost:1313/posts/dailydev/neo4j/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>NIN</h2></header><div class=entry-content><p>NIN Abstract 本文提出了一种新的网络结构，称为 “网络中的网络”(NIN)，以提高模型对感受野内局部图块的可辨别性。传统的卷积层使用线性滤波器和非线性激活函数来扫描输入。相反，本文建立了结构更复杂的微神经网络，以抽象出感受野内的数据。本文用MLP来实例化微神经网络，MLP是一个有效的函数近似器。通过类似CNN的方式在输入上滑动微网络来提取特征图，然后将它们送入下一层。深度NIN可以通过堆叠多个上述结构来实现。通过微网络加强局部建模，能够在分类层中利用全局平均池化的特征图，这比传统的全连接层更容易解释，也更不容易过拟合。本文在CIFAR-10和CIFAR-100上用NIN取得了最先进的分类性能，其在SVHN和MNIST数据集上也有合理的表现。
1 Introduction 卷积神经网络[1]由交替的卷积层和池化层组成。卷积层采取线性滤波器和底层感受野的内积，然后在输入的每个局部使用非线性激活函数，由此产生的输出称为特征图。
CNN中的卷积滤波器是基础数据块的广义线性模型（GLM），本文认为GLM的抽象程度很低。这里所说的抽象性是指特征对同一概念的变体是不变的[2]。用一个更有力的非线性函数近似器代替GLM可以提高局部模型的抽象能力。当潜在概念的样本是线性可分离的，即概念的变体都在GLM定义的分离平面的一侧时，GLM可以实现良好的抽象程度。因此，传统的CNN隐含了潜在概念是线性可分离的假设。然而，同一概念的数据往往存在于一个非线性流形上，因此捕捉这些概念的表征通常是输入的高度非线性函数。在NIN中，GLM被一个 “微网络 “结构所取代，它是一个通用的非线性函数近似器。在这项工作中，本文选择MLP[3]作为微网络的实例，它是一个通用的函数逼近器并可通过反向传播训练的神经网络。
图1：线性卷积层和mlpconv层的比较。线性卷积层包括一个线性滤波器，而mlpconv层包括一个微型网络（本文选择mlp）。这两个层都将局部感受野映射为潜伏概念的置信度值。
由此产生的结构，本文称之为mlpconv层，与图1中的CNN进行比较。线性卷积层和mlpconv层都将局部感受野映射到输出的特征向量上。mlpconv用MLP将输入的局部图块映射到输出特征向量，MLP由多个具有非线性激活函数的全连接层组成，它的权重在所有的局部感受区之间共享。通过在输入上滑动MLP获得特征图，其方式与CNN类似，然后被送入下一层。NIN的整体结构是多个mlpconv层的堆叠。
在CNN中，本文没有采用传统的全连接层进行分类，而是通过全局平均池化层直接输出最后一个mlpconv层的特征图的空间平均值作为类别的置信度，然后将所得向量送入softmax层。在传统的CNN中，由于全连接层作为中间的黑匣子，很难解释来自目标成本层的类别级信息是如何传递回前一个卷积层的。相比之下，全局平均集合更有意义，也更容易解释，因为它强化了特征图和类别之间的对应关系，而这是通过使用微网络进行更强的局部建模实现的。此外，全连接层容易出现过拟合，并严重依赖dropout正则化[4] [5]，而全局平均池化本身就是一个结构正则化器，它天生就能防止整体结构的过拟合。
作者认为传统的GLM在特征提取的过程中根本不能区分这些中间过程里所形成的特征元素，除非这些特征元素是线性可分的。所以，在作者的眼里，传统CNN有效的一个假设就是这些特征元素能够线性可分。
例如在一个用于识别汽车图片的卷积网络模型中，靠前的卷积层会被用于提取一些粗糙的原始的特征（如：线段、棱角等）；而靠后的卷积层则会以前面的为基础提取到更为高级一些的特征（如：轮胎、车门等）。同时，在每个阶段里所形的这些特征原始都被称之为 “latent concept”，因为事实上还有很多抽像的特征我们人类是无法辨认的（它可能是有用的特征，也可能不是），所以被称为“latent”。作者认为，传统的GLM在进行每一阶段的特征提取中，根本不足以区分这些特征元素——例如某个卷积层可能提取得到了很多“轮胎”这一类的same concept，但是GLM区分不了这些非线性的特征（到底是哪一类汽车的轮胎）——所以导致最终的任务精度不那么的尽如人意。
2 Convolutional Neural Networks 经典的卷积神经元网络[1]由交替堆叠的卷积层和空间汇集层组成。卷积层通过线性卷积滤波器再加上非线性激活函数（rectifier, sigmoid, tanh等）生成特征图。以ReLU为例，特征图可以计算如下： $$ f_{i,j,k}=max(w_k^Tx_{i,j},0). $$ 是特征图中每个像素的索引， $x_{ij}$ 表示输入patch$(i, j)$, $k$ 用于索引特征图的通道。
当潜在概念的实例是线性可分离时，这种线性卷积足以实现抽象化。然而，实现良好抽象的表征通常是输入数据的高度非线性函数。在传统的CNN中，这一点可以通过利用一套过度完整的滤波器[6]来弥补，以覆盖潜概念的所有变化。也就是说，可以学习单个线性滤波器来检测同一概念的不同变化。然而，对一个概念有太多的过滤器会给下一层带来额外的负担，它需要考虑上一层的所有变化的组合[7]。如同在CNN中，来自高层的过滤器映射到原始输入中的更大区域。它通过结合下面一层的低级概念来生成一个更高级的概念。因此，本文认为，在将它们组合成更高层次的概念之前，对每个局部图块做一个更好的抽象是有益的。
在最近的maxout网络[8]中，通过对仿生特征图（仿生特征图是不应用激活函数的线性卷积的直接结果）的最大集合来减少特征图的数量。对线性函数的最大化使得一个分片线性逼近器能够逼近任何凸函数。与进行线性分离的传统卷积层相比，maxout网络更有效力，因为它可以分离位于凸集内的概念。这一改进使maxout网络在几个基准数据集上具有最佳性能。
然而，maxout网络强加了一个先验，即潜在概念的实例位于输入空间的凸集内，这并不一定成立。当潜在概念的分布更加复杂时，有必要采用一个更通用的函数近似器。所以本文试图通过引入新颖的 “网中网 “结构来实现这一点，在每个卷积层中引入一个微型网络来计算局部斑块的更抽象的特征。
在以前的一些工作中已经提出在输入上滑动微网络。例如，结构化多层感知器（SMLP）[9]在输入图像的不同斑块上应用共享多层感知器；在另一项工作中，基于神经网络的过滤器被训练用于人脸检测[10]。然而，它们都是为特定的问题而设计的，并且都只包含滑动网络结构中的一个层。NIN是从一个更普遍的角度提出的，微网络被整合到CNN结构中，以寻求对各级特征的更好的抽象。
可以先利用浅层的网络来对各个阶段里所形成的非线性特征元素进行特征表示，然后再通过卷积层来完成分类类别间线性不可分的抽象表示，以此来提高模型最后的任务精度。
3 Network In Network 本文首先强调了提出的 “网中网 “结构的关键部分：MLP卷积层和全局平均池层，分别在第3.1和第3.2节。然后，在第3.3节中详细介绍整个NIN。
3.1 MLP Convolution Layers 鉴于没有关于潜在概念分布的先验，最好使用通用函数近似器对局部斑块进行特征提取，因为它能够对潜在概念的更抽象的表示进行近似。径向基网络和MLP是两个著名的通用函数近似器。本文在这项工作中选择MLP有两个原因。首先，MLP与卷积神经网络的结构兼容，它是用反向传播法训练的。第二，MLP本身可以是一个深度模型，这与特征重用的精神是一致的[2]。这种新型的层在本文中被称为mlpconv，其中MLP取代了GLM对输入进行卷积。图1说明了线性卷积层和mlpconv层的区别。mlpconv层所进行的计算如下所示：
这里$n$是MLP中的层数。MLP中使用ReLU作为激活函数。
从跨通道（跨特征图）池化的角度来看，式2相当于在正常卷积层上的级联跨通道参数池化。每个池化层对输入的特征图进行加权线性重组，然后经过一个ReLU。跨通道池化后的特征图在下一层中再次进行跨通道池化。这种级联式跨渠道参数池结构允许跨渠道信息的复杂和可学习的相互作用。
跨信道参数池层也相当于一个具有1x1卷积核的卷积层，这种解释可以直接理解NIN的结构。
图2：Network In Network的整体结构。在本文中，NINs包括三个mlpconv层和一个全局平均池层的堆叠.
与maxout层的比较：maxout网络中的maxout层在多个仿生特征图上进行max pooling[8]。maxout层的特征图的计算方法如下： $$ f_{i,j,k}=max(w_{k_m}^{T}x_{i,j}). $$ 线性函数的Maxout形成了一个片状线性函数，能够对任何凸函数进行建模。对于一个凸函数，函数值低于特定阈值的样本形成一个凸集。因此，通过近似局部补丁的凸函数，maxout有能力为样本在凸集内的概念形成分离超平面（即$l_2$球、凸锥）。Mlpconv层与maxout层的不同之处在于，凸函数近似器被一个通用函数近似器所取代，它在模拟各种潜在概念的分布方面具有更大的能力。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to NIN" href=http://localhost:1313/posts/classicpapertranslation/nin/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>ResNet</h2></header><div class=entry-content><p>ResNet Abstract 更深的神经网络更难训练。我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。在ImageNet数据集上我们评估了深度高达152层的残差网络——比VGG[40]深8倍但仍具有较低的复杂度。这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。这个结果在ILSVRC 2015分类任务上赢得了第一名。我们也在CIFAR-10上分析了100层和1000层的残差网络。
对于许多视觉识别任务而言，表示的深度是至关重要的。仅由于我们非常深度的表示，我们便在COCO目标检测数据集上得到了28%的相对提高。深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。
1. Introduction 深度卷积神经网络[22, 21]导致了图像分类[21, 49, 39]的一系列突破。深度网络自然地将低/中/高级特征[49]和分类器以端到端多层方式进行集成，特征的“级别”可以通过堆叠层的数量（深度）来丰富。最近的证据[40, 43]显示网络深度至关重要，在具有挑战性的ImageNet数据集上领先的结果都采用了“非常深”[40]的模型，深度从16 [40]到30 [16]之间。许多其它重要的视觉识别任务[7, 11, 6, 32, 27]也从非常深的模型中得到了极大受益。
在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。
当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。意外的是，这种下降不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差，正如[10, 41]中报告的那样，并且由我们的实验完全证实。图1显示了一个典型的例子。
图1：20层和56层的“简单”网络在CIFAR-10上的训练误差（左）和测试误差（右）。更深的网络有更高的训练误差和测试误差。ImageNet上的类似现象如图4所示。
退化（训练准确率）表明不是所有的系统都很容易优化。让我们考虑一个较浅的架构及其更深层次的对象，为其添加更多的层。存在通过构建得到更深层模型的解决方案：添加的层是恒等映射，其他层是从学习到的较浅模型的拷贝。 这种构造解决方案的存在表明，较深的模型不应该产生比其对应的较浅模型更高的训练误差。但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）。
在本文中，我们通过引入深度残差学习框架解决了退化问题。我们明确地让这些层拟合残差映射，而不是希望每几个堆叠的层直接拟合期望的基础映射。形式上，将期望的基础映射表示为$H(x)$，我们将堆叠的非线性层拟合另一个映射$F(x):=H(x)−x$。原始的映射重写为$F(x)+x$。我们假设残差映射比原始的、未参考的映射更容易优化。在极端情况下，如果一个恒等映射是最优的，那么将残差置为零比通过一堆非线性层来拟合恒等映射更容易。
公式$F(x)+x$可以通过带有“快捷连接”的前向神经网络（图2）来实现。快捷连接[2, 33, 48]是那些跳过一层或更多层的连接。在我们的案例中，快捷连接简单地执行恒等映射，并将其输出添加到堆叠层的输出（图2）。恒等快捷连接既不增加额外的参数也不增加计算复杂度。整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。
图2：残差学习的构建块
我们在ImageNet[35]上进行了综合实验来显示退化问题并评估我们的方法。我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。
CIFAR-10数据集上[20]也显示出类似的现象，这表明了优化的困难以及我们的方法的影响不仅仅是针对一个特定的数据集。我们在这个数据集上展示了成功训练的超过100层的模型，并探索了超过1000层的模型。
在ImageNet分类数据集[35]中，我们通过非常深的残差网络获得了很好的结果。我们的152层残差网络是ImageNet上最深的网络，同时还具有比VGG网络[40]更低的复杂性。我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。极深的表示在其它识别任务中也有极好的泛化性能，并带领我们在进一步赢得了第一名：包括ILSVRC & COCO 2015竞赛中的ImageNet检测，ImageNet定位，COCO检测和COCO分割。坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。
2. Related Work 残差表示。在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。它们都是图像检索和图像分类[4,47]中强大的浅层表示。对于矢量量化，编码残差矢量[17]被证明比编码原始矢量更有效。
在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。这些方法表明，良好的重构或预处理可以简化优化。
快捷连接。导致快捷连接[2,33,48]的实践和理论已经被研究了很长时间。训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出[33,48]。在[43,24]中，一些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸。论文[38,37,31,46]提出了通过快捷连接实现层间响应，梯度和传播误差的方法。在[43]中，一个“inception”层由一个快捷分支和一些更深的分支组成。
和我们同时进行的工作，“highway networks” [41, 42]提出了门功能[15]的快捷连接。这些门是数据相关且有参数的，与我们不具有参数的恒等快捷连接相反。当门控快捷连接“关闭”（接近零）时，高速网络中的层表示非残差函数。相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。此外，高速网络还没有证实极度增加的深度（例如，超过100个层）带来的准确性收益。
3. Deep Residual Learning 3.1. Residual Learning 我们考虑$H(x)$作为几个堆叠层（不必是整个网络）要拟合的基础映射，xx表示这些层中第一层的输入。假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x)−x$(假设输入输出是相同维度)。因此，我们明确让这些层近似参数函数 $F(x):=H(x)−x$，而不是期望堆叠层近似$H(x)$。因此原始函数变为$F(x)+x$。尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。
关于退化问题的反直觉现象激发了这种重构（图1左）。正如我们在引言中讨论的那样，如果添加的层可以被构建为恒等映射，更深模型的训练误差应该不大于它对应的更浅版本。退化问题表明求解器通过多个非线性层来近似恒等映射可能有困难。通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。
在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的抖动，而不是将该函数作为新函数来学习。我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to ResNet" href=http://localhost:1313/posts/classicpapertranslation/resnet/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>SQL</h2></header><div class=entry-content><p>第一章：数据库基础 数据库：保存有组织的数据的容器（通常是一个文件或一组文件）
数据库软件（DBMS）：MySql，Oracle，MongoDB之类。人们通常用数据库来代替数据库软件的名称
表（table）：某种特定类型数据的结构化清单
模式：关于数据库和表的布局及特性的信息
列（column）：表中的一个字段（该列由字段来唯一标识），所有表都是由一个或多个列组成的，每一列都有自己的数据类型
行（row）：表中的一条数据是由行来存储的
主键（primary key）：唯一标识表中每行的这个列就是主键，应该总是定义主键
关于主键：
任意两行都不具有相同的主键值 每个行都必须有一个主键值 SQL：结构化查询语言
第二章：MySQL简介 略
第三章：使用MySQL 登录数据库
默认主机名：localhost
默认端口：3306
1 2 mysql -u root -p # 然后输入密码 选择数据库
1 USE database_name; 了解数据库和表
查看所有数据库
1 SHOW DATABASES; 查看一个数据库中的所有表
1 SHOW TABLES; 查看一个表的所有字段
1 SHOW COLUMNS FROM table; 还有一种快捷写法
1 DESCRIBE table; 在返回的列表中可以看到一些建表信息，如字段名，数据类型，键类型，是否为NULL，默认值，其他类型。
其他的show语句
1 2 3 4 5 SHOW GRANTS; # 显示授予用户的安全权限 SHOW ERRORS; # 显示服务器错误 SHOW WARNINGS; # 显示服务器警告信息 SHOW STATUS; # 显示服务器的状态信息 HELP SHOW; # 显示mysql允许的show语句 有一个书写规则：select这样的关键词要大写，表名、列名、数据库名要小写
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to SQL" href=http://localhost:1313/posts/dailydev/mysql%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>VGG</h2></header><div class=entry-content><p>VGG ABSTRACT 本文研究了卷积网络深度在大规模的图像识别环境下对准确性的影响。主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。这些发现是ImageNet Challenge 2014提交的基础，本文作者团队在定位和分类过程中分别获得了第一名和第二名。
1 INTRODUCTION 随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。在本文中，解决了ConvNet架构设计的另一个重要方面——其深度。为此修正了架构的其它参数，并通过添加更多的卷积层来稳定地增加网络的深度，这是可行的，因为在所有层中使用非常小的（3×3）卷积滤波器。
本文的其余部分组织如下。在第2节，描述了ConvNet配置。图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。第5节总结了论文。
2 CONVNET CONFIGURATIONS 为了衡量ConvNet深度在公平环境中所带来的改进，所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。在本节中，首先描述ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。最后，网络的设计选择将在2.3节进行讨论并与现有技术进行比较。
2.1 ARCHITECTURE 在训练期间，ConvNet的输入是固定大小的224×224 RGB图像。唯一的预处理是从每个像素中减去在训练集上计算的RGB均值。图像通过若干卷积（conv.）层，选择使用感受野很小的滤波器：3×3（这是捕获图像左/右，上/下，中心特征的最小尺寸）。在其中一种配置中还使用了1×1卷积滤波器，1×1卷积可以看作输入通道的线性变换（后面是非线性）。卷积步长固定为1个像素；卷积层输入的空间填充要满足卷积之后保留空间分辨率，即3×3卷积层的填充为1个像素。空间池化由五个最大池化层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。在2×2像素窗口上进行最大池化，步长为2。
一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。最后一层是soft-max层。所有网络中全连接层的配置是相同的。
所有隐藏层都配备了ReLU。可以注意到，所有网络（除了一个）都不包含LRN（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。
2.2 CONFIGURATIONS 本文中评估的ConvNet配置在表1中列出，每列为一个网络。接下来将按A-E顺序来提及网络。所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。
ConvNet配置（以列显示）。随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。卷积层参数表示为“conv⟨感受野大小⟩-通道数⟩”。为了简洁起见，不显示ReLU激活功能。
在表2中，包含了每个配置的参数数量。尽管深度很大，但网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。
表2：参数数量（百万级别）
2.3 DISCUSSION 本文的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。本文在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。很容易看到两个3×3卷积层堆叠（没有空间池化）有5×5的有效感受野；三个这样的层具有7×7的有效感受野。
那么我们获得了什么？例如通过使用三个3×3卷积层的堆叠来替换单个7×7层。首先，结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。其次，减少了参数的数量：假设三层3×3卷积堆叠的输入和输出有$C$个通道，堆叠卷积层的参数为$3(3^2C^2)=27C^2$个权重；同时，单个7×7卷积层将需要$7^2C^2=49C^2$个参数，即参数多81％。这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。
3 CLASSIFICATION FRAMEWORK 在本节中，将介绍分类ConvNet训练和评估的细节。
3.1 TRAINING ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。批量大小设为256，动量为0.9。训练通过权重衰减（L2惩罚乘子设定为$5\times10^{-4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。学习率初始设定为$10^{-2}$，然后当验证集准确率停止改善时，减少10倍。学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。经过推测，尽管与（Krizhevsky等，2012）相比本文网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。
网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。为了规避这个问题，开始训练配置A（表1），其足够浅能够以随机初始化进行训练。然后，当训练更深的架构时，用网络A的层初始化前四个卷积层和最后三个全连接层（中间层被随机初始化）。没有减少预初始化层的学习率，允许他们在学习过程中改变。对于随机初始化（如果应用），选择从均值为0和方差为$10^{-2}$的正态分布中采样权重。偏置初始化为零。
为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。下面解释训练图像归一化。
训练图像大小。令$S$是等轴归一化的训练图像的最小边，ConvNet输入从$S$中裁剪（我们也将S称为训练尺度）。虽然裁剪尺寸固定为224×224，但原则上S可以是不小于224的任何值：对于$S=224$，裁剪图像将捕获整个图像的统计数据，完全扩展训练图像的最小边；对于$S»224$，裁剪图像将对应于图像的一小部分，包含小对象或对象的一部分。
考虑两种方法来设置训练尺度$S$。第一种是修正对应单尺度训练的S（注意，采样裁剪图像中的图像内容仍然可以表示多尺度图像统计）。在本文的实验中评估了以两个固定尺度训练的模型：（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和$S=384$。给定ConvNet配置，首先使用$S=256$来训练网络。为了加速$S=384$网络的训练，用$S=256$预训练的权重来进行初始化，使用较小的初始学习率$10^{-3}$。
设置S的第二种方法是多尺度训练，其中每个训练图像通过从一定范围$S_{min}$，$S_{max}$（我们使用$S_{min}=256$和$S_{max}=512$）随机采样$S$来单独进行归一化。由于图像中的目标可能具有不同的大小，因此在训练期间考虑到这一点是有益的。这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。为了速度，通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的$S=384$进行预训练。
3.2 TESTING 在测试时，给出训练的ConvNet和输入图像，它按以下方式分类。首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。我们注意到，Q不一定等于训练尺度S（正如我们在第4节中所示，每个S使用Q的几个值会导致性能改进）。然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。即，全连接层首先被转换成卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层）。然后将所得到的全卷积网络应用于整个（未裁剪）图像上。结果是类得分图的通道数等于类别的数量，以及取决于输入图像大小的可变空间分辨率。最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。
由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。同时，如Szegedy等人（2014）所做的那样，使用大量的裁剪图像可以提高准确度，因为与全卷积网络相比，它使输入图像的采样更精细。此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。
3.3 IMPLEMENTATION DETAILS 略（GPU上的实现细节，模型实现细节，已过时）
4 CLASSIFICATION EXPERIMENTS 在本节中，介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。数据集包括1000个类别的图像，并分为三组：训练（130万张图像），验证（5万张图像）和测试（留有类标签的10万张图像）。使用两个措施评估分类性能：top-1和top-5错误率。前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。
4.1 SINGLE SCALE EVALUATION 首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。测试图像大小设置如下：对于固定$S$的$Q=S$，对于抖动$S∈[S_{min},S_{max}]，Q=0.5(S_{min}+S_{max})$.结果如表3所示。
表3：在单测试尺度的ConvNet性能
首先注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。因此，在较深的架构（B-E）中不采用归一化。
第二，可以观察到分类误差随着ConvNet深度的增加而减小：从A中的11层到E中的19层。值得注意的是，尽管深度相同，配置C（包含三个1×1卷积层）比在整个网络层中使用3×3卷积的配置D更差。这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。当深度达到19层时，架构的错误率饱和，但更深的模型可能有益于较大的数据集。同时还将网络B与具有5×5卷积层的浅层网络进行了比较，浅层网络可以通过用单个5×5卷积层替换B中每对3×3卷积层得到（其具有相同的感受野如第2.3节所述）。测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。
最后，训练时的尺度抖动（$S∈[256;512]$）得到了与固定最小边（$S=256$或$S=384$）的图像训练相比更好的结果，即使在测试时使用单尺度。这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to VGG" href=http://localhost:1313/posts/classicpapertranslation/vgg/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>正则表达式</h2></header><div class=entry-content><p>第二章. 匹配单个字符 ’ . ’ 用来匹配任意单一字符, 元字符的一种
’ \ ‘为转义字符, 属于元字符的一种, 元字符: 有特殊含义的字符
正则表达式被称为模式(pattern)
第三章. 匹配一组字符 ’ [ ] ‘为元字符, 表示一个字符集合, 必须匹配其中的一个或多个字符, 也可以全部匹配. ’ [ ] ‘可以用来匹配大小写, 如[Aa].*就匹配任意以A或a或Aa开头的字符串. 还有几种常见的用法, 如[a-z] [A-Z] [0-9], 这几种很常用, 还有一个用法[A-Za-z0-9] 这个字符集可以匹配以上三种用法的合集
’ - ‘表示连字符, 是一种较为特殊的元字符, 只有在’ [ ] ’ 里才是元字符, 在其他地方就是一个普通的字符’ - ‘, 也因此在这种情况下它不需要转义
’ ^ ’ 表示排除, 也是元字符. 在上面的几种用法中, 在集合的最前面加上’ ^ ‘, 就表示匹配除了该集合以外的字符, 而且需要注意的是, ’ ^ ‘的作用域是整个字符集合, 而不是紧跟在其身后的单个字符什么的
第四章. 使用元字符 如果要匹配元字符本身, 可以用 ’ \ . 如: 匹配[ ], 用\ [ \ ]
...</p></div><footer class=entry-footer><span title='2024-06-26 00:00:00 +0000 UTC'>June 26, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to 正则表达式" href=http://localhost:1313/posts/dailydev/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A__%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=http://localhost:1313/cover/%E9%A2%86%E5%9F%9F%E7%BB%BC%E8%BF%B0.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>KnowledgeGraph Abstract</h2></header><div class=entry-content><p>本文是对封面论文《A Survey on Knowledge Graphs: Representation, Acquisition and Applications》的提炼，目的是获取对该领域的一个大致认识。
Abstract 人类知识为我们对世界的形式化理解提供了基础。知识图谱作为表示实体之间结构关系的工具，已成为越来越受欢迎的研究方向，旨在实现认知和人类级别的智能。在这份综述中，我们全面回顾了知识图谱的研究主题，包括：1）知识图谱表示学习，2）知识获取与补全，3）时间知识图谱，以及4）知识感知应用，并总结了近期的突破和未来研究的展望方向。我们提出了一个全面的分类和新的分类法。知识图谱嵌入从四个方面进行组织：表示空间、评分函数、编码模型和辅助信息。对于知识获取，特别是知识图谱的补全，我们回顾了嵌入方法、路径推断和逻辑规则推理。此外，我们还探讨了几个新兴的主题，包括元关系学习、常识推理和时间知识图谱。为了促进知识图谱的未来研究，我们还提供了一份精选的不同任务的数据集和开源库。最后，我们对几个有前景的研究方向进行了详细的展望。
通过该论文的摘要可以非常清晰的看出本文主要内容是关于知识图谱领域的综述。
1. Introduction 知识图谱是一种结构化的事实表示，包括实体、关系和语义描述。实体可以是现实世界的对象和抽象概念，关系表示实体之间的关联，实体及其关系的语义描述包含具有明确定义含义的类型和属性。属性图或带属性的图广泛应用，其中节点和关系具有属性或特性。
知识图谱与知识库这两个术语在某种程度上是同义的，但存在一些细微差异。当考虑到图形结构时，知识图谱可以被视为一个图。而当涉及到形式化语义时，它可以被视为用于解释和推理事实的知识库。图1中展示了知识库和知识图谱的示例。知识可以用事实三元组的形式来表达，例如（头部，关系，尾部）或（主题，谓词，对象），在资源描述框架（RDF）下，例如（阿尔伯特·爱因斯坦，获奖者，诺贝尔奖）。它还可以表示为一个有向图，其中节点表示实体，边表示关系。为了简化，本文将知识图谱和知识库这两个术语视为可以互换使用。
资源描述框架（RDF）是用于描述网络资源的 W3C 标准，比如网页的标题、作者、修改日期、内容以及版权信息。RDF 被设计为提供一种描述信息的通用方法，这样就可以被计算机应用程序读取并理解。具体内容参考RDF 教程 (w3school.com.cn)
知识图谱的最新研究进展（2021年）聚焦于知识表示学习（KRL）或知识图嵌入（KGE），通过将实体和关系映射到低维向量中，同时捕捉它们的语义含义 。具体的知识获取任务包括知识图谱补全（KGC）、三元组分类、实体识别和关系抽取。知识感知模型受益于异构信息、丰富的本体和语义以及多语言知识的整合。因此，许多现实世界的应用，如推荐系统和问答系统，都因其具备常识理解和推理能力而繁荣发展。
本文的剩余部分中，在第二部分提供了知识图谱的概述，包括历史、符号、定义和分类；然后，在第三部分中，从四个角度讨论了KRL；接下来，在第四部分和第五部分详细探讨了知识获取和时间知识图谱的任务；在第六部分介绍了下游应用；最后讨论了未来的研究方向，并在结尾处总结。
2. Overview A. A Brief History of Knowledge Bases 图形化知识表示的概念最早可以追溯到1956年，当时Richens提出了“语义网”的概念。而符号逻辑知识则可以追溯到1959年的“通用问题求解器”。知识库首次用于基于知识的推理和问题求解系统。MYCIN是最著名的基于规则的专家系统之一，用于医学诊断，其知识库包含约600条规则。随后，人类知识表示领域出现了基于框架、基于规则和混合表示的发展。大约在这一时期末，Cyc项目开始，旨在整合人类知识。Resource Description Framework（RDF）和Web Ontology Language（OWL）相继发布，并成为语义Web的重要标准。此后，许多开放的知识库或本体被发布，例如WordNet、DBpedia、YAGO和Freebase。1988年，Stokman和Vries提出了一种现代的基于图形的结构化知识的理念。然而，直到2012年，知识图谱的概念才因Google搜索引擎的首次推出而广受欢迎，其中提出了名为“知识金库”的大规模知识图谱融合框架。
B. Definitions and Notations 知识图谱定义为 $G = \{E, R, F\}$ ，其中 $E$、$R$ 和 $F$ 分别是实体、关系和事实的集合。事实被三元组 $(h,r,t) \in F$ 表示，其中 $h$ 、 $r$ 和 $t$ 分别是 head、relation、tail。
C. Categorization of Research on Knowledge Graph ...</p></div><footer class=entry-footer><span title='2024-06-25 00:00:00 +0000 UTC'>June 25, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to KnowledgeGraph Abstract" href=http://localhost:1313/posts/knowledgegraph/%E9%A2%86%E5%9F%9F%E7%BB%BC%E8%BF%B0/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=http://localhost:1313/>«&nbsp;Prev&nbsp;
</a><a class=next href=http://localhost:1313/page/3/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>