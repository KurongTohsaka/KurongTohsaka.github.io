<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CS224N | KurongBlog</title>
<meta name=keywords content><meta name=description content="记录日常"><meta name=author content="Kurong"><link rel=canonical href=http://localhost:1313/tags/cs224n/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/tags/cs224n/index.xml><link rel=alternate hreflang=en href=http://localhost:1313/tags/cs224n/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="CS224N"><meta property="og:description" content="记录日常"><meta property="og:type" content="website"><meta property="og:url" content="http://localhost:1313/tags/cs224n/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="KurongBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="CS224N"><meta name=twitter:description content="记录日常"></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/tags/>Tags</a></div><h1>CS224N</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 10: Pretrained Model</h2></header><div class=entry-content><p>Word structure and subword models We assume a fixed vocab of tens of thousands of words, built from the training set. All novel words seen at test time are mapped to a single UNK.
Finite vocabulary assumptions make even less sense in many languages. Many languages exhibit complex morphology, or word structure.
The byte-pair encoding algorithm (BPE) Subword modeling in NLP encompasses a wide range of methods for reasoning about structure below the word level.
...</p></div><footer class=entry-footer><span title='2024-08-16 00:00:00 +0000 UTC'>August 16, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 10: Pretrained Model" href=http://localhost:1313/posts/cs224n/lesson_10/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 9: Transformer</h2></header><div class=entry-content><p>Issues with recurrent models Linear interaction distance RNNs are unrolled “left-to-right”
Problem: RNNs take O(sequence length) steps for distant word pairs to interact
What does the O Problem means ?
Hard to learn long-distance dependencies (because gradient problems! ) Linear order of words is “baked in”; we already know linear order isn’t the right way to think about sentences… Lack of parallelizability Forward and backward passes have O(sequence length) unparallelizable operations GPUs can perform a bunch of independent computations at once, but future RNN hidden states can’t be computed in full before past RNN hidden states have been computed Self-Attention Recall: Attention operates on queries, keys, and values.
...</p></div><footer class=entry-footer><span title='2024-08-13 00:00:00 +0000 UTC'>August 13, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 9: Transformer" href=http://localhost:1313/posts/cs224n/lesson_9/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 8: Attention</h2></header><div class=entry-content><p>Sequence-to-sequence with attention Attention: in equations We have encoder hidden states $h_1,...,h_N \in \R^h$
On timestep $t$​ , we have decoder hidden state $s_t \in \R^h$
We get the attention scores $e^t$ for this step: $$ e^t=[s^T_th_1,...,s^T_th_N] \in \R^N $$ We take softmax to get the attention distribution $\alpha^t$​ for this step (this is a probability distribution and sums to 1) $$ \alpha^t=softmax(e^t) \in \R^N $$ We use $\alpha^t$ to take a weighted sum of the encoder hidden states to get the attention output $a_i$ ...</p></div><footer class=entry-footer><span title='2024-07-27 00:00:00 +0000 UTC'>July 27, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 8: Attention" href=http://localhost:1313/posts/cs224n/lesson_8/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 7: Machine Translation and Sequence to Sequence</h2></header><div class=entry-content><p>Machine Translation Machine Translation is the task of translating a sentence $x$ from one language to a sentence $y$​ in another language.
Simple History:
1990s-2010s: Statistical Machine Translation After 2014: Neural Machine Translation Sequence to Sequence Model The sequence-to-sequence model is an example of a Conditional Language Model
Language Model because the decoder is predicting the next word of the target sentence $y$ Conditional because its predictions are also conditioned on the source sentence $x$ ...</p></div><footer class=entry-footer><span title='2024-07-13 00:00:00 +0000 UTC'>July 13, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 7: Machine Translation and Sequence to Sequence" href=http://localhost:1313/posts/cs224n/lesson_7/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 6: Long Short-Term Memory RNNs</h2></header><div class=entry-content><p>Training an RNN Language Model Get a big corpus of text which is a sequence of words $x^{(1)},...,x^{(T)}$ Feed into RNN-LM; compute output distribution $\hat y ^{(t)}$ for every timestep $t$​ Backpropagation for RNNs Problems with Vanishing and Exploding Gradients Vanishing gradient intuition Why is vanishing gradient a problem? 来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多 因此，模型权重只会根据近期效应而不是长期效应进行更新 If gradient is small, the model can’t learn this dependency. So, the model is unable to predict similar long distance dependencies at test time.
...</p></div><footer class=entry-footer><span title='2024-07-11 00:00:00 +0000 UTC'>July 11, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 6: Long Short-Term Memory RNNs" href=http://localhost:1313/posts/cs224n/lesson_6/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 5: Language Models and Recurrent Neural Network</h2></header><div class=entry-content><p>Basic Tricks on NN L2 Regularization A full loss function includes regularization over all parameters $\theta$ , e.g., L2 regularization: $$ J(\theta)=f(x)+\lambda \sum_k \theta^2_k $$ Regularization produces models that generalize well when we have a “big” model.
Dropout Training time: at each instance of evaluation (in online SGD-training), randomly set 50% of the inputs to each neuron to 0 Test time: halve the model weights (now twice as many) This prevents feature co-adaptation Can be thought of as a form of model bagging (i.e., like an ensemble model) Nowadays usually thought of as strong, feature-dependent regularizer Vectorization ...</p></div><footer class=entry-footer><span title='2024-07-05 00:00:00 +0000 UTC'>July 5, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 5: Language Models and Recurrent Neural Network" href=http://localhost:1313/posts/cs224n/lesson_5/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 4: Dependency parsing</h2></header><div class=entry-content><p>Two views of linguistic structure Context-free grammars (CFGs) Phrase structure organizes words into nested constituents.
Dependency structure Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.
modify：修饰词，attach to：连接词
Why do we need sentence structure? Humans communicate complex ideas by composing words together into bigger units to convey complex meanings. Listeners need to work out what modifies [attaches to] what A model needs to understand sentence structure in order to be able to interpret language correctly Linguistic Ambiguities Prepositional phrase attachment ambiguity 介词短语附着歧义 Coordination scope ambiguity 对等范围歧义 Adjectival/Adverbial Modifier Ambiguity 形容词修饰语歧义 Verb Phrase (VP) attachment ambiguity 动词短语依存歧义 Dependency paths identify semantic relations 依赖路径识别语义关系 help extract semantic interpretation.
...</p></div><footer class=entry-footer><span title='2024-07-04 00:00:00 +0000 UTC'>July 4, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 4: Dependency parsing" href=http://localhost:1313/posts/cs224n/lesson_4/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Lecture 1: Introduction and Word Vectors</h2></header><div class=entry-content><p>Meaning Definition: meaning 语义
the idea that is represented by a word, phrase, etc. the idea that a person wants to express by using words, signs, etc. the idea that is expressed in a work of writing, art, etc. WordNet Common NLP solution: Use, e.g., WordNet, a thesaurus containing lists of synonym (同义词) sets and hypernyms (上位词) (“is a” relationships).
Problems with resources like WordNet Great as a resource but missing nuance
...</p></div><footer class=entry-footer><span title='2024-06-27 00:00:00 +0000 UTC'>June 27, 2024</span>&nbsp;·&nbsp;Kurong</footer><a class=entry-link aria-label="post link to Lecture 1: Introduction and Word Vectors" href=http://localhost:1313/posts/cs224n/lesson_1/></a></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>KurongBlog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>