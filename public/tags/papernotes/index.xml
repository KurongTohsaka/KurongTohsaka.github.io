<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>PaperNotes on KurongBlog</title>
    <link>http://localhost:1313/tags/papernotes/</link>
    <description>Recent content in PaperNotes on KurongBlog</description>
    <image>
      <title>KurongBlog</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.135.0</generator>
    <language>en</language>
    <lastBuildDate>Sat, 05 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/papernotes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>《Distantly-Supervised Joint Extraction with Noise-Robust Learning》笔记</title>
      <link>http://localhost:1313/posts/papernotes/distantly-supervised_joint_extraction_with_noise-robust_learning/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/distantly-supervised_joint_extraction_with_noise-robust_learning/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.04994#:~:text=We%20propose%20DENRL,%20a%20generalizable%20framework%20that%201&#34;&gt;https://arxiv.org/abs/2310.04994#:~:text=We%20propose%20DENRL,%20a%20generalizable%20framework%20that%201&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Accepted by ACL 2024.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;联合抽取&lt;/strong&gt;旨在使用单一模型检测实体及其关系，这是自动知识库构建中的关键步骤。为了廉价地获取大量标注的联合训练数据，提出了&lt;strong&gt;远程监督（Distantly Supervise，DS）&lt;/strong&gt;，通过将知识库（Knowledge Base，KB）与未标注的语料库对齐，自动生成训练数据。假设如果一个实体对在 KB 中有关系，则包含该对的所有句子都表达相应的关系。&lt;/p&gt;
&lt;p&gt;然而，DS 带来了大量的&lt;strong&gt;噪声标签&lt;/strong&gt;，显著降低了联合抽取模型的性能。此外，由于开放域 KB 中实体的模糊性和覆盖范围有限，DS 还会生成噪声和不完整的实体标签。在某些情况下，DS 可能导致 KB 中包含超过30%的噪声实例，使得学习有用特征变得不可能。&lt;/p&gt;
&lt;p&gt;处理这些噪声标签的先前研究要么考虑弱标注的实体，即远程监督的命名实体识别（NER），要么考虑噪声关系标签，即远程监督的关系抽取（RE），它们专注于设计新颖的手工制作关系特征、神经架构和标注方案以提高关系抽取性能。此外，使用大型语言模型（LLMs）的上下文学习（ICL）也很流行。然而，它们资源需求高，对提示设计敏感，可能在处理复杂任务时表现不佳。&lt;/p&gt;
&lt;p&gt;为了廉价地减轻两种噪声源，我们提出了 &lt;strong&gt;DENRL&lt;/strong&gt; （&lt;strong&gt;D&lt;/strong&gt;istantly-supervised joint &lt;strong&gt;E&lt;/strong&gt;xtraction with &lt;strong&gt;N&lt;/strong&gt;oise-&lt;strong&gt;R&lt;/strong&gt;obust &lt;strong&gt;L&lt;/strong&gt;earning）。DENRL 假设&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可靠的关系标签，其关系模式显著表明实体对之间的关系，应该由模型解释；&lt;/li&gt;
&lt;li&gt;可靠的关系标签也隐含地表明相应实体对的可靠实体标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具体来说，DENRL应用词袋正则化（BR）引导模型关注解释正确关系标签的显著关系模式，并使用基于本体的逻辑融合（OLF）通过概率软逻辑（PSL）教授底层实体关系依赖性。这两种信息源被整合形成噪声鲁棒损失，正则化标注模型从具有正确实体和关系标签的实例中学习。接下来，如果学习到的模型能够清晰地定位关系模式并理解候选实例的实体关系逻辑，它们将被选择用于后续的自适应学习。我们进一步采样包含已识别模式中对应头实体或尾实体的负实例以减少实体噪声。我们迭代学习一个可解释的模型并选择高质量实例。这两个步骤相互强化——更可解释的模型有助于选择更高质量的子集，反之亦然。&lt;/p&gt;
&lt;h2 id=&#34;joint-extraction-architecture&#34;&gt;Joint Extraction Architecture&lt;/h2&gt;
&lt;h3 id=&#34;tagging-scheme&#34;&gt;Tagging Scheme&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Distantly-Supervised_Joint_Extraction_with_Noise-Robust_Learning/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Distantly-Supervised_Joint_Extraction_with_Noise-Robust_Learning/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;为了同时抽取实体（提及和类型）和关系，我们为每个起始位置 $p$ 标注四元组 ${e_1, tag_1, e_2, r_e}$，并定义 “BIO” 标记来编码位置。对于一个 $T$ 个 token 的句子，我们根据不同的起始位置标注 $T$ 个不同的标记序列。&lt;/p&gt;
&lt;p&gt;对于每个标记序列，如果 $p$ 是一个实体的起始位置（该序列是一个实例），则在 $p$ 处标注实体类型，并用关系类型标注与 $p$ 处实体有关系的其他实体。其余的令牌标注为 “O”（Outside），表示它们不对应头实体。这样，每个标记序列将生成一个关系四元组。&lt;/p&gt;
&lt;p&gt;我们将包含至少一个关系的实例定义为正实例，没有关系的实例定义为负实例。“BIO”（Begin, Inside, Outside）标记用于指示每个实体中令牌的位置信息，以便同时提取多词实体和关系类型。注意，我们不需要尾实体类型，因为每个实体都会被查询，我们可以从 T 标记序列中获得所有实体类型及其关系。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Summarization as Indirect Supervision for Relation Extraction》笔记</title>
      <link>http://localhost:1313/posts/papernotes/summarization_as_indirect_supervision_for_relation_extraction/</link>
      <pubDate>Sun, 29 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/summarization_as_indirect_supervision_for_relation_extraction/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2205.09837v2&#34;&gt;2205.09837v2] Summarization as Indirect Supervision for Relation Extraction (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted EMNLP 2022.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;关系抽取（RE）旨在从文本中提取实体之间的关系。例如，给定句子“Steve Jobs 是 Apple 的创始人”，RE 模型会识别出“创立”这一关系。RE 是自然语言理解的重要任务，也是构建知识库的关键步骤。先进的 RE 模型对于对话系统、叙事预测和问答等知识驱动的下游任务至关重要。&lt;/p&gt;
&lt;p&gt;现有的 RE 模型通常依赖于带有昂贵注释的训练数据，这限制了它们的应用。为了应对这一问题，本文提出了一种新的方法——&lt;strong&gt;SURE（Summarization as Relation Extraction）&lt;/strong&gt;，将 RE 转化为摘要任务，通过间接监督来提高 RE 的精度和资源效率。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Summarization_as_Indirect_Supervision_for_Relation_Extraction/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;图1展示了 SURE 的结构。具体来说，SURE 通过关系和句子转换技术将 RE 转化为摘要任务，并应用约束推理进行关系预测。我们采用实体信息口语化技术，突出包含实体信息的句子上下文，并将关系口语化为模板式的简短摘要。这样，转换后的RE输入和输出自然适合摘要模型。然后，我们通过在转换后的RE数据上进行微调，将摘要模型适配于RE任务。在推理过程中，设计了一种 Trie 评分技术来推断关系。通过这种方式，SURE 充分利用了摘要的间接监督，即使在资源匮乏的情况下也能获得精确的RE模型。&lt;/p&gt;
&lt;p&gt;这项工作的贡献有两个方面。首先，据我们所知，这是首次研究利用摘要的间接监督进行RE。由于摘要的目标与 RE 自然对齐，它允许在不完全依赖直接任务注释的情况下训练出精确的 RE 模型，并在资源匮乏的情况下表现出色。其次，我们研究了有效桥接摘要和 RE 任务形式的输入转换技术，以及进一步增强基于摘要的RE推理的约束技术。我们的贡献通过在三个广泛使用的句子级 RE 数据集 TACRED、TACREV 和 SemEval 以及 TACRED 的三个低资源设置上的实验得到验证。我们观察到，SURE 在低资源设置下（使用10%的 TACRED 训练数据）优于各种基线。SURE 还在 TACRED 和 TACREV上 分别以75.1%和83.5%的 micro-F1 得分达到了SOTA 性能。我们还进行了全面的消融研究，展示了摘要的间接监督的有效性以及 SURE 输入转换的最佳选项。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Modular Self-Supervision for Document-Level Relation Extraction》笔记</title>
      <link>http://localhost:1313/posts/papernotes/modular_self-supervision_for_document-level_relation_extraction/</link>
      <pubDate>Sat, 28 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/modular_self-supervision_for_document-level_relation_extraction/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2109.05362&#34;&gt;2109.05362] Modular Self-Supervision for Document-Level Relation Extraction (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted at EMNLP 2021&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;信息抽取的先前工作通常集中在句子内的二元关系。然而，实际应用往往需要跨大段文本提取复杂关系。这在生物医学等高价值领域尤为重要，因为获取最新发现的高召回率至关重要。例如，图1显示了一个三元（药物、基因、突变）关系，表明具有 MAP2K1 突变 K57T 的肿瘤对 cobimetinib 敏感，但这些实体从未在同一段落中同时出现。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Modular_Self-Supervision_for_Document-Level_Relation_Extraction/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;先前的工作都将文档级关系抽取视为一个单一的整体问题，这在推理和学习上都带来了重大挑战。尽管最近取得了一些进展，但在使用最先进的神经架构（如LSTM 和 transformer）建模长文本范围时仍存在显著挑战。此外，直接监督稀缺，任务特定的自监督（如距离监督）在应用于短文本范围之外时变得极其嘈杂。&lt;/p&gt;
&lt;p&gt;在本文中，我们通过将文档级关系抽取分解为局部关系检测和全局推理来探索一种替代范式。具体来说，我们使用 Davidsonian 语义表示 $n$ 元关系，并结合段落级关系分类和使用全局推理规则（例如，参数解析的传递性）的篇章级参数解析。每个组件问题都存在于短文本范围内，其相应的自监督错误率要低得多。我们的方法借鉴了模块化神经网络和神经逻辑编程的灵感，将复杂任务分解为局部神经学习和全局结构化集成。然而，我们不是从端到端的直接监督中学习，而是承认组件问题的模块化自监督（Modular Self-Supervision），这更容易获得。&lt;/p&gt;
&lt;p&gt;这种模块化方法不仅使我们能够处理长文本，还能扩展到所有先前方法无法覆盖的跨段落关系。我们在精准肿瘤学的生物医学机器阅读中进行了全面评估，其中跨段落关系尤为普遍。我们的方法在最具挑战性的关系中表现尤为突出，这些关系的参数从未在段落中同时出现，其F1分数比之前的最先进方法（如多尺度学习（和图神经网络高出20多个百分点。&lt;/p&gt;
&lt;h2 id=&#34;document-level-relation-extraction&#34;&gt;Document-Level Relation Extraction&lt;/h2&gt;
&lt;p&gt;设 $E,R,D$ 分别代表实体、关系、文档，那在图2中的 $R$ 为精准癌症药物反应，实体 $E_1,E_2,E_3$​ 分别药物 cobimetinib、基因 MAP2K1 和突变 K57T。这个关系跨越多个段落和几十个句子。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Modular_Self-Supervision_for_Document-Level_Relation_Extraction/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;用新戴维森语义表示 n 元关系抽取：
&lt;/p&gt;
$$
R_D(E_1, \cdots, E_n) \equiv \exists T \in D \exists r. [R_T(r) \land A_1(r, E_1) \land \cdots \land A_n(r, E_n)]
$$&lt;p&gt;
其中，$T$ 为文档 $D$ 中的片段，$r$ 为引入的事件变量以表示 $R$​ 。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction》笔记</title>
      <link>http://localhost:1313/posts/papernotes/separating_retention_from_extraction/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/separating_retention_from_extraction/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2109.12008v1&#34;&gt;2109.12008v1] Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted at EMNLP 2021&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;信息抽取（Information Extraction, IE）旨在将文本中表达的信息转换为预定义的结构化知识格式。这个总体目标被分解为更容易自动执行和评估的子任务。因此，命名实体识别（Named Entity Recognition, NER）和关系抽取（Relation Extraction, RE）是两个关键的 IE 任务。传统上，这些任务是通过流水线方式执行的。也可以采用联合方式处理，以建模它们的相互依赖性，减少错误传播并获得更现实的评估设置。&lt;/p&gt;
&lt;p&gt;随着 NLP 领域的总体趋势，最近在实体和关系抽取基准测试中报告的定量改进至少部分归因于使用了越来越大的预训练语言模型（Language Models, LMs），如 BERT 来获得上下文词表示。同时，人们意识到需要新的评估协议，以更好地理解所获得的神经网络模型的优缺点，而不仅仅是对一个保留测试集上的单一整体指标。&lt;/p&gt;
&lt;p&gt;特别是，对未见数据的泛化是评估深度神经网络的关键因素。在涉及提取提及的IE任务中，这一点尤为重要：小范围的词语可能会同时出现在评估和训练数据集中。已证明这种词汇重叠与NER中神经网络的性能相关。对于流水线 RE，神经模型过度依赖候选参数的类型或其上下文中存在的特定触发词。&lt;/p&gt;
&lt;p&gt;在端到端关系抽取中，我们可以预期这些 NER 和 RE 会结合在一起。在这项工作中，我们认为当前的评估基准不仅衡量了从文本中提取信息的能力，还衡量了模型在训练期间简单保留标记的（头、谓词、尾）三元组的能力。当模型在训练期间看到的句子上进行评估时，很难区分这两种行为中的哪一种占主导地位。&lt;/p&gt;
&lt;p&gt;然而，我们可以假设模型可以简单地检索先前看到的信息，像一个被压缩的知识库一样，通过相关查询进行探测。因此，在包含过多已见三元组的示例上进行测试可能会高估模型的泛化能力。&lt;/p&gt;
&lt;p&gt;即使没有标记数据，LMs也能够学习一些单词之间的关系，可以通过填空句子进行探测，其中一个参数被掩盖。&lt;/p&gt;
&lt;h2 id=&#34;datasets-and-models&#34;&gt;Datasets and Models&lt;/h2&gt;
&lt;p&gt;数据集选用了 CoNLL04、ACE05、SciERC。&lt;/p&gt;
&lt;p&gt;模型选用了三个模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PURE：Pipeline 模型&lt;/li&gt;
&lt;li&gt;SpERT：Joint 模型&lt;/li&gt;
&lt;li&gt;Two are better than one（TABTO）：Joint 模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;partitioning-by-lexical-overlap基于词汇重叠的划分&#34;&gt;Partitioning by Lexical Overlap（基于词汇重叠的划分）&lt;/h2&gt;
&lt;p&gt;我们根据与训练集的词汇重叠情况对测试集中的实体提及进行划分。我们区分了已见和未见的提及，并将这种划分扩展到关系上。&lt;/p&gt;
&lt;p&gt;我们实现了一个简单的保留启发式方法（Retention Heuristic，启发式方法），将训练集中确切存在的实体提及或关系标记为其多数标签。我们在表1中报告了 NER 和 RE 的 Micro-avg. 精度、召回率和 F1 分数。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Better Few-Shot Relation Extraction with Label Prompt Dropout》笔记</title>
      <link>http://localhost:1313/posts/papernotes/better_few-shot_relation_extraction_with_label_prompt_dropout/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/better_few-shot_relation_extraction_with_label_prompt_dropout/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2210.13733&#34;&gt;2210.13733] Better Few-Shot Relation Extraction with Label Prompt Dropout (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted EMNLP 2022.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;在这项工作中，我们提出了一种称为标签提示丢弃（&lt;strong&gt;L&lt;/strong&gt;abel &lt;strong&gt;P&lt;/strong&gt;rompt &lt;strong&gt;D&lt;/strong&gt;ropout, LPD）的新方法。我们直接将文本标签和上下文句子连接在一起，并将它们一起输入到 Transformer Encoder 中。文本标签作为标签提示，通过自注意力机制引导和规范 Transformer Encoder 输出标签感知的关系表示。在训练过程中，我们随机丢弃提示标记，使模型必须学会在有和没有关系描述的情况下工作。实验表明，我们的方法在两个标准的FSRE数据集上取得了显著的改进。我们进行了广泛的消融研究，以证明我们方法的有效性。此外，我们强调了先前研究工作评估设置中的一个潜在问题，即预训练数据中包含的关系类型实际上与测试集中的关系类型重叠。我们认为这对于少样本学习来说可能不是一个理想的设置，并表明现有工作的性能提升可能部分归因于这种“知识泄漏”问题。我们建议过滤掉预训练数据中所有重叠的关系类型，并进行更严格的少样本评估。总之，我们做出了以下贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们提出了 LPD，一种新的标签提示丢弃方法，使 FSRE 中的文本标签得到了更好的利用。这种简单的设计显著优于以前使用复杂网络结构将文本标签和上下文句子融合的方法。&lt;/li&gt;
&lt;li&gt;我们识别了文献中先前实验设置的局限性，并提出了一个更严格的FSRE评估设置。对于这两种设置，我们都显示出比以前的最先进方法更强的改进。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;few-shot-relation-extraction&#34;&gt;Few-Shot Relation Extraction&lt;/h3&gt;
&lt;h3 id=&#34;prompt-based-fine-tuning&#34;&gt;Prompt-Based Fine-Tuning&lt;/h3&gt;
&lt;p&gt;基于提示的模型在小样本和零样本学习中表现出色。这一研究方向的模型尝试将下游微调任务与预训练的掩码语言建模目标对齐，以更好地利用预训练语言模型的潜在知识。&lt;/p&gt;
&lt;p&gt;然而，与许多其他自然语言处理任务（如二元情感分析中的“正面/负面”）的标签语义直观不同，关系抽取中的关系类型可能非常复杂，通常需要较长的句子来描述。例如，FewRel 中的关系 P2094 被描述为“由监管机构进行的官方分类，主体（事件、团队、参与者或设备）符合纳入标准”。基于提示的模型在这种情况下会遇到困难，因为它们需要固定的模板（例如，提示模板中的 [MASK] 令牌数量必须固定）。以前的方法不得不依赖手动设计的提示模板，并使用关系名称而不是关系描述。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，我们提出直接使用整个关系描述作为提示，而不使用任何掩码令牌。在传统的基于提示的模型中，提示用于创建自然描述，以便模型可以在 [MASK] 位置进行更好的预测，而本研究中使用的标签提示通过自然描述来帮助规范模型输出更好的类别表示。&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Better_Few-Shot_Relation_Extraction_with_Label_Prompt_Dropout/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h3 id=&#34;training-with-label-prompt-dropout&#34;&gt;Training with Label Prompt Dropout&lt;/h3&gt;
&lt;p&gt;对于每个支持实例，我们直接将关系描述和上下文句子用“:”连接起来。例如，句子“北京举办了2022年冬季奥运会”将变成“事件地点: 北京举办了2022年冬季奥运会。” 这个想法是创建一个自然的实例，其中定义首先给出，然后是例子。关系描述和冒号作为标签提示，引导 Transformer Encoder 输出一个标签感知的关系表示。为了防止模型完全依赖标签提示而忽略上下文句子，标签提示会以 $α_{train}$ 的概率随机丢弃。例如，上图中的支持实例“十进制数最早在印度发展起来”保持其初始形式，因为其标签提示被丢弃了。对于查询实例，我们直接输入句子而不带任何标签提示。这是因为查询集本质上与测试集相同，我们不应假设可以访问真实知识。随后，使用特殊实体标记来标记头部和尾部，并在句子的前后添加特殊的分类和分隔标记，例如“[CLS] 事件地点: [E1] 北京 [/E1] 举办了 [E2] 2022年冬季奥运会 [/E2]。” 解析后的句子然后被送入Transformer Encoder。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors》笔记</title>
      <link>http://localhost:1313/posts/papernotes/making_pre-trained_language_models_better_continual_few-shot_relation_extractors/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/making_pre-trained_language_models_better_continual_few-shot_relation_extractors/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2402.15713&#34;&gt;2402.15713] Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted COLING 2024&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;COLING: CCF B&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;关系抽取是自然语言处理领域中的一个基本且重要的任务，旨在从句子或文档中提取实体之间的潜在关系。传统的 RE 方法在大量标注样本上训练模型，然后在具有相同标签空间的数据上进行测试。然而，在现实生活中，新关系不断涌现，这些模型在适应新关系时可能会出现显著的性能下降。此外，这些模型严重依赖于大量标注数据，这需要大量时间和精力来收集。&lt;/p&gt;
&lt;p&gt;因此，提出了&lt;strong&gt;持续少样本关系抽取（Continual Few-shot Relation Extraction, CFRE）&lt;/strong&gt;，其目标是在有限的标注数据约束下，持续学习新关系的同时保留先前学习的关系知识。这一实际任务带来了两个重大挑战：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;灾难性遗忘&lt;/strong&gt;：模型在学习新任务时突然忘记从前任务中获得的知识。最新研究指出，即使在大型语言模型中也存在灾难性遗忘问题，这使得这一问题值得研究。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;过拟合&lt;/strong&gt;：模型在训练数据上表现异常好，但由于拟合噪声或无关模式，无法有效泛化到未见数据，这在训练数据稀少的低资源场景中更为明显。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结一下，我们的主要贡献包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们利用提示学习来探索预训练语言模型（PLM）的隐含能力，并提出了 &lt;strong&gt;C&lt;/strong&gt;ontrastive &lt;strong&gt;P&lt;/strong&gt;rompt &lt;strong&gt;L&lt;/strong&gt;earning framework (CPL) 框架，将其与一种新的基于边际的对比学习目标（CFRL）结合起来，同时缓解灾难性遗忘和过拟合问题。&lt;/li&gt;
&lt;li&gt;我们通过利用大型语言模型（LLM）的力量引入了一种记忆增强策略，以提升较小的 PLM。这种策略使用精心设计的提示来指导 ChatGPT 生成样本，从而更好地对抗过拟合。&lt;/li&gt;
&lt;li&gt;在两个 RE 基准上的大量实验表明，我们的方法优于最先进的模型，证明了缓解灾难性遗忘和过拟合的有效性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;h3 id=&#34;continual-learning&#34;&gt;Continual Learning&lt;/h3&gt;
&lt;p&gt;持续学习（Continual Learning, CL）旨在从一系列任务中不断学习新知识，同时避免遗忘旧知识。CL的主要挑战是灾难性遗忘。&lt;/p&gt;
&lt;p&gt;现有的CL方法分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;正则化方法&lt;/strong&gt;：使用额外的约束来限制参数更新，使模型能够记住更多旧知识。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动态架构方法&lt;/strong&gt;：动态扩展模型架构，以在任务序列不断出现时存储新知识。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于记忆的方法&lt;/strong&gt;：存储当前任务的一些典型样本，并在学习任务序列后重放记忆以复习旧知识。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这些方法中，基于记忆的方法在自然语言处理（NLP）任务中最为有效。然而，新任务的数据并不总是充足的，而且获取高质量数据往往既昂贵又耗时。我们也采用基于记忆的策略，但我们更注重如何更好地利用预训练语言模型（PLMs）来解决 CFRE。&lt;/p&gt;
&lt;h3 id=&#34;prompt-learning&#34;&gt;Prompt Learning&lt;/h3&gt;
&lt;p&gt;提示学习随着GPT-3系列的诞生而出现，并在自然语言处理任务中取得了显著的性能，尤其是在小样本场景中。它通过添加提示词将下游任务重新表述为预训练任务，并引导预训练语言模型（PLMs）理解各种任务。之前的提示学习方法可以分为三类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;硬提示&lt;/strong&gt;：在句子中添加手工制作的提示词，并将其转换为掩码语言建模问题。尽管有效，但它需要针对不同任务的复杂专家知识，这既繁琐又耗时。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;软提示&lt;/strong&gt;：在句子中添加可连续训练的向量，这些向量可以被模型自动学习。然而，在没有任何先验专家知识的情况下，模型并不总能学到合适的提示，尤其是在低资源场景中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;混合提示&lt;/strong&gt;：结合不可调的硬提示和可调的软提示，使模型能够在少量人工干预下轻松学习合适的模板。它被验证为最有效的方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们专注于少样本设置，并采用混合提示来帮助预训练语言模型（PLMs）缓解灾难性遗忘和过拟合问题。&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;h3 id=&#34;framework-overview&#34;&gt;Framework Overview&lt;/h3&gt;
&lt;p&gt;整个 CPL 框架有三个模块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prompt Representation，Contrastive Learning，Memory Augmentation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Making_Pre-trained_Language_Models_Better_Continual_Few-Shot_Relation_Extractors/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning》笔记</title>
      <link>http://localhost:1313/posts/papernotes/efficient_information_extraction_in_few-shot_relation_classification_through_contrastive_representation_learning/</link>
      <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/efficient_information_extraction_in_few-shot_relation_classification_through_contrastive_representation_learning/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2403.16543&#34;&gt;2403.16543] Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted NAACL 2024.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;关系分类（Relation Classification, RC）是关系抽取中的一个重要子任务，主要关注在给定文本上下文中识别实体对之间的关系类型。为了实现这一目标，RC 模型必须从句子中提取丰富的信息，包括上下文线索、实体属性和关系特征。虽然语言模型在提取文本表示方面重要，但它们在句子表示中的向量空间使用并不理想。为了改进这一点，最近的研究通过各种技术增强了句子表示。&lt;/p&gt;
&lt;p&gt;关系抽取在许多关系类型上面临数据有限的挑战，并且数据获取成本不成比例。为了解决这一挑战，通过小样本 RC 训练模型以快速适应新关系类型，仅使用少量标记示例。&lt;/p&gt;
&lt;p&gt;由于区分各种关系类型的内在复杂性，RC 应用通常将实体标记令牌的表示作为句子表示。最近的工作在少样本 RC 中使用对比学习以获得更具辨别力的表示。此外，研究表明，通过提示使用 [MASK] 令牌表示句子可以改善句子表示。&lt;/p&gt;
&lt;p&gt;本文贡献如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;新方法&lt;/strong&gt;：我们引入了一种使用对比学习对齐多重表示的方法，用于小样本关系分类。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;适应性&lt;/strong&gt;：我们的方法能够适应各种资源限制，并扩展到包括关系描述在内的额外信息源。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源效率&lt;/strong&gt;：我们强调了该方法的资源效率，提升了在低资源环境下的性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;实体标记：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;实体标记技术通过在输入句子中添加标记来指示文本中的实体。例如，句子“他在2006年世界杯上为墨西哥效力”可以被标记为“他在[E1S]2006年世界杯[E1E]上为[E2S]墨西哥[E2E]效力”。&lt;/li&gt;
&lt;li&gt;在 BERT 编码器中，句子的表示是通过连接实体开始标记的表示来构建的。这种方法增强了模型对实体及其关系的理解。&lt;/li&gt;
&lt;li&gt;这种技术有助于模型更好地捕捉句子中的上下文线索、实体属性和关系特征，从而提高关系分类的准确性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;对比学习：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对比学习是一种用于增强模型表示能力的方法，特别是在少样本关系分类任务中。&lt;/li&gt;
&lt;li&gt;对比学习的主要目标是使相似的样本在表示空间中更接近，而使不相似的样本更远离。&lt;/li&gt;
&lt;li&gt;在训练过程中，对比学习会创建正样本对（相似样本）和负样本对（不相似样本），并通过优化模型使正样本对的表示更接近，负样本对的表示更远。而表现在损失函数中，对比学习的损失函数旨在最大化同一输入句子不同表示之间的相似性，同时最小化不同输入句子表示之间的相似性。&lt;/li&gt;
&lt;li&gt;在少样本关系分类中，对比学习通过对齐多个句子表示（如[CLS]标记、[MASK]标记和实体标记）来提取补充的判别信息，从而提高模型在低资源环境中的表现。&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;方法一览：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Efficient_Information_Extraction_in_Few-Shot_Relation_Classification_through_Contrastive_Representation_Learning/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h3 id=&#34;sentence-representations&#34;&gt;Sentence Representations&lt;/h3&gt;
&lt;p&gt;使用了&lt;strong&gt;平均池化&lt;/strong&gt;从BERT编码器生成各种句子表示。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过平均 token 表示来计算句子表示。同时，BERT-Base 编码器预训练期间使用 [CLS] 标记作为句子表示，捕捉整个输入序列的信息。实体标记技术通过在文本中标记实体来增强输入句子。这将输入增强为 $x = [x_0, …, [E1S], x_i, [E1E], …, x_n]$ 。句子表示通过连接实体开始标记表示 [E1S] 和 [E2S] 构建。&lt;/li&gt;
&lt;li&gt;在 Prompt 方法中，RC 任务被重新表述为掩码语言建模问题。使用模板 T，每个输入被转换为包含至少一个 [MASK] 标记的 $x_{prompt} = T(x)$ 。这个掩码标记表示关系标签，并从上下文中预测，例如 $\hat x = [MASK]: x$​ 。&lt;/li&gt;
&lt;li&gt;使用 dropout 掩码生成增强句子表示的方法。由于实体标记表示不适用于关系描述，我们使用提示和[CLS]表示，并使用不同的dropout掩码。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Prompt-Mask Method&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Entity Concept-enhanced Few-shot Relation Extraction》笔记</title>
      <link>http://localhost:1313/posts/papernotes/entity_concept-enhanced_few-shot_relation_extraction/</link>
      <pubDate>Wed, 18 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/entity_concept-enhanced_few-shot_relation_extraction/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.02401&#34;&gt;2106.02401 (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted ACL 2021。&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;小样本关系抽取（FSRE）大致可以分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;仅使用纯文本数据，不包含外部信息。如：Siamese、Prototypical、BERT-PAIR&lt;/li&gt;
&lt;li&gt;引入外部信息，以补偿 FSRE 的信息不足，如：TD-Proto&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然引入文本描述的知识可以为 FSRE 提供外部信息并实现最先进的性能，但 TD-Proto 仅为每个实体引入一个 Wikidata 中的文本描述。然而，这可能会因为实体和文本描述之间的不匹配而导致性能下降。此外，由于每个实体的文本描述通常较长，提取长文本描述中最有用的信息并不容易。&lt;/p&gt;
&lt;p&gt;与长文本描述相比，概念是对实体的直观和简洁的描述，可以从概念数据库（如 YAGO3、ConceptNet 和 Concept Graph 等）中轻松获得。此外，概念比每个实体的具体文本描述更抽象，这是对 FSRE 场景中有限信息的理想补充。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Entity_Concept-enhanced_Few-shot_Relation_Extraction/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;为了应对上述挑战，我们提出了一种新的实体概念增强的少样本关系抽取方案（&lt;strong&gt;CONCEPT&lt;/strong&gt;-enhanced &lt;strong&gt;FE&lt;/strong&gt;w-shot &lt;strong&gt;R&lt;/strong&gt;elation &lt;strong&gt;E&lt;/strong&gt;xtraction，&lt;strong&gt;ConceptFERE&lt;/strong&gt;），该方案引入实体概念以提供有效的关系预测线索。首先，如上表所示，一个实体可能有多个来自不同方面或层次的概念，只有一个概念可能对最终的关系分类有价值。因此，我们设计了一个概念-句子注意力模块，通过比较句子和每个概念的语义相似性来选择最合适的概念。其次，由于句子嵌入和预训练的概念嵌入不是在同一语义空间中学习的，我们采用自注意力机制对句子和选定概念进行词级语义融合，以进行最终的关系分类。&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;下图为 ConceptFERE 的结构。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Entity_Concept-enhanced_Few-shot_Relation_Extraction/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h3 id=&#34;system-overview&#34;&gt;System Overview&lt;/h3&gt;
&lt;p&gt;Sentence Representation Module 使用 BERT 作为 Encoder 来获取 Sentence Embedding，Concept Representation Module 使用 skip-grim 在 Wikipedia 文本和概念图上学习概念的表示，得到 Concept Embedding 。Relation Classifier 采用全连接层实现。&lt;/p&gt;
&lt;h3 id=&#34;concept-sentence-attention-module&#34;&gt;Concept-Sentence Attention Module&lt;/h3&gt;
&lt;p&gt;直观上，需要更多关注与句子语义相关度高的概念，这些概念可以为关系抽取提供更有效的线索。&lt;/p&gt;
&lt;p&gt;首先，由于预训练的 Sentence Embedding $v_s$ 和 Concept Embedding  $v_c$ 不是在同一语义空间中学习的，因此不能直接比较语义相似度。所以通过将 $v_c$ 和 $v_s$ 乘以投影矩阵 $P$ 来进行语义转换，以在同一语义空间中获得它们的表示 $v_cP$ 和 $v_sP$ ，其中 $P$ 可以通过全连接网络学习。其次，通过计算句子和每个实体概念之间的语义相似度，得到 $v_c$ 和 $v_s$ 的点积作为相似度 $sim_{cs}$ 。最后，为了从计算的相似度值中选择合适的概念，我们设计了 01-GATE 。相似度值通过 Softmax 函数归一化。如果 $sim_{cs}$ 小于设定的阈值 $α$，01-GATE 将为相应概念的注意力分数分配 0，该概念将在后续的关系分类中被排除。我们选择注意力分数为 1 的合适概念，作为参与关系预测的有效线索。&lt;/p&gt;</description>
    </item>
    <item>
      <title>RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction</title>
      <link>http://localhost:1313/posts/papernotes/rapl/</link>
      <pubDate>Tue, 17 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/rapl/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2310.15743&#34;&gt;2310.15743] RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted EMNLP 2023.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;EMNLP：CCF B&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;related-works&#34;&gt;Related Works&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;这一部分内容本来是在论文的最后面，但是考虑到这篇论文也算是打开了新世界的大门，所以把这个放在最前面。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;关系抽取（Relation Extraction，RE）大致可以分为三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;语句级 RE（Sentence-Level RE）&lt;/strong&gt;：早期的研究主要集中在预测单个句子内两个实体之间的关系。各种基于模式和神经网络的方法在句子级关系抽取上取得了令人满意的结果。然而，句子级关系抽取在抽取范围和规模上有显著的局限性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;可以说是早期的 RE 大多是这一类别。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;文档级 RE  （Document-Level RE，DocRE）&lt;/strong&gt;：现有的大多数文档级关系抽取研究都基于数据驱动的监督场景，通常分为基于图和基于序列的方法。基于图的方法通常通过图结构抽象文档，并使用图神经网络进行推理。基于序列的方法则使用仅包含变压器的架构来编码长距离的上下文依赖关系。这两类方法在文档级关系抽取中都取得了令人印象深刻的结果。然而，这些方法对大规模标注文档的依赖使得它们难以适应低资源场景。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;小样本文档级 RE （Few-Shot Document-Level RE，FSDLRE）&lt;/strong&gt;：为了应对现实世界文档级关系抽取场景中普遍存在的数据稀缺问题，&lt;a href=&#34;https://aclanthology.org/2022.naacl-main.421/&#34;&gt;Popovic等&lt;/a&gt;将文档级关系抽取任务形式化为小样本学习任务。为了完成这一任务，他们提出了多种基于度量的模型，这些模型建立在最先进的监督文档级关系抽取方法和少样本句子级关系抽取方法的基础上，旨在解决不同任务设置的问题。有效的基于度量的少样本文档级关系抽取方法的每个类别的原型应该准确捕捉相应的关系语义。然而，由于现有方法的粗粒度关系原型学习策略和”一刀切”的 NOTA 原型学习策略，这对现有方法来说是一个挑战。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;术语解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;原型学习（Prototype-Based Learning）是一种通过存储一组代表性样本（原型）来进行分类、回归或聚类的学习方法。原型学习的主要步骤包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;选择原型&lt;/strong&gt;：从训练数据中选择一组代表性的样本作为原型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;计算距离&lt;/strong&gt;：使用距离度量（如欧氏距离、曼哈顿距离等）来确定测试样本与原型之间的相似性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分类或聚类&lt;/strong&gt;：将测试样本分配给最接近的原型，从而确定其所属的类别或簇。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;NOTA Prototype&lt;/strong&gt; 在本文中指的是 &lt;strong&gt;“None-Of-The-Above”&lt;/strong&gt; 原型，用于处理那些不属于任何目标关系类型的实体对。以下是其主要特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;任务特定&lt;/strong&gt;：每个任务生成特定的 NOTA 原型，以更好地捕捉该任务中的 NOTA 语义。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;基础原型&lt;/strong&gt;：引入一组可学习的基础 NOTA 原型，这些原型需要在每个任务中进一步修正。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;支持实例选择&lt;/strong&gt;：从支持文档中选择 NOTA 实例，并将其与基础NOTA原型融合，生成最终的任务特定NOTA 原型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;语义捕捉&lt;/strong&gt;：通过这种方法，NOTA 原型不仅包含了元学习的通用知识，还能捕捉每个任务中的特定NOTA 语义。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;FSDLRE 任务的简单描述：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/RAPL/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>《实体关系抽取方法研究综述》笔记</title>
      <link>http://localhost:1313/posts/papernotes/%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E6%9D%8E%E5%86%AC%E6%A2%85/</link>
      <pubDate>Wed, 14 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E6%9D%8E%E5%86%AC%E6%A2%85/</guid>
      <description>&lt;h2 id=&#34;关系抽取-relation-extraction&#34;&gt;关系抽取 Relation Extraction&lt;/h2&gt;
&lt;p&gt;定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通常将实体间的关系形式化地描述为关系三元组 $\{E_1,R,E_2\}$ ，其中 $E$ 为实体类型，$R$ 为关系描述类型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关系抽取与命名实体识别、关系触发词识别构成一个端到端任务的框架：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;命名实体识别 Name Entity Recognition：是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等；&lt;/li&gt;
&lt;li&gt;关系触发词识别 Relation trigger word identification：是指对触发实体关系的词进行分类，识别出是触发词还是非触发词，判定抽取出的关系是正类还是负类。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关系抽取是一个文本分类问题，相比于情感分类等任物，其具有以下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;领域众多，关系模型构建复杂。由于限定了关系类别，可采用基于规则、词典以及本体的方法，也可采用传统机器学习的有监督、半监督以及无监督方法，深度学习的有监督、远程监督方法。这类方法的模型构建难度相对于开放领域难度较低，但是移植性和扩展性较差。而针对开放领域的关系抽取，由于关系类型多样且不确定，可以采用无监督和远程监督等方法&lt;/li&gt;
&lt;li&gt;数据来源广泛，主要有结构化、半结构化、无结构3类。针对表格文档、数据库数据等结构化数据，方法众多，现通常采用深度学习相关的方法等；针对纯文本的无结构数据，由于无法预料全部关系类型，一般采用以聚类为核心的无监督方法等；而针对维基百科、百度百科等半结构化数据，通常采用半监督和远程监督方法等&lt;/li&gt;
&lt;li&gt;关系种类繁多复杂，噪音数据无法避免。实体之间的关系多样，有一种或多种关系，早期方法主要针对一种关系（忽略重叠关系）进行抽取，这类方法忽略了实体间的多种关系，对实体间的潜在关系难以处理。近年来，图结构逐渐应用于关系抽取领域，为关系重叠和实体重叠提供了新思路。而针对噪音数据，有人发现少量对抗样本会避免模型过拟合，提出使用对抗训练提高模型的性能&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;评价指标用 Precision、Recall、F1.&lt;/p&gt;
&lt;h2 id=&#34;基于深度学习的关系抽取方法&#34;&gt;基于深度学习的关系抽取方法&lt;/h2&gt;
&lt;h3 id=&#34;监督学习pipeline&#34;&gt;监督学习：Pipeline&lt;/h3&gt;
&lt;p&gt;常见的模型有CNN、RNN、LSTM/BiLSTM、GCN（图神经网络）、混合抽取（模型融合）。&lt;/p&gt;
&lt;h3 id=&#34;监督学习joint&#34;&gt;监督学习：Joint&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;基于共享参数的方法：命名实体识别和关系抽取通过共享编码层在训练过程中产生的共享参数相互依赖，最终训练得到最佳的全局参数。因此，基于共享参数方法有效地改善了流水线方法中存在的错误累积传播问题和忽视2个子任务间关系依赖的问题，提高模型的鲁棒性&lt;/li&gt;
&lt;li&gt;基于序列标注的方法：由于基于共性参数的方法容易产生信息冗余，因此可以将命名实体识别和实体关系抽取融合成一个序列标注问题，可以同时识别出实体和关系。该方法利用一个端到端的神经网络模型抽取出实体之间的关系三元组，减少了无效实体对模型的影响，提高了关系抽取的召回率和准确率&lt;/li&gt;
&lt;li&gt;基于图结构的方法：针对前2种方法无法解决的实体重叠、关系重叠问题，基于图结构的方法能有效得解决&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;远程监督学习&#34;&gt;远程监督学习&lt;/h3&gt;
&lt;p&gt;远程监督学习（Distant Supervision）是一种基于外部知识的监督学习方法，主要用于自动标注大规模文本数据，以训练关系抽取模型。其核心思想是利用已知的关系图谱（如知识图谱）来标注文本数据。例如，如果两个实体在知识图谱中存在关系，那么包含这两个实体的句子就可以被认为是该关系的正例。&lt;/p&gt;
&lt;p&gt;远程监督的实体关系抽取方法极大地减少了对人工的依赖，可以自动地抽取大量的实体对，从而扩大了知识库的规模。&lt;/p&gt;
&lt;p&gt;然而这类方法在数据标注过程会带来2个问题：噪音数据和抽取特征的误差传播。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于远程监督的基本假设，海量数据的实体对的关系会被错误标记，从而产生了噪音数据&lt;/li&gt;
&lt;li&gt;由于利用自然语言处理工具抽取的特征也存在一定的误差，会引起特征的传播误差和错误积累&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;bert&#34;&gt;BERT&lt;/h3&gt;
&lt;h2 id=&#34;基于开放领域的关系抽取方法&#34;&gt;基于开放领域的关系抽取方法&lt;/h2&gt;
&lt;p&gt;由于传统关系抽取基于特定领域、特定关系进行抽取，导致关系抽取这一任务耗时耗力，成本极高，同时不利于扩展语料类型。近年来，针对开放领域的实体关系抽取方法逐渐受到人们的广泛关注。由于互联网不断发展，开放语料的规模不断扩大，并且包含的关系类型愈加复杂，研究者直接面向大多未经人工标注的开放语料进行关系抽取，有利于促进实体关系抽取的发展，而且具有更大的实际意义。&lt;/p&gt;
&lt;p&gt;开放领域关系抽取的方法是信息抽取领域的新的研究方向。该关系抽取方法主要分为半监督和无监督2种，并结合语形特征和语义特征自动地在大规模非限定类型的语料库中进行关系抽取。开放领域关系抽取的方法无需事先人为制定关系类型，减轻了人工标注的负担，而由此设计的系统可移植性较强，极大地促进关系抽取的发展。&lt;/p&gt;
&lt;p&gt;开放领域的关系抽取方法主要有3个流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;深层解析小规模的语料集，自动抽取实体间关系三元组，利用朴素贝叶斯分类器训练已标注可信和不可信的关系三元组构建关系表示模型；&lt;/li&gt;
&lt;li&gt;利用关系抽取模型并输入词性、序列等特征等数据，在训练好的分类器上进行大量网络文献的关系抽取，获取候选关系三元组；&lt;/li&gt;
&lt;li&gt;合并候选三元组，通过统计的方法计算各个关系三元组的可信度，并建立索引。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本文链接&#34;&gt;本文链接&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://crad.ict.ac.cn/fileJSJYJYFZ/journal/article/jsjyjyfz/HTML/2020-7-1424.shtml&#34;&gt;实体关系抽取方法研究综述 (ict.ac.cn)&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>《基于MRC的设备故障命名实体识别方法》笔记</title>
      <link>http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8Emrc%E7%9A%84%E8%AE%BE%E5%A4%87%E6%95%85%E9%9A%9C%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95_%E5%BE%90%E9%B9%8F/</link>
      <pubDate>Tue, 06 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8Emrc%E7%9A%84%E8%AE%BE%E5%A4%87%E6%95%85%E9%9A%9C%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95_%E5%BE%90%E9%B9%8F/</guid>
      <description>&lt;h2 id=&#34;mrc&#34;&gt;MRC&lt;/h2&gt;
&lt;p&gt;本论文关于设备故障领域的NER任务，将 NER 的序列标注问题重构为 MRC 问题。&lt;/p&gt;
&lt;p&gt;MRC（Machine Reading Comprehension）机器阅读理解，与文生成和机器翻译任务类似。本论文使用该方法，根堆每个实体类型，设计了相应地自然语言形式的查询，通过上下文的查询来定位并提取实体。例如，针对故障代码 the machine suffers from error[#10482] 的命名实体识别，被形式化为 “文本中提到了哪种故障代码 ” 。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;人话：先通过某个文生成/机器翻译模型将错误代码转化为中文，然后通过预定的错误代码进行映射&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文的主要工作如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 NER 任务转化为 MRC 问题，采用自然语言查询来实现命名实体的识别。&lt;/li&gt;
&lt;li&gt;通过在查询中集成预设的命名实体类别信息，克服传统方法中实体类别语义信息缺少的限制，提升命名实体识别的准确性。&lt;/li&gt;
&lt;li&gt;结合设备故障数据集的构建，开展相关的实 验对比，并进行定量分析，证明了本文方法的有效性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;模型结构&#34;&gt;模型结构&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8eMRC%e7%9a%84%e8%ae%be%e5%a4%87%e6%95%85%e9%9a%9c%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e6%96%b9%e6%b3%95_%e5%be%90%e9%b9%8f/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;编码层为 ALBERT&lt;/li&gt;
&lt;li&gt;特征提取为 BiLSTM&lt;/li&gt;
&lt;li&gt;分类器为两个sigmoid，分别预测实体的起始索引、结束索引&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;数据集&#34;&gt;数据集&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8eMRC%e7%9a%84%e8%ae%be%e5%a4%87%e6%95%85%e9%9a%9c%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e6%96%b9%e6%b3%95_%e5%be%90%e9%b9%8f/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;对比试验&#34;&gt;对比试验&lt;/h2&gt;
&lt;p&gt;模型为以下几类：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8eMRC%e7%9a%84%e8%ae%be%e5%a4%87%e6%95%85%e9%9a%9c%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e6%96%b9%e6%b3%95_%e5%be%90%e9%b9%8f/img4.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8eMRC%e7%9a%84%e8%ae%be%e5%a4%87%e6%95%85%e9%9a%9c%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e6%96%b9%e6%b3%95_%e5%be%90%e9%b9%8f/img3.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;消融实验&#34;&gt;消融实验&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8eMRC%e7%9a%84%e8%ae%be%e5%a4%87%e6%95%85%e9%9a%9c%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e6%96%b9%e6%b3%95_%e5%be%90%e9%b9%8f/img5.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;</description>
    </item>
    <item>
      <title>《基于预训练模型的医药说明书实体抽取方法研究》笔记</title>
      <link>http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BB%E8%8D%AF%E8%AF%B4%E6%98%8E%E4%B9%A6%E5%AE%9E%E4%BD%93%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6_%E9%99%88%E4%BB%B2%E6%B0%B8/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BB%E8%8D%AF%E8%AF%B4%E6%98%8E%E4%B9%A6%E5%AE%9E%E4%BD%93%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6_%E9%99%88%E4%BB%B2%E6%B0%B8/</guid>
      <description>&lt;h2 id=&#34;基于预训练模型的部分标签命名实体识别模型&#34;&gt;基于预训练模型的部分标签命名实体识别模型&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8e%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8c%bb%e8%8d%af%e8%af%b4%e6%98%8e%e4%b9%a6%e5%ae%9e%e4%bd%93%e6%8a%bd%e5%8f%96%e6%96%b9%e6%b3%95%e7%a0%94%e7%a9%b6_%e9%99%88%e4%bb%b2%e6%b0%b8/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;对于输入的药品说明书文本先利用少量样本微调的预训练模型进行实体抽取，如上图所示，“云南白药成分：三七、麝香、草乌等”通过预训练模型识别出 “麝香”和“草乌”两个“成分”类型的实体。由于受标注样本数量限制，预训练模型经少量样本微调后，其召回率不高，如例子中“云南白药”“三七”两个实体未能识别，但把预训练模型识别出的部分实体及该实体的类型标签作为一种提示信息输入到后面的部分标签模型，这将有助于部分标签模型进行实体抽取任务。&lt;/p&gt;
&lt;p&gt;部分标签模型采用平面格结构对输入文本及预训练语言模型识别的部分实体进行整合，整合信息包括字符或实体词 token、标签 tag、头部位置标记 head 和尾部位置标记 tail。部分标签模型使用 Transformer 建模平面格结构的编码输入，通过 Transformer 的自注意力机制使字符可以与潜在实体词直接交互。最后将 Transformer 提取特征表示，输入到 CRF层预测实体标签。&lt;/p&gt;
&lt;h2 id=&#34;训练策略&#34;&gt;训练策略&lt;/h2&gt;
&lt;p&gt;对整体模型训练分两阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用少量完全标注的语料对预训练模型进行微调，再对这些完全标注语料采用标注样本实体掩盖（mask）策略进行样本数据增广，使用增广后的样本数据集对部分标签模型进行训练；&lt;/li&gt;
&lt;li&gt;预训练模型和部分标签模型进行联合训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;对比实验&#34;&gt;对比实验&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8e%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8c%bb%e8%8d%af%e8%af%b4%e6%98%8e%e4%b9%a6%e5%ae%9e%e4%bd%93%e6%8a%bd%e5%8f%96%e6%96%b9%e6%b3%95%e7%a0%94%e7%a9%b6_%e9%99%88%e4%bb%b2%e6%b0%b8/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;尽管 BERT- BILSTMCRF 模型在医疗、政务等领域的命名实体识别得到广泛应用 ，但由于采用 LSTM 神经网络提取字符特征，其效果明显低于 FLAT 和本文模型，而 FLAT、 MECT4CNER 以及本文模型都采用 Transformer 网络提取特征。&lt;/p&gt;
&lt;p&gt;MECT4CNER 是在 FLAT 的基础上结合汉字结构信息与词典信息设计的模型，但本次实验表明 MECT4CNER 应用于药品说明书命名实体识别时， 汉字结构信息未能对提高模型性能带来更多增益， 反而降低了召回率，使得 F1 值比 FLAT 模型更低。 本文模型对 FLAT 模型所提出的平面格结构进行了扩展，增加的标签信息能对提升模型性能带来增益， 从而 F1值取得了较优的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FLAT模型是为中文命名实体识别设计的，它将复杂的字词格结构转换为平坦的结构。每个跨度对应于原始格中的一个字符或潜在词及其位置。FLAT利用Transformer的强大功能和精心设计的位置编码，能够充分利用格信息，并具有出色的并行能力。实验表明，FLAT在性能和效率上都优于其他基于词典的模型。&lt;/p&gt;
&lt;p&gt;MECT4CNER模型结合了字词格和部首流，不仅具有FLAT的词边界和语义学习能力，还增加了汉字部首的结构信息。通过融合汉字的结构特征，MECT能够更好地捕捉汉字的语义信息，从而提高中文NER的性能。&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>《中文命名实体识别研究综述》笔记</title>
      <link>http://localhost:1313/posts/papernotes/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E8%B5%B5%E7%BB%A7%E8%B4%B5/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E8%B5%B5%E7%BB%A7%E8%B4%B5/</guid>
      <description>&lt;h2 id=&#34;中文ner的难点&#34;&gt;中文NER的难点&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;词边界模糊。中文没有像英文等语言一样使用空格或其他分隔符来表示词边界，这种特点导致中文命名实体识别面临着边界歧义和识别困难的问题。例如，“计算机科学与技术系”中“计算机科学与技术”是一个 复合词，边界不明确。&lt;/li&gt;
&lt;li&gt;语义多样化。中文存在大量多义词，一个词汇可能会被用于不同的上下文中表示不同的含义。&lt;/li&gt;
&lt;li&gt;形态特征模糊。在英语中，一些指定类型的实体的第一个字母通常是大写的，例如指定人员或地点的名称。这种信息是识别一些命名实体的位置和边界的明确特征。在中文命名实体识别中缺乏汉语形态的显式特征，增加了识别的难度。&lt;/li&gt;
&lt;li&gt;中文语料库内容较少。命名实体识别需要大量 的标注数据来训练模型，但中文标注数据数量及质量有限，导致命名实体识别模型的训练更为困难。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;cner数据集&#34;&gt;CNER数据集&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e4%b8%ad%e6%96%87%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0_%e8%b5%b5%e7%bb%a7%e8%b4%b5/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;cner标注方式性能指标&#34;&gt;CNER标注方式、性能指标&lt;/h2&gt;
&lt;p&gt;BIO标注、 BMES标注以及BIOSE标注方案。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e4%b8%ad%e6%96%87%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0_%e8%b5%b5%e7%bb%a7%e8%b4%b5/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;性能指标和NER一样，Precision、Recall、Accuracy、F1-score 是常用指标。&lt;/p&gt;
&lt;h2 id=&#34;近几年的cner模型构成&#34;&gt;近几年的CNER模型构成&lt;/h2&gt;
&lt;h3 id=&#34;嵌入层&#34;&gt;嵌入层&lt;/h3&gt;
&lt;h4 id=&#34;基于字符的模型&#34;&gt;基于字符的模型&lt;/h4&gt;
&lt;p&gt;基于字符的模型将单词表示为字符序列的方法，它通过输入文本的字符级别表示，不需要明确的词边界信息，可以更好地处理CNER中的边界模糊问题。为解决相邻字符之间强联系的问题，Zhang 等人[40] 提出一种新的动态嵌入方法，该方法使用注意力机制来 组合嵌入层中的字符和单词向量特征。基于字符的模型存在不能携带语义信息、难以处理歧义词的缺点[42] 。&lt;/p&gt;
&lt;h4 id=&#34;基于词的模型&#34;&gt;基于词的模型&lt;/h4&gt;
&lt;p&gt;基于词的模型是将中文数据集的文本以词语的形 式作为输入，借助分词系统[43] 对数据集进行分词。基于词的模型可以捕捉到词与词之间的语义关系，在处理一些长词汇的实体时具有良好的效果。基于词的模型存在分词错误和在处理不规则的词以及新词时比较困难的缺点。Ma等人[45] 使用双向 LSTM、CNN 和 CRF 的组合，提出一种 中性网络结构，自动从单词和字符级别的表示中获益， 实现了端到端的 NER。&lt;/p&gt;
&lt;h4 id=&#34;混合模型&#34;&gt;混合模型&lt;/h4&gt;
&lt;p&gt;混合模型是将基于字符的模型和基于词的模型结合起来，由于基于字符的模型存在字与字之间语义提取缺失问题，基于词的模型存在分词错误的问题，同时将字符和词作为嵌入表示可以使模型具有较好的鲁棒性和识别精度。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的双向编码（bidirectional encoder representations from Transformer，BERT） 模型[51] 是中文命名实体识别中最常用的预训练模型， BERT模型可以考虑整个输入句子的上下文信息，有助于提高模型对命名实体的理解和识别准确性。对于给定的字符，BERT将其字符位置嵌入、句子位置嵌入和字符嵌入作为输入连接起来，然后使用掩码语言模型[52] 对输入句子进行深度双向表示预训练，以获得强大的上下文字符嵌入。&lt;/p&gt;
&lt;h3 id=&#34;编码层&#34;&gt;编码层&lt;/h3&gt;
&lt;p&gt;编码层主要是将嵌入层输入的文本转换成一个高 维的特征向量，方便后续的分类器对文本进行分类。&lt;/p&gt;
&lt;h4 id=&#34;卷积神经网络&#34;&gt;卷积神经网络&lt;/h4&gt;
&lt;p&gt;CNN最初是为计算 机视觉研究开发的，但它已被证明可以有效地捕获具有卷积运算的 n-gram 的信息语义特征[61] 。&lt;/p&gt;
&lt;h4 id=&#34;循环神经网络&#34;&gt;循环神经网络&lt;/h4&gt;
&lt;p&gt;Quyang 等人[67] 提出一种用于 CNER 的深度学习模型，该模型采用双向 RNN-CRF 架构，使用连接的 n -gram 字符表示来捕获丰富的上下文信息。Dong 等人[37] 将双向 LSTM-CRF 神经网络用于 CNER，该网络同时利用字符级和部首级表示，是第一个研究 BiLSTM-CRF 架构中的中文部首级表示，并且在没有精心设计的功能的情况下获得更好的性能。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
