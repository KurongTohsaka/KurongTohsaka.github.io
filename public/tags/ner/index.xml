<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NER on KurongBlog</title>
    <link>http://localhost:1313/tags/ner/</link>
    <description>Recent content in NER on KurongBlog</description>
    <image>
      <title>KurongBlog</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.135.0</generator>
    <language>en</language>
    <lastBuildDate>Sat, 05 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ner/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>《Distantly-Supervised Joint Extraction with Noise-Robust Learning》笔记</title>
      <link>http://localhost:1313/posts/papernotes/distantly-supervised_joint_extraction_with_noise-robust_learning/</link>
      <pubDate>Sat, 05 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/distantly-supervised_joint_extraction_with_noise-robust_learning/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.04994#:~:text=We%20propose%20DENRL,%20a%20generalizable%20framework%20that%201&#34;&gt;https://arxiv.org/abs/2310.04994#:~:text=We%20propose%20DENRL,%20a%20generalizable%20framework%20that%201&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Accepted by ACL 2024.&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;联合抽取&lt;/strong&gt;旨在使用单一模型检测实体及其关系，这是自动知识库构建中的关键步骤。为了廉价地获取大量标注的联合训练数据，提出了&lt;strong&gt;远程监督（Distantly Supervise，DS）&lt;/strong&gt;，通过将知识库（Knowledge Base，KB）与未标注的语料库对齐，自动生成训练数据。假设如果一个实体对在 KB 中有关系，则包含该对的所有句子都表达相应的关系。&lt;/p&gt;
&lt;p&gt;然而，DS 带来了大量的&lt;strong&gt;噪声标签&lt;/strong&gt;，显著降低了联合抽取模型的性能。此外，由于开放域 KB 中实体的模糊性和覆盖范围有限，DS 还会生成噪声和不完整的实体标签。在某些情况下，DS 可能导致 KB 中包含超过30%的噪声实例，使得学习有用特征变得不可能。&lt;/p&gt;
&lt;p&gt;处理这些噪声标签的先前研究要么考虑弱标注的实体，即远程监督的命名实体识别（NER），要么考虑噪声关系标签，即远程监督的关系抽取（RE），它们专注于设计新颖的手工制作关系特征、神经架构和标注方案以提高关系抽取性能。此外，使用大型语言模型（LLMs）的上下文学习（ICL）也很流行。然而，它们资源需求高，对提示设计敏感，可能在处理复杂任务时表现不佳。&lt;/p&gt;
&lt;p&gt;为了廉价地减轻两种噪声源，我们提出了 &lt;strong&gt;DENRL&lt;/strong&gt; （&lt;strong&gt;D&lt;/strong&gt;istantly-supervised joint &lt;strong&gt;E&lt;/strong&gt;xtraction with &lt;strong&gt;N&lt;/strong&gt;oise-&lt;strong&gt;R&lt;/strong&gt;obust &lt;strong&gt;L&lt;/strong&gt;earning）。DENRL 假设&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可靠的关系标签，其关系模式显著表明实体对之间的关系，应该由模型解释；&lt;/li&gt;
&lt;li&gt;可靠的关系标签也隐含地表明相应实体对的可靠实体标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具体来说，DENRL应用词袋正则化（BR）引导模型关注解释正确关系标签的显著关系模式，并使用基于本体的逻辑融合（OLF）通过概率软逻辑（PSL）教授底层实体关系依赖性。这两种信息源被整合形成噪声鲁棒损失，正则化标注模型从具有正确实体和关系标签的实例中学习。接下来，如果学习到的模型能够清晰地定位关系模式并理解候选实例的实体关系逻辑，它们将被选择用于后续的自适应学习。我们进一步采样包含已识别模式中对应头实体或尾实体的负实例以减少实体噪声。我们迭代学习一个可解释的模型并选择高质量实例。这两个步骤相互强化——更可解释的模型有助于选择更高质量的子集，反之亦然。&lt;/p&gt;
&lt;h2 id=&#34;joint-extraction-architecture&#34;&gt;Joint Extraction Architecture&lt;/h2&gt;
&lt;h3 id=&#34;tagging-scheme&#34;&gt;Tagging Scheme&lt;/h3&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Distantly-Supervised_Joint_Extraction_with_Noise-Robust_Learning/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/Distantly-Supervised_Joint_Extraction_with_Noise-Robust_Learning/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;为了同时抽取实体（提及和类型）和关系，我们为每个起始位置 $p$ 标注四元组 ${e_1, tag_1, e_2, r_e}$，并定义 “BIO” 标记来编码位置。对于一个 $T$ 个 token 的句子，我们根据不同的起始位置标注 $T$ 个不同的标记序列。&lt;/p&gt;
&lt;p&gt;对于每个标记序列，如果 $p$ 是一个实体的起始位置（该序列是一个实例），则在 $p$ 处标注实体类型，并用关系类型标注与 $p$ 处实体有关系的其他实体。其余的令牌标注为 “O”（Outside），表示它们不对应头实体。这样，每个标记序列将生成一个关系四元组。&lt;/p&gt;
&lt;p&gt;我们将包含至少一个关系的实例定义为正实例，没有关系的实例定义为负实例。“BIO”（Begin, Inside, Outside）标记用于指示每个实体中令牌的位置信息，以便同时提取多词实体和关系类型。注意，我们不需要尾实体类型，因为每个实体都会被查询，我们可以从 T 标记序列中获得所有实体类型及其关系。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction》笔记</title>
      <link>http://localhost:1313/posts/papernotes/separating_retention_from_extraction/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/separating_retention_from_extraction/</guid>
      <description>&lt;h2 id=&#34;link&#34;&gt;Link&lt;/h2&gt;
&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2109.12008v1&#34;&gt;2109.12008v1] Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction (arxiv.org)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Accepted at EMNLP 2021&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;信息抽取（Information Extraction, IE）旨在将文本中表达的信息转换为预定义的结构化知识格式。这个总体目标被分解为更容易自动执行和评估的子任务。因此，命名实体识别（Named Entity Recognition, NER）和关系抽取（Relation Extraction, RE）是两个关键的 IE 任务。传统上，这些任务是通过流水线方式执行的。也可以采用联合方式处理，以建模它们的相互依赖性，减少错误传播并获得更现实的评估设置。&lt;/p&gt;
&lt;p&gt;随着 NLP 领域的总体趋势，最近在实体和关系抽取基准测试中报告的定量改进至少部分归因于使用了越来越大的预训练语言模型（Language Models, LMs），如 BERT 来获得上下文词表示。同时，人们意识到需要新的评估协议，以更好地理解所获得的神经网络模型的优缺点，而不仅仅是对一个保留测试集上的单一整体指标。&lt;/p&gt;
&lt;p&gt;特别是，对未见数据的泛化是评估深度神经网络的关键因素。在涉及提取提及的IE任务中，这一点尤为重要：小范围的词语可能会同时出现在评估和训练数据集中。已证明这种词汇重叠与NER中神经网络的性能相关。对于流水线 RE，神经模型过度依赖候选参数的类型或其上下文中存在的特定触发词。&lt;/p&gt;
&lt;p&gt;在端到端关系抽取中，我们可以预期这些 NER 和 RE 会结合在一起。在这项工作中，我们认为当前的评估基准不仅衡量了从文本中提取信息的能力，还衡量了模型在训练期间简单保留标记的（头、谓词、尾）三元组的能力。当模型在训练期间看到的句子上进行评估时，很难区分这两种行为中的哪一种占主导地位。&lt;/p&gt;
&lt;p&gt;然而，我们可以假设模型可以简单地检索先前看到的信息，像一个被压缩的知识库一样，通过相关查询进行探测。因此，在包含过多已见三元组的示例上进行测试可能会高估模型的泛化能力。&lt;/p&gt;
&lt;p&gt;即使没有标记数据，LMs也能够学习一些单词之间的关系，可以通过填空句子进行探测，其中一个参数被掩盖。&lt;/p&gt;
&lt;h2 id=&#34;datasets-and-models&#34;&gt;Datasets and Models&lt;/h2&gt;
&lt;p&gt;数据集选用了 CoNLL04、ACE05、SciERC。&lt;/p&gt;
&lt;p&gt;模型选用了三个模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PURE：Pipeline 模型&lt;/li&gt;
&lt;li&gt;SpERT：Joint 模型&lt;/li&gt;
&lt;li&gt;Two are better than one（TABTO）：Joint 模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;partitioning-by-lexical-overlap基于词汇重叠的划分&#34;&gt;Partitioning by Lexical Overlap（基于词汇重叠的划分）&lt;/h2&gt;
&lt;p&gt;我们根据与训练集的词汇重叠情况对测试集中的实体提及进行划分。我们区分了已见和未见的提及，并将这种划分扩展到关系上。&lt;/p&gt;
&lt;p&gt;我们实现了一个简单的保留启发式方法（Retention Heuristic，启发式方法），将训练集中确切存在的实体提及或关系标记为其多数标签。我们在表1中报告了 NER 和 RE 的 Micro-avg. 精度、召回率和 F1 分数。&lt;/p&gt;</description>
    </item>
    <item>
      <title>《基于预训练模型的医药说明书实体抽取方法研究》笔记</title>
      <link>http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BB%E8%8D%AF%E8%AF%B4%E6%98%8E%E4%B9%A6%E5%AE%9E%E4%BD%93%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6_%E9%99%88%E4%BB%B2%E6%B0%B8/</link>
      <pubDate>Sun, 28 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/%E5%9F%BA%E4%BA%8E%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8C%BB%E8%8D%AF%E8%AF%B4%E6%98%8E%E4%B9%A6%E5%AE%9E%E4%BD%93%E6%8A%BD%E5%8F%96%E6%96%B9%E6%B3%95%E7%A0%94%E7%A9%B6_%E9%99%88%E4%BB%B2%E6%B0%B8/</guid>
      <description>&lt;h2 id=&#34;基于预训练模型的部分标签命名实体识别模型&#34;&gt;基于预训练模型的部分标签命名实体识别模型&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8e%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8c%bb%e8%8d%af%e8%af%b4%e6%98%8e%e4%b9%a6%e5%ae%9e%e4%bd%93%e6%8a%bd%e5%8f%96%e6%96%b9%e6%b3%95%e7%a0%94%e7%a9%b6_%e9%99%88%e4%bb%b2%e6%b0%b8/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;对于输入的药品说明书文本先利用少量样本微调的预训练模型进行实体抽取，如上图所示，“云南白药成分：三七、麝香、草乌等”通过预训练模型识别出 “麝香”和“草乌”两个“成分”类型的实体。由于受标注样本数量限制，预训练模型经少量样本微调后，其召回率不高，如例子中“云南白药”“三七”两个实体未能识别，但把预训练模型识别出的部分实体及该实体的类型标签作为一种提示信息输入到后面的部分标签模型，这将有助于部分标签模型进行实体抽取任务。&lt;/p&gt;
&lt;p&gt;部分标签模型采用平面格结构对输入文本及预训练语言模型识别的部分实体进行整合，整合信息包括字符或实体词 token、标签 tag、头部位置标记 head 和尾部位置标记 tail。部分标签模型使用 Transformer 建模平面格结构的编码输入，通过 Transformer 的自注意力机制使字符可以与潜在实体词直接交互。最后将 Transformer 提取特征表示，输入到 CRF层预测实体标签。&lt;/p&gt;
&lt;h2 id=&#34;训练策略&#34;&gt;训练策略&lt;/h2&gt;
&lt;p&gt;对整体模型训练分两阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;利用少量完全标注的语料对预训练模型进行微调，再对这些完全标注语料采用标注样本实体掩盖（mask）策略进行样本数据增广，使用增广后的样本数据集对部分标签模型进行训练；&lt;/li&gt;
&lt;li&gt;预训练模型和部分标签模型进行联合训练。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;对比实验&#34;&gt;对比实验&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e5%9f%ba%e4%ba%8e%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8c%bb%e8%8d%af%e8%af%b4%e6%98%8e%e4%b9%a6%e5%ae%9e%e4%bd%93%e6%8a%bd%e5%8f%96%e6%96%b9%e6%b3%95%e7%a0%94%e7%a9%b6_%e9%99%88%e4%bb%b2%e6%b0%b8/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;尽管 BERT- BILSTMCRF 模型在医疗、政务等领域的命名实体识别得到广泛应用 ，但由于采用 LSTM 神经网络提取字符特征，其效果明显低于 FLAT 和本文模型，而 FLAT、 MECT4CNER 以及本文模型都采用 Transformer 网络提取特征。&lt;/p&gt;
&lt;p&gt;MECT4CNER 是在 FLAT 的基础上结合汉字结构信息与词典信息设计的模型，但本次实验表明 MECT4CNER 应用于药品说明书命名实体识别时， 汉字结构信息未能对提高模型性能带来更多增益， 反而降低了召回率，使得 F1 值比 FLAT 模型更低。 本文模型对 FLAT 模型所提出的平面格结构进行了扩展，增加的标签信息能对提升模型性能带来增益， 从而 F1值取得了较优的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FLAT模型是为中文命名实体识别设计的，它将复杂的字词格结构转换为平坦的结构。每个跨度对应于原始格中的一个字符或潜在词及其位置。FLAT利用Transformer的强大功能和精心设计的位置编码，能够充分利用格信息，并具有出色的并行能力。实验表明，FLAT在性能和效率上都优于其他基于词典的模型。&lt;/p&gt;
&lt;p&gt;MECT4CNER模型结合了字词格和部首流，不仅具有FLAT的词边界和语义学习能力，还增加了汉字部首的结构信息。通过融合汉字的结构特征，MECT能够更好地捕捉汉字的语义信息，从而提高中文NER的性能。&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>《中文命名实体识别研究综述》笔记</title>
      <link>http://localhost:1313/posts/papernotes/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E8%B5%B5%E7%BB%A7%E8%B4%B5/</link>
      <pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/papernotes/%E4%B8%AD%E6%96%87%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0_%E8%B5%B5%E7%BB%A7%E8%B4%B5/</guid>
      <description>&lt;h2 id=&#34;中文ner的难点&#34;&gt;中文NER的难点&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;词边界模糊。中文没有像英文等语言一样使用空格或其他分隔符来表示词边界，这种特点导致中文命名实体识别面临着边界歧义和识别困难的问题。例如，“计算机科学与技术系”中“计算机科学与技术”是一个 复合词，边界不明确。&lt;/li&gt;
&lt;li&gt;语义多样化。中文存在大量多义词，一个词汇可能会被用于不同的上下文中表示不同的含义。&lt;/li&gt;
&lt;li&gt;形态特征模糊。在英语中，一些指定类型的实体的第一个字母通常是大写的，例如指定人员或地点的名称。这种信息是识别一些命名实体的位置和边界的明确特征。在中文命名实体识别中缺乏汉语形态的显式特征，增加了识别的难度。&lt;/li&gt;
&lt;li&gt;中文语料库内容较少。命名实体识别需要大量 的标注数据来训练模型，但中文标注数据数量及质量有限，导致命名实体识别模型的训练更为困难。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;cner数据集&#34;&gt;CNER数据集&lt;/h2&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e4%b8%ad%e6%96%87%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0_%e8%b5%b5%e7%bb%a7%e8%b4%b5/img1.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;h2 id=&#34;cner标注方式性能指标&#34;&gt;CNER标注方式、性能指标&lt;/h2&gt;
&lt;p&gt;BIO标注、 BMES标注以及BIOSE标注方案。&lt;/p&gt;
&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;http://localhost:1313/img/PaperNotes/%e4%b8%ad%e6%96%87%e5%91%bd%e5%90%8d%e5%ae%9e%e4%bd%93%e8%af%86%e5%88%ab%e7%a0%94%e7%a9%b6%e7%bb%bc%e8%bf%b0_%e8%b5%b5%e7%bb%a7%e8%b4%b5/img2.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;性能指标和NER一样，Precision、Recall、Accuracy、F1-score 是常用指标。&lt;/p&gt;
&lt;h2 id=&#34;近几年的cner模型构成&#34;&gt;近几年的CNER模型构成&lt;/h2&gt;
&lt;h3 id=&#34;嵌入层&#34;&gt;嵌入层&lt;/h3&gt;
&lt;h4 id=&#34;基于字符的模型&#34;&gt;基于字符的模型&lt;/h4&gt;
&lt;p&gt;基于字符的模型将单词表示为字符序列的方法，它通过输入文本的字符级别表示，不需要明确的词边界信息，可以更好地处理CNER中的边界模糊问题。为解决相邻字符之间强联系的问题，Zhang 等人[40] 提出一种新的动态嵌入方法，该方法使用注意力机制来 组合嵌入层中的字符和单词向量特征。基于字符的模型存在不能携带语义信息、难以处理歧义词的缺点[42] 。&lt;/p&gt;
&lt;h4 id=&#34;基于词的模型&#34;&gt;基于词的模型&lt;/h4&gt;
&lt;p&gt;基于词的模型是将中文数据集的文本以词语的形 式作为输入，借助分词系统[43] 对数据集进行分词。基于词的模型可以捕捉到词与词之间的语义关系，在处理一些长词汇的实体时具有良好的效果。基于词的模型存在分词错误和在处理不规则的词以及新词时比较困难的缺点。Ma等人[45] 使用双向 LSTM、CNN 和 CRF 的组合，提出一种 中性网络结构，自动从单词和字符级别的表示中获益， 实现了端到端的 NER。&lt;/p&gt;
&lt;h4 id=&#34;混合模型&#34;&gt;混合模型&lt;/h4&gt;
&lt;p&gt;混合模型是将基于字符的模型和基于词的模型结合起来，由于基于字符的模型存在字与字之间语义提取缺失问题，基于词的模型存在分词错误的问题，同时将字符和词作为嵌入表示可以使模型具有较好的鲁棒性和识别精度。&lt;/p&gt;
&lt;p&gt;基于 Transformer 的双向编码（bidirectional encoder representations from Transformer，BERT） 模型[51] 是中文命名实体识别中最常用的预训练模型， BERT模型可以考虑整个输入句子的上下文信息，有助于提高模型对命名实体的理解和识别准确性。对于给定的字符，BERT将其字符位置嵌入、句子位置嵌入和字符嵌入作为输入连接起来，然后使用掩码语言模型[52] 对输入句子进行深度双向表示预训练，以获得强大的上下文字符嵌入。&lt;/p&gt;
&lt;h3 id=&#34;编码层&#34;&gt;编码层&lt;/h3&gt;
&lt;p&gt;编码层主要是将嵌入层输入的文本转换成一个高 维的特征向量，方便后续的分类器对文本进行分类。&lt;/p&gt;
&lt;h4 id=&#34;卷积神经网络&#34;&gt;卷积神经网络&lt;/h4&gt;
&lt;p&gt;CNN最初是为计算 机视觉研究开发的，但它已被证明可以有效地捕获具有卷积运算的 n-gram 的信息语义特征[61] 。&lt;/p&gt;
&lt;h4 id=&#34;循环神经网络&#34;&gt;循环神经网络&lt;/h4&gt;
&lt;p&gt;Quyang 等人[67] 提出一种用于 CNER 的深度学习模型，该模型采用双向 RNN-CRF 架构，使用连接的 n -gram 字符表示来捕获丰富的上下文信息。Dong 等人[37] 将双向 LSTM-CRF 神经网络用于 CNER，该网络同时利用字符级和部首级表示，是第一个研究 BiLSTM-CRF 架构中的中文部首级表示，并且在没有精心设计的功能的情况下获得更好的性能。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Named Entity Recognition 相关概念与技术</title>
      <link>http://localhost:1313/posts/nlp/ner/</link>
      <pubDate>Fri, 05 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nlp/ner/</guid>
      <description>命名实体识别快速入门</description>
    </item>
  </channel>
</rss>
